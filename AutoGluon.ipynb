{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6e2c11",
   "metadata": {},
   "source": [
    "# Zindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59fb8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutoGluonZindi/\"\n",
      "Presets specified: ['best_quality']\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'auto_stack': True}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': None,\n",
      " 'auto_stack': True,\n",
      " 'calibrate': 'auto',\n",
      " 'excluded_model_types': None,\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'keep_only_best': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': None,\n",
      " 'num_bag_sets': None,\n",
      " 'num_stack_levels': None,\n",
      " 'pseudo_data': None,\n",
      " 'quantile_levels': None,\n",
      " 'refit_full': False,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 3}\n",
      "========================================\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Saving AutoGluonZindi/\\learner.pkl\n",
      "Saving AutoGluonZindi/\\predictor.pkl\n",
      "Beginning AutoGluon training ... Time limit = 5400s\n",
      "AutoGluon will save models to \"AutoGluonZindi/\\\"\n",
      "AutoGluon Version:  0.5.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Train Data Rows:    9923\n",
      "Train Data Columns: 32\n",
      "Label Column: pm2_5\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (440.92, 1.1604, 57.10731, 27.48295)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9896.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.7 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float64', 'float') : 30 | ['site_latitude', 'humidity', 'temp_mean', 'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', ...]\n",
      "\t\t\t\t('object', 'object') :  2 | ['ID', 'device']\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])  : 30 | ['site_latitude', 'humidity', 'temp_mean', 'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', ...]\n",
      "\t\t\t\t('object', []) :  2 | ['ID', 'device']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])  : 30 | ['site_latitude', 'humidity', 'temp_mean', 'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', ...]\n",
      "\t\t\t\t('object', []) :  2 | ['ID', 'device']\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t32 features in original data used to generate 32 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])  : 30 | ['site_latitude', 'humidity', 'temp_mean', 'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', ...]\n",
      "\t\t\t\t('object', []) :  2 | ['ID', 'device']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])  : 30 | ['site_latitude', 'humidity', 'temp_mean', 'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', ...]\n",
      "\t\t\t\t('object', []) :  2 | ['ID', 'device']\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t32 features in original data used to generate 32 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 30 | ['site_latitude', 'humidity', 'temp_mean', 'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 30 | ['site_latitude', 'humidity', 'temp_mean', 'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t30 features in original data used to generate 30 features in processed data.\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 2 | ['ID', 'device']\n",
      "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 2 | ['ID', 'device']\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('object', []) : 2 | ['ID', 'device']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) : 2 | ['ID', 'device']\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t2 features in original data used to generate 2 features in processed data.\n",
      "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) :  1 | ['device']\n",
      "\t\t\t\t('float', [])    : 30 | ['site_latitude', 'humidity', 'temp_mean', 'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) :  1 | ['device']\n",
      "\t\t\t\t('float', [])    : 30 | ['site_latitude', 'humidity', 'temp_mean', 'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t31 features in original data used to generate 31 features in processed data.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('float64', 'float') : 30 | ['site_latitude', 'humidity', 'temp_mean', 'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', ...]\n",
      "\t\t('object', 'object') :  1 | ['device']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 30 | ['site_latitude', 'humidity', 'temp_mean', 'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', ...]\n",
      "\t\t('object', []) :  1 | ['device']\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('category', 'category') :  1 | ['device']\n",
      "\t\t('float64', 'float')     : 30 | ['site_latitude', 'humidity', 'temp_mean', 'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  1 | ['device']\n",
      "\t\t('float', [])    : 30 | ['site_latitude', 'humidity', 'temp_mean', 'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t31 features in original data used to generate 31 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.39 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving AutoGluonZindi/\\learner.pkl\n",
      "Saving AutoGluonZindi/\\utils\\data\\X.pkl\n",
      "Saving AutoGluonZindi/\\utils\\data\\y.pkl\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Model configs that will be trained (in order):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tKNeighborsUnif_BAG_L1: \t{'weights': 'uniform', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Unif', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tKNeighborsDist_BAG_L1: \t{'weights': 'distance', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Dist', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBMXT_BAG_L1: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tLightGBM_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tRandomForestMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
      "\tExtraTreesMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n",
      "\tXGBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
      "\tNeuralNetTorch_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
      "\tLightGBMLarge_BAG_L1: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 3598.8s of the 5399.56s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\KNeighborsUnif_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\KNeighborsUnif_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonZindi/\\models\\KNeighborsUnif_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\KNeighborsUnif_BAG_L1\\model.pkl\n",
      "\t-18.0451\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t2.71s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 3595.99s of the 5396.74s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\KNeighborsDist_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\KNeighborsDist_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonZindi/\\models\\KNeighborsDist_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\KNeighborsDist_BAG_L1\\model.pkl\n",
      "\t-17.0273\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t2.56s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3593.38s of the 5394.13s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-7.7377\t = Validation score   (-mean_absolute_error)\n",
      "\t42.98s\t = Training   runtime\n",
      "\t15.72s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 3531.84s of the 5332.59s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-7.6165\t = Validation score   (-mean_absolute_error)\n",
      "\t36.45s\t = Training   runtime\n",
      "\t7.02s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3487.86s of the 5288.62s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\RandomForestMSE_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\RandomForestMSE_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonZindi/\\models\\RandomForestMSE_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\RandomForestMSE_BAG_L1\\model.pkl\n",
      "\t-8.893\t = Validation score   (-mean_absolute_error)\n",
      "\t10.52s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 3476.72s of the 5277.47s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "\t-7.3022\t = Validation score   (-mean_absolute_error)\n",
      "\t700.27s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 2773.78s of the 4574.53s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L1\\model.pkl\n",
      "\t-9.7345\t = Validation score   (-mean_absolute_error)\n",
      "\t2.4s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2770.66s of the 4571.41s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4424, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py\", line 171, in _fit\n",
      "    try_import_fastai()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 191, in try_import_fastai\n",
      "    import autogluon.tabular.models.fastainn.imports_helper\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\imports_helper.py\", line 1, in <module>\n",
      "    from fastai.tabular.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\tabular\\all.py\", line 1, in <module>\n",
      "    from ..basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\basics.py\", line 1, in <module>\n",
      "    from .data.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\data\\all.py\", line 1, in <module>\n",
      "    from ..torch_basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\torch_basics.py\", line 1, in <module>\n",
      "    from torch import multiprocessing\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 457, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 425, in after_all_folds_scheduled\n",
      "    time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\worker.py\", line 1831, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(OSError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4424, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py\", line 171, in _fit\n",
      "    try_import_fastai()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 191, in try_import_fastai\n",
      "    import autogluon.tabular.models.fastainn.imports_helper\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\imports_helper.py\", line 1, in <module>\n",
      "    from fastai.tabular.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\tabular\\all.py\", line 1, in <module>\n",
      "    from ..basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\basics.py\", line 1, in <module>\n",
      "    from .data.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\data\\all.py\", line 1, in <module>\n",
      "    from ..torch_basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\torch_basics.py\", line 1, in <module>\n",
      "    from torch import multiprocessing\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 2752.84s of the 4553.59s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L1\\model.pkl\n",
      "\t-8.0356\t = Validation score   (-mean_absolute_error)\n",
      "\t162.59s\t = Training   runtime\n",
      "\t1.0s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 2581.71s of the 4382.46s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\NeuralNetTorch_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\NeuralNetTorch_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=7844, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 142, in _fit\n",
      "    try_import_torch()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 199, in try_import_torch\n",
      "    import torch\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 457, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 425, in after_all_folds_scheduled\n",
      "    time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\worker.py\", line 1831, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(OSError): \u001b[36mray::_ray_fit()\u001b[39m (pid=7844, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 142, in _fit\n",
      "    try_import_torch()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 199, in try_import_torch\n",
      "    import torch\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2575.71s of the 4376.46s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-7.4479\t = Validation score   (-mean_absolute_error)\n",
      "\t118.92s\t = Training   runtime\n",
      "\t13.41s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 2/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2440.03s of the 4240.78s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-7.5077\t = Validation score   (-mean_absolute_error)\n",
      "\t79.27s\t = Training   runtime\n",
      "\t30.26s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2395.64s of the 4196.4s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-7.4082\t = Validation score   (-mean_absolute_error)\n",
      "\t76.54s\t = Training   runtime\n",
      "\t13.91s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2348.38s of the 4149.13s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "\t-7.118\t = Validation score   (-mean_absolute_error)\n",
      "\t1413.39s\t = Training   runtime\n",
      "\t0.62s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L1\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1632.68s of the 3433.44s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L1\\model.pkl\n",
      "\t-7.766\t = Validation score   (-mean_absolute_error)\n",
      "\t305.69s\t = Training   runtime\n",
      "\t2.02s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1486.62s of the 3287.37s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-7.2955\t = Validation score   (-mean_absolute_error)\n",
      "\t238.42s\t = Training   runtime\n",
      "\t26.54s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Not enough time left to finish repeated k-fold bagging, stopping early ...\n",
      "Completed 2/20 k-fold bagging repeats ...\n",
      "Loading: AutoGluonZindi/\\models\\KNeighborsUnif_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\KNeighborsDist_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\RandomForestMSE_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3155.82s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\WeightedEnsemble_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\WeightedEnsemble_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "Ensemble size: 41\n",
      "Ensemble weights: \n",
      "[0.         0.         0.07317073 0.02439024 0.         0.56097561\n",
      " 0.         0.         0.34146341]\n",
      "Saving AutoGluonZindi/\\models\\WeightedEnsemble_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "\t-6.9647\t = Validation score   (-mean_absolute_error)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tLightGBMXT_BAG_L2: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tLightGBM_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tRandomForestMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
      "\tExtraTreesMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n",
      "\tXGBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tNeuralNetTorch_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
      "\tLightGBMLarge_BAG_L2: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}}\n",
      "Fitting 9 L2 models ...\n",
      "Loading: AutoGluonZindi/\\models\\KNeighborsUnif_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\KNeighborsDist_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\RandomForestMSE_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 3155.49s of the 3155.47s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.0672\t = Validation score   (-mean_absolute_error)\n",
      "\t4.96s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 3147.8s of the 3147.77s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-7.0113\t = Validation score   (-mean_absolute_error)\n",
      "\t5.24s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 3139.77s of the 3139.75s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\RandomForestMSE_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\RandomForestMSE_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonZindi/\\models\\RandomForestMSE_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\RandomForestMSE_BAG_L2\\model.pkl\n",
      "\t-7.1157\t = Validation score   (-mean_absolute_error)\n",
      "\t13.52s\t = Training   runtime\n",
      "\t0.49s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 3125.52s of the 3125.49s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\CatBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\CatBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\CatBoost_BAG_L2\\model.pkl\n",
      "\t-6.9994\t = Validation score   (-mean_absolute_error)\n",
      "\t35.69s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 3087.21s of the 3087.18s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L2\\model.pkl\n",
      "\t-6.9872\t = Validation score   (-mean_absolute_error)\n",
      "\t3.21s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 3083.24s of the 3083.22s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\NeuralNetFastAI_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\NeuralNetFastAI_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=12604, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py\", line 171, in _fit\n",
      "    try_import_fastai()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 191, in try_import_fastai\n",
      "    import autogluon.tabular.models.fastainn.imports_helper\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\imports_helper.py\", line 1, in <module>\n",
      "    from fastai.tabular.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\tabular\\all.py\", line 1, in <module>\n",
      "    from ..basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\basics.py\", line 1, in <module>\n",
      "    from .data.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\data\\all.py\", line 1, in <module>\n",
      "    from ..torch_basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\torch_basics.py\", line 1, in <module>\n",
      "    from torch import multiprocessing\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 457, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 425, in after_all_folds_scheduled\n",
      "    time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\worker.py\", line 1831, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(OSError): \u001b[36mray::_ray_fit()\u001b[39m (pid=12604, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py\", line 171, in _fit\n",
      "    try_import_fastai()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 191, in try_import_fastai\n",
      "    import autogluon.tabular.models.fastainn.imports_helper\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\imports_helper.py\", line 1, in <module>\n",
      "    from fastai.tabular.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\tabular\\all.py\", line 1, in <module>\n",
      "    from ..basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\basics.py\", line 1, in <module>\n",
      "    from .data.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\data\\all.py\", line 1, in <module>\n",
      "    from ..torch_basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\torch_basics.py\", line 1, in <module>\n",
      "    from torch import multiprocessing\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 3076.7s of the 3076.67s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.9381\t = Validation score   (-mean_absolute_error)\n",
      "\t15.15s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 3053.43s of the 3053.41s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\NeuralNetTorch_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\NeuralNetTorch_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=11548, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 142, in _fit\n",
      "    try_import_torch()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 199, in try_import_torch\n",
      "    import torch\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 457, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 425, in after_all_folds_scheduled\n",
      "    time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\worker.py\", line 1831, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(OSError): \u001b[36mray::_ray_fit()\u001b[39m (pid=11548, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 142, in _fit\n",
      "    try_import_torch()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 199, in try_import_torch\n",
      "    import torch\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3047.82s of the 3047.79s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-7.1176\t = Validation score   (-mean_absolute_error)\n",
      "\t11.95s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 2/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 3027.51s of the 3027.49s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.0405\t = Validation score   (-mean_absolute_error)\n",
      "\t9.82s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 3019.66s of the 3019.64s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.975\t = Validation score   (-mean_absolute_error)\n",
      "\t10.79s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L2\\model.pkl\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 3011.33s of the 3011.31s of remaining time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "2022-10-03 22:39:57,280\tERROR serialization.py:342 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\exceptions.py\", line 38, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\serialization.py\", line 340, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\serialization.py\", line 260, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\exceptions.py\", line 32, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\exceptions.py\", line 41, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tSystem error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\exceptions.py\", line 38, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\serialization.py\", line 340, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\serialization.py\", line 260, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\exceptions.py\", line 32, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\exceptions.py\", line 41, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 457, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 425, in after_all_folds_scheduled\n",
      "    time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\worker.py\", line 1833, in get\n",
      "    raise value\n",
      "ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\exceptions.py\", line 38, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\serialization.py\", line 340, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\serialization.py\", line 260, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\exceptions.py\", line 32, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\exceptions.py\", line 41, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 3002.91s of the 3002.89s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.9076\t = Validation score   (-mean_absolute_error)\n",
      "\t30.32s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2979.79s of the 2979.76s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-7.0191\t = Validation score   (-mean_absolute_error)\n",
      "\t24.63s\t = Training   runtime\n",
      "\t0.6s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 3/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2964.1s of the 2964.07s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.0254\t = Validation score   (-mean_absolute_error)\n",
      "\t14.9s\t = Training   runtime\n",
      "\t0.61s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2956.12s of the 2956.09s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9679\t = Validation score   (-mean_absolute_error)\n",
      "\t16.4s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2947.61s of the 2947.58s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.9017\t = Validation score   (-mean_absolute_error)\n",
      "\t46.5s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2928.42s of the 2928.4s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-7.0122\t = Validation score   (-mean_absolute_error)\n",
      "\t37.46s\t = Training   runtime\n",
      "\t0.88s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 4/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2912.54s of the 2912.51s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.0145\t = Validation score   (-mean_absolute_error)\n",
      "\t20.11s\t = Training   runtime\n",
      "\t0.82s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2904.35s of the 2904.32s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9693\t = Validation score   (-mean_absolute_error)\n",
      "\t22.08s\t = Training   runtime\n",
      "\t0.51s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2895.8s of the 2895.77s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.8992\t = Validation score   (-mean_absolute_error)\n",
      "\t62.82s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2876.47s of the 2876.45s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-7.0064\t = Validation score   (-mean_absolute_error)\n",
      "\t50.83s\t = Training   runtime\n",
      "\t1.17s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 5/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2860.1s of the 2860.08s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.0139\t = Validation score   (-mean_absolute_error)\n",
      "\t25.46s\t = Training   runtime\n",
      "\t0.99s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2851.81s of the 2851.78s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9633\t = Validation score   (-mean_absolute_error)\n",
      "\t27.78s\t = Training   runtime\n",
      "\t0.66s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2843.06s of the 2843.04s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.9009\t = Validation score   (-mean_absolute_error)\n",
      "\t79.32s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2823.57s of the 2823.55s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9952\t = Validation score   (-mean_absolute_error)\n",
      "\t63.99s\t = Training   runtime\n",
      "\t1.48s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 6/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2807.35s of the 2807.32s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.0107\t = Validation score   (-mean_absolute_error)\n",
      "\t30.82s\t = Training   runtime\n",
      "\t1.2s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2799.14s of the 2799.11s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9602\t = Validation score   (-mean_absolute_error)\n",
      "\t33.44s\t = Training   runtime\n",
      "\t0.78s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2790.41s of the 2790.38s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.8991\t = Validation score   (-mean_absolute_error)\n",
      "\t95.9s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2770.76s of the 2770.72s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9886\t = Validation score   (-mean_absolute_error)\n",
      "\t77.54s\t = Training   runtime\n",
      "\t1.77s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 7/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2754.14s of the 2754.11s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.0101\t = Validation score   (-mean_absolute_error)\n",
      "\t36.14s\t = Training   runtime\n",
      "\t1.41s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2745.84s of the 2745.82s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9598\t = Validation score   (-mean_absolute_error)\n",
      "\t39.1s\t = Training   runtime\n",
      "\t0.9s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2737.15s of the 2737.13s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-6.8978\t = Validation score   (-mean_absolute_error)\n",
      "\t112.51s\t = Training   runtime\n",
      "\t0.92s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2717.5s of the 2717.48s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9894\t = Validation score   (-mean_absolute_error)\n",
      "\t90.81s\t = Training   runtime\n",
      "\t2.07s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 8/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2701.22s of the 2701.18s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S8F1 - S8F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.0073\t = Validation score   (-mean_absolute_error)\n",
      "\t41.61s\t = Training   runtime\n",
      "\t1.6s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2692.52s of the 2692.5s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S8F1 - S8F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9581\t = Validation score   (-mean_absolute_error)\n",
      "\t44.7s\t = Training   runtime\n",
      "\t1.03s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2683.8s of the 2683.77s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S8F1 - S8F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.8946\t = Validation score   (-mean_absolute_error)\n",
      "\t129.39s\t = Training   runtime\n",
      "\t1.03s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2663.79s of the 2663.76s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S8F1 - S8F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9861\t = Validation score   (-mean_absolute_error)\n",
      "\t104.5s\t = Training   runtime\n",
      "\t2.38s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 9/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2646.86s of the 2646.82s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S9F1 - S9F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.008\t = Validation score   (-mean_absolute_error)\n",
      "\t46.88s\t = Training   runtime\n",
      "\t1.77s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2638.49s of the 2638.47s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S9F1 - S9F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9608\t = Validation score   (-mean_absolute_error)\n",
      "\t50.6s\t = Training   runtime\n",
      "\t1.14s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2629.53s of the 2629.5s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S9F1 - S9F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.8991\t = Validation score   (-mean_absolute_error)\n",
      "\t146.38s\t = Training   runtime\n",
      "\t1.17s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2609.53s of the 2609.51s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S9F1 - S9F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9892\t = Validation score   (-mean_absolute_error)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t118.1s\t = Training   runtime\n",
      "\t2.67s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 10/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2592.73s of the 2592.7s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S10F1 - S10F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.0059\t = Validation score   (-mean_absolute_error)\n",
      "\t52.17s\t = Training   runtime\n",
      "\t1.99s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2584.33s of the 2584.31s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S10F1 - S10F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9604\t = Validation score   (-mean_absolute_error)\n",
      "\t56.39s\t = Training   runtime\n",
      "\t1.27s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2575.38s of the 2575.34s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S10F1 - S10F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.8986\t = Validation score   (-mean_absolute_error)\n",
      "\t164.23s\t = Training   runtime\n",
      "\t1.3s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2554.33s of the 2554.31s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S10F1 - S10F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9892\t = Validation score   (-mean_absolute_error)\n",
      "\t131.72s\t = Training   runtime\n",
      "\t2.97s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 11/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2537.61s of the 2537.58s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S11F1 - S11F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.0043\t = Validation score   (-mean_absolute_error)\n",
      "\t57.96s\t = Training   runtime\n",
      "\t2.23s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2528.61s of the 2528.58s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S11F1 - S11F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9569\t = Validation score   (-mean_absolute_error)\n",
      "\t62.18s\t = Training   runtime\n",
      "\t1.4s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2519.78s of the 2519.76s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S11F1 - S11F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.8975\t = Validation score   (-mean_absolute_error)\n",
      "\t181.66s\t = Training   runtime\n",
      "\t1.42s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2499.4s of the 2499.36s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S11F1 - S11F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9866\t = Validation score   (-mean_absolute_error)\n",
      "\t145.8s\t = Training   runtime\n",
      "\t3.3s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 12/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2482.14s of the 2482.1s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S12F1 - S12F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.0047\t = Validation score   (-mean_absolute_error)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t63.68s\t = Training   runtime\n",
      "\t2.43s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2473.45s of the 2473.41s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S12F1 - S12F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9556\t = Validation score   (-mean_absolute_error)\n",
      "\t68.17s\t = Training   runtime\n",
      "\t1.52s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2464.38s of the 2464.35s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S12F1 - S12F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.8961\t = Validation score   (-mean_absolute_error)\n",
      "\t199.14s\t = Training   runtime\n",
      "\t1.54s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2443.87s of the 2443.84s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S12F1 - S12F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9874\t = Validation score   (-mean_absolute_error)\n",
      "\t159.35s\t = Training   runtime\n",
      "\t3.61s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 13/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2427.12s of the 2427.1s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S13F1 - S13F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.0036\t = Validation score   (-mean_absolute_error)\n",
      "\t69.34s\t = Training   runtime\n",
      "\t2.67s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2418.33s of the 2418.3s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S13F1 - S13F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9549\t = Validation score   (-mean_absolute_error)\n",
      "\t74.17s\t = Training   runtime\n",
      "\t1.66s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2409.15s of the 2409.13s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S13F1 - S13F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.8943\t = Validation score   (-mean_absolute_error)\n",
      "\t216.88s\t = Training   runtime\n",
      "\t1.67s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2388.48s of the 2388.46s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S13F1 - S13F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9864\t = Validation score   (-mean_absolute_error)\n",
      "\t173.83s\t = Training   runtime\n",
      "\t3.99s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 14/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2370.76s of the 2370.72s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S14F1 - S14F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.0015\t = Validation score   (-mean_absolute_error)\n",
      "\t74.93s\t = Training   runtime\n",
      "\t2.88s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2362.08s of the 2362.04s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S14F1 - S14F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9562\t = Validation score   (-mean_absolute_error)\n",
      "\t79.78s\t = Training   runtime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t1.77s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2353.24s of the 2353.22s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S14F1 - S14F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.894\t = Validation score   (-mean_absolute_error)\n",
      "\t234.36s\t = Training   runtime\n",
      "\t1.83s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2332.63s of the 2332.61s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S14F1 - S14F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9864\t = Validation score   (-mean_absolute_error)\n",
      "\t187.52s\t = Training   runtime\n",
      "\t4.29s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 15/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2315.69s of the 2315.67s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S15F1 - S15F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-7.0006\t = Validation score   (-mean_absolute_error)\n",
      "\t80.34s\t = Training   runtime\n",
      "\t3.08s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2307.11s of the 2307.08s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S15F1 - S15F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9559\t = Validation score   (-mean_absolute_error)\n",
      "\t85.67s\t = Training   runtime\n",
      "\t1.89s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2298.1s of the 2298.07s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S15F1 - S15F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.8939\t = Validation score   (-mean_absolute_error)\n",
      "\t251.73s\t = Training   runtime\n",
      "\t1.98s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2277.74s of the 2277.71s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S15F1 - S15F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9872\t = Validation score   (-mean_absolute_error)\n",
      "\t201.53s\t = Training   runtime\n",
      "\t4.57s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 16/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2260.57s of the 2260.55s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S16F1 - S16F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-6.9999\t = Validation score   (-mean_absolute_error)\n",
      "\t85.7s\t = Training   runtime\n",
      "\t3.31s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2252.14s of the 2252.11s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S16F1 - S16F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9555\t = Validation score   (-mean_absolute_error)\n",
      "\t91.35s\t = Training   runtime\n",
      "\t2.03s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2243.21s of the 2243.18s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S16F1 - S16F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.8935\t = Validation score   (-mean_absolute_error)\n",
      "\t269.56s\t = Training   runtime\n",
      "\t2.11s\t = Validation runtime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2222.33s of the 2222.31s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S16F1 - S16F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9839\t = Validation score   (-mean_absolute_error)\n",
      "\t215.96s\t = Training   runtime\n",
      "\t4.89s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 17/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2204.66s of the 2204.63s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S17F1 - S17F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-6.9963\t = Validation score   (-mean_absolute_error)\n",
      "\t91.66s\t = Training   runtime\n",
      "\t3.57s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2195.6s of the 2195.57s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S17F1 - S17F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9531\t = Validation score   (-mean_absolute_error)\n",
      "\t97.25s\t = Training   runtime\n",
      "\t2.15s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2186.73s of the 2186.71s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S17F1 - S17F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.8925\t = Validation score   (-mean_absolute_error)\n",
      "\t287.16s\t = Training   runtime\n",
      "\t2.23s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2166.17s of the 2166.15s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S17F1 - S17F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9814\t = Validation score   (-mean_absolute_error)\n",
      "\t230.36s\t = Training   runtime\n",
      "\t5.23s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 18/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2148.54s of the 2148.52s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S18F1 - S18F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-6.9958\t = Validation score   (-mean_absolute_error)\n",
      "\t96.9s\t = Training   runtime\n",
      "\t3.78s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2140.21s of the 2140.19s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S18F1 - S18F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9534\t = Validation score   (-mean_absolute_error)\n",
      "\t103.01s\t = Training   runtime\n",
      "\t2.27s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2131.29s of the 2131.26s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S18F1 - S18F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.8909\t = Validation score   (-mean_absolute_error)\n",
      "\t304.24s\t = Training   runtime\n",
      "\t2.38s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2111.07s of the 2111.05s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S18F1 - S18F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9796\t = Validation score   (-mean_absolute_error)\n",
      "\t244.43s\t = Training   runtime\n",
      "\t5.54s\t = Validation runtime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 19/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2093.79s of the 2093.76s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S19F1 - S19F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-6.995\t = Validation score   (-mean_absolute_error)\n",
      "\t102.6s\t = Training   runtime\n",
      "\t3.96s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2085.04s of the 2085.02s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S19F1 - S19F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9526\t = Validation score   (-mean_absolute_error)\n",
      "\t108.91s\t = Training   runtime\n",
      "\t2.4s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2076.0s of the 2075.97s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S19F1 - S19F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.8891\t = Validation score   (-mean_absolute_error)\n",
      "\t321.52s\t = Training   runtime\n",
      "\t2.5s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2055.72s of the 2055.7s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S19F1 - S19F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9789\t = Validation score   (-mean_absolute_error)\n",
      "\t258.63s\t = Training   runtime\n",
      "\t5.85s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 20/20\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2038.21s of the 2038.19s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S20F1 - S20F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-6.9962\t = Validation score   (-mean_absolute_error)\n",
      "\t107.96s\t = Training   runtime\n",
      "\t4.16s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2029.86s of the 2029.83s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S20F1 - S20F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-6.9525\t = Validation score   (-mean_absolute_error)\n",
      "\t114.84s\t = Training   runtime\n",
      "\t2.52s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2020.73s of the 2020.7s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S20F1 - S20F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "\t-6.8885\t = Validation score   (-mean_absolute_error)\n",
      "\t339.13s\t = Training   runtime\n",
      "\t2.63s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2000.12s of the 2000.1s of remaining time.\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S20F1 - S20F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-6.9786\t = Validation score   (-mean_absolute_error)\n",
      "\t272.78s\t = Training   runtime\n",
      "\t6.14s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Completed 20/20 k-fold bagging repeats ...\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\RandomForestMSE_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L3: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 1982.52s of remaining time.\n",
      "Saving AutoGluonZindi/\\models\\WeightedEnsemble_L3\\utils\\model_template.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutoGluonZindi/\\models\\WeightedEnsemble_L3\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "Ensemble size: 81\n",
      "Ensemble weights: \n",
      "[0.13580247 0.07407407 0.         0.22222222 0.56790123 0.        ]\n",
      "Saving AutoGluonZindi/\\models\\WeightedEnsemble_L3\\utils\\oof.pkl\n",
      "Saving AutoGluonZindi/\\models\\WeightedEnsemble_L3\\model.pkl\n",
      "\t-6.8629\t = Validation score   (-mean_absolute_error)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "AutoGluon training complete, total runtime = 3417.78s ... Best model: \"WeightedEnsemble_L3\"\n",
      "Loading: AutoGluonZindi/\\models\\trainer.pkl\n",
      "Saving AutoGluonZindi/\\models\\trainer.pkl\n",
      "Saving AutoGluonZindi/\\learner.pkl\n",
      "Saving AutoGluonZindi/\\predictor.pkl\n",
      "Saving AutoGluonZindi/\\__version__ with contents \"0.5.2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutoGluonZindi/\\\")\n",
      "Loading: AutoGluonZindi/\\models\\KNeighborsUnif_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\KNeighborsDist_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\RandomForestMSE_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\RandomForestMSE_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\WeightedEnsemble_L3\\model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                     model  score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3  -6.862925      89.265571  2691.637641                0.000000           0.234747            3       True         18\n",
      "1           XGBoost_BAG_L2  -6.888504      82.111129  2465.389842                2.628043         339.127272            2       True         16\n",
      "2          LightGBM_BAG_L2  -6.952464      81.999919  2241.101225                2.516833         114.838656            2       True         12\n",
      "3      WeightedEnsemble_L2  -6.964652      71.341209  1807.904410                0.008064           0.285998            2       True         10\n",
      "4     LightGBMLarge_BAG_L2  -6.978558      85.618182  2399.044394                6.135096         272.781824            2       True         17\n",
      "5     ExtraTreesMSE_BAG_L2  -6.987236      79.959283  2129.473569                0.476197           3.210999            2       True         15\n",
      "6        LightGBMXT_BAG_L2  -6.996170      83.644497  2234.225966                4.161411         107.963397            2       True         11\n",
      "7          CatBoost_BAG_L2  -6.999439      79.562181  2161.956599                0.079095          35.694030            2       True         14\n",
      "8   RandomForestMSE_BAG_L2  -7.115652      79.972188  2139.778765                0.489102          13.516195            2       True         13\n",
      "9          CatBoost_BAG_L1  -7.118035       0.620301  1413.387696                0.620301        1413.387696            1       True          6\n",
      "10    LightGBMLarge_BAG_L1  -7.295504      26.541156   238.421776               26.541156         238.421776            1       True          9\n",
      "11         LightGBM_BAG_L1  -7.408239      13.908169    76.537692               13.908169          76.537692            1       True          4\n",
      "12       LightGBMXT_BAG_L1  -7.507739      30.263520    79.271249               30.263520          79.271249            1       True          3\n",
      "13          XGBoost_BAG_L1  -7.766048       2.023612   305.690903                2.023612         305.690903            1       True          8\n",
      "14  RandomForestMSE_BAG_L1  -8.892981       0.409533    10.521525                0.409533          10.521525            1       True          5\n",
      "15    ExtraTreesMSE_BAG_L1  -9.734494       0.446061     2.395602                0.446061           2.395602            1       True          7\n",
      "16   KNeighborsDist_BAG_L1 -17.027309       2.562036     0.019998                2.562036           0.019998            1       True          2\n",
      "17   KNeighborsUnif_BAG_L1 -18.045081       2.708699     0.016129                2.708699           0.016129            1       True          1\n",
      "Number of models trained: 18\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'StackerEnsembleModel_XT', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_CatBoost', 'StackerEnsembleModel_RF', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_XGBoost'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) :  1 | ['device']\n",
      "('float', [])    : 30 | ['site_latitude', 'humidity', 'temp_mean', 'SulphurDioxide_cloud_fraction', 'SulphurDioxide_sensor_azimuth_angle', ...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:138: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n",
      "Loading: AutoGluonZindi/\\models\\WeightedEnsemble_L3\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\CatBoost_BAG_L1\\model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\xgboost\\compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "Loading: AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\KNeighborsDist_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\KNeighborsUnif_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\RandomForestMSE_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\ExtraTreesMSE_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonZindi/\\models\\XGBoost_BAG_L2\\model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "label = 'pm2_5'  # name of target variable to predict in this competition\n",
    "eval_metric = 'mae'  # Optional: specify that competition evaluation metric is AUC\n",
    "save_path = 'AutoGluonZindi/'  # where to store trained models\n",
    "\n",
    "train_data = pd.read_csv('train_zindi.csv', low_memory=False)\n",
    "predictor = TabularPredictor(label=label, eval_metric=eval_metric, path=save_path, verbosity=3).fit(\n",
    "    train_data, presets='best_quality', time_limit=5400\n",
    ")\n",
    "\n",
    "results = predictor.fit_summary()\n",
    "test_data = pd.read_csv('test_zindi.csv', low_memory=False)\n",
    "y_predproba = predictor.predict_proba(test_data)\n",
    "\n",
    "test_data['pm2_5'] = y_predproba\n",
    "test_data[['ID', 'pm2_5']].to_csv('zindi_autogluon.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36e6485",
   "metadata": {},
   "source": [
    "# Sber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b49ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutoGluonSber/\"\n",
      "Presets specified: ['best_quality']\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'auto_stack': True}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': None,\n",
      " 'auto_stack': True,\n",
      " 'calibrate': 'auto',\n",
      " 'excluded_model_types': None,\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'keep_only_best': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': None,\n",
      " 'num_bag_sets': None,\n",
      " 'num_stack_levels': None,\n",
      " 'pseudo_data': None,\n",
      " 'quantile_levels': None,\n",
      " 'refit_full': False,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 3}\n",
      "========================================\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Saving AutoGluonSber/\\learner.pkl\n",
      "Saving AutoGluonSber/\\predictor.pkl\n",
      "Beginning AutoGluon training ... Time limit = 5400s\n",
      "AutoGluon will save models to \"AutoGluonSber/\\\"\n",
      "AutoGluon Version:  0.5.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Train Data Rows:    30471\n",
      "Train Data Columns: 291\n",
      "Label Column: price_doc\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == int and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (111111112, 100000, 7123035.27774, 4780111.32963)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    8785.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 96.84 MB (1.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 17 features to boolean dtype as they only contain 2 unique values.\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float64', 'float') : 119 | ['life_sq', 'floor', 'max_floor', 'material', 'build_year', ...]\n",
      "\t\t\t\t('int64', 'int')     : 156 | ['id', 'full_sq', 'raion_popul', 'children_preschool', 'preschool_education_centers_raion', ...]\n",
      "\t\t\t\t('object', 'object') :  16 | ['timestamp', 'product_type', 'sub_area', 'culture_objects_top_25', 'thermal_power_plant_raion', ...]\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])                      : 119 | ['life_sq', 'floor', 'max_floor', 'material', 'build_year', ...]\n",
      "\t\t\t\t('int', [])                        : 156 | ['id', 'full_sq', 'raion_popul', 'children_preschool', 'preschool_education_centers_raion', ...]\n",
      "\t\t\t\t('object', [])                     :  15 | ['product_type', 'sub_area', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', ...]\n",
      "\t\t\t\t('object', ['datetime_as_object']) :   1 | ['timestamp']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])                      : 119 | ['life_sq', 'floor', 'max_floor', 'material', 'build_year', ...]\n",
      "\t\t\t\t('int', [])                        : 152 | ['id', 'full_sq', 'raion_popul', 'children_preschool', 'preschool_education_centers_raion', ...]\n",
      "\t\t\t\t('int', ['bool'])                  :  17 | ['product_type', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', ...]\n",
      "\t\t\t\t('object', [])                     :   2 | ['sub_area', 'ecology']\n",
      "\t\t\t\t('object', ['datetime_as_object']) :   1 | ['timestamp']\n",
      "\t\t\t0.3s = Fit runtime\n",
      "\t\t\t291 features in original data used to generate 291 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])                      : 119 | ['life_sq', 'floor', 'max_floor', 'material', 'build_year', ...]\n",
      "\t\t\t\t('int', [])                        : 152 | ['id', 'full_sq', 'raion_popul', 'children_preschool', 'preschool_education_centers_raion', ...]\n",
      "\t\t\t\t('int', ['bool'])                  :  17 | ['product_type', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', ...]\n",
      "\t\t\t\t('object', [])                     :   2 | ['sub_area', 'ecology']\n",
      "\t\t\t\t('object', ['datetime_as_object']) :   1 | ['timestamp']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])                      : 119 | ['life_sq', 'floor', 'max_floor', 'material', 'build_year', ...]\n",
      "\t\t\t\t('int', [])                        : 152 | ['id', 'full_sq', 'raion_popul', 'children_preschool', 'preschool_education_centers_raion', ...]\n",
      "\t\t\t\t('int', ['bool'])                  :  17 | ['product_type', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', ...]\n",
      "\t\t\t\t('object', [])                     :   2 | ['sub_area', 'ecology']\n",
      "\t\t\t\t('object', ['datetime_as_object']) :   1 | ['timestamp']\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t291 features in original data used to generate 291 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 119 | ['life_sq', 'floor', 'max_floor', 'material', 'build_year', ...]\n",
      "\t\t\t\t('int', [])       : 152 | ['id', 'full_sq', 'raion_popul', 'children_preschool', 'preschool_education_centers_raion', ...]\n",
      "\t\t\t\t('int', ['bool']) :  17 | ['product_type', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 119 | ['life_sq', 'floor', 'max_floor', 'material', 'build_year', ...]\n",
      "\t\t\t\t('int', [])       : 152 | ['id', 'full_sq', 'raion_popul', 'children_preschool', 'preschool_education_centers_raion', ...]\n",
      "\t\t\t\t('int', ['bool']) :  17 | ['product_type', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t288 features in original data used to generate 288 features in processed data.\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 2 | ['sub_area', 'ecology']\n",
      "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 2 | ['sub_area', 'ecology']\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('object', []) : 2 | ['sub_area', 'ecology']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) : 2 | ['sub_area', 'ecology']\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t2 features in original data used to generate 2 features in processed data.\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('object', ['datetime_as_object']) : 1 | ['timestamp']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', ['datetime_as_int']) : 5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t1 features in original data used to generate 5 features in processed data.\n",
      "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])             :   2 | ['sub_area', 'ecology']\n",
      "\t\t\t\t('float', [])                : 119 | ['life_sq', 'floor', 'max_floor', 'material', 'build_year', ...]\n",
      "\t\t\t\t('int', [])                  : 152 | ['id', 'full_sq', 'raion_popul', 'children_preschool', 'preschool_education_centers_raion', ...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t\t\t('int', ['bool'])            :  17 | ['product_type', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', ...]\n",
      "\t\t\t\t('int', ['datetime_as_int']) :   5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])             :   2 | ['sub_area', 'ecology']\n",
      "\t\t\t\t('float', [])                : 119 | ['life_sq', 'floor', 'max_floor', 'material', 'build_year', ...]\n",
      "\t\t\t\t('int', [])                  : 152 | ['id', 'full_sq', 'raion_popul', 'children_preschool', 'preschool_education_centers_raion', ...]\n",
      "\t\t\t\t('int', ['bool'])            :  17 | ['product_type', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', ...]\n",
      "\t\t\t\t('int', ['datetime_as_int']) :   5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t\t\t0.2s = Fit runtime\n",
      "\t\t\t295 features in original data used to generate 295 features in processed data.\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('float64', 'float') : 119 | ['life_sq', 'floor', 'max_floor', 'material', 'build_year', ...]\n",
      "\t\t('int64', 'int')     : 156 | ['id', 'full_sq', 'raion_popul', 'children_preschool', 'preschool_education_centers_raion', ...]\n",
      "\t\t('object', 'object') :  16 | ['timestamp', 'product_type', 'sub_area', 'culture_objects_top_25', 'thermal_power_plant_raion', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 119 | ['life_sq', 'floor', 'max_floor', 'material', 'build_year', ...]\n",
      "\t\t('int', [])                        : 156 | ['id', 'full_sq', 'raion_popul', 'children_preschool', 'preschool_education_centers_raion', ...]\n",
      "\t\t('object', [])                     :  15 | ['product_type', 'sub_area', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', ...]\n",
      "\t\t('object', ['datetime_as_object']) :   1 | ['timestamp']\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('category', 'category') :   2 | ['sub_area', 'ecology']\n",
      "\t\t('float64', 'float')     : 119 | ['life_sq', 'floor', 'max_floor', 'material', 'build_year', ...]\n",
      "\t\t('int32', 'int')         :   4 | ['timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t\t('int64', 'int')         : 153 | ['id', 'full_sq', 'raion_popul', 'children_preschool', 'preschool_education_centers_raion', ...]\n",
      "\t\t('int8', 'int')          :  17 | ['product_type', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             :   2 | ['sub_area', 'ecology']\n",
      "\t\t('float', [])                : 119 | ['life_sq', 'floor', 'max_floor', 'material', 'build_year', ...]\n",
      "\t\t('int', [])                  : 152 | ['id', 'full_sq', 'raion_popul', 'children_preschool', 'preschool_education_centers_raion', ...]\n",
      "\t\t('int', ['bool'])            :  17 | ['product_type', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', ...]\n",
      "\t\t('int', ['datetime_as_int']) :   5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t1.3s = Fit runtime\n",
      "\t291 features in original data used to generate 295 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 67.4 MB (0.8% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.49s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving AutoGluonSber/\\learner.pkl\n",
      "Saving AutoGluonSber/\\utils\\data\\X.pkl\n",
      "Saving AutoGluonSber/\\utils\\data\\y.pkl\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Model configs that will be trained (in order):\n",
      "\tKNeighborsUnif_BAG_L1: \t{'weights': 'uniform', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Unif', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tKNeighborsDist_BAG_L1: \t{'weights': 'distance', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Dist', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBMXT_BAG_L1: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tLightGBM_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tRandomForestMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
      "\tExtraTreesMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n",
      "\tXGBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
      "\tNeuralNetTorch_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
      "\tLightGBMLarge_BAG_L1: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 3598.11s of the 5398.51s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\KNeighborsUnif_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\KNeighborsUnif_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t0.03s \t= Train Time (Using 10000/30471 rows) (3598.02s remaining time)\n",
      "\t0.05s \t= Train Time (Using 20000/30471 rows) (3597.97s remaining time)\n",
      "\t0.02s \t= Train Time (Using 30471/30471 rows) (3597.95s remaining time)\n",
      "\t25.85s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonSber/\\models\\KNeighborsUnif_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\KNeighborsUnif_BAG_L1\\model.pkl\n",
      "\t-5138587.3686\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t28.79s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 3568.11s of the 5368.51s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\KNeighborsDist_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\KNeighborsDist_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t0.03s \t= Train Time (Using 10000/30471 rows) (3568.04s remaining time)\n",
      "\t0.06s \t= Train Time (Using 20000/30471 rows) (3567.98s remaining time)\n",
      "\t0.01s \t= Train Time (Using 30471/30471 rows) (3567.97s remaining time)\n",
      "\t24.25s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonSber/\\models\\KNeighborsDist_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\KNeighborsDist_BAG_L1\\model.pkl\n",
      "\t-5140266.8022\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t28.7s\t = Validation runtime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3538.19s of the 5338.58s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2572637.3464\t = Validation score   (-root_mean_squared_error)\n",
      "\t57.54s\t = Training   runtime\n",
      "\t2.46s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 3476.82s of the 5277.21s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2608882.8826\t = Validation score   (-root_mean_squared_error)\n",
      "\t57.41s\t = Training   runtime\n",
      "\t1.96s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3415.67s of the 5216.07s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\RandomForestMSE_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\RandomForestMSE_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t3.63s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonSber/\\models\\RandomForestMSE_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\RandomForestMSE_BAG_L1\\model.pkl\n",
      "\t-2675102.6688\t = Validation score   (-root_mean_squared_error)\n",
      "\t218.37s\t = Training   runtime\n",
      "\t4.66s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 3190.17s of the 4990.56s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=15764, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\catboost\\catboost_model.py\", line 211, in _fit\n",
      "    self.model.fit(X, **fit_final_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\catboost\\core.py\", line 5590, in fit\n",
      "    return self._fit(X, y, cat_features, None, None, None, sample_weight, None, None, None, None, baseline,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\catboost\\core.py\", line 2278, in _fit\n",
      "    self._train(\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\catboost\\core.py\", line 1705, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 4585, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 4634, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: bad allocation\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 457, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 425, in after_all_folds_scheduled\n",
      "    time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\worker.py\", line 1831, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(CatBoostError): \u001b[36mray::_ray_fit()\u001b[39m (pid=15764, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\catboost\\catboost_model.py\", line 211, in _fit\n",
      "    self.model.fit(X, **fit_final_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\catboost\\core.py\", line 5590, in fit\n",
      "    return self._fit(X, y, cat_features, None, None, None, sample_weight, None, None, None, None, baseline,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\catboost\\core.py\", line 2278, in _fit\n",
      "    self._train(\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\catboost\\core.py\", line 1705, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 4585, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 4634, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: bad allocation\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 3181.63s of the 4982.02s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\ExtraTreesMSE_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\ExtraTreesMSE_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t3.96s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonSber/\\models\\ExtraTreesMSE_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\ExtraTreesMSE_BAG_L1\\model.pkl\n",
      "\t-2645659.8961\t = Validation score   (-root_mean_squared_error)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t96.55s\t = Training   runtime\n",
      "\t4.46s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 3079.56s of the 4879.96s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=13908, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py\", line 171, in _fit\n",
      "    try_import_fastai()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 191, in try_import_fastai\n",
      "    import autogluon.tabular.models.fastainn.imports_helper\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\imports_helper.py\", line 1, in <module>\n",
      "    from fastai.tabular.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\tabular\\all.py\", line 1, in <module>\n",
      "    from ..basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\basics.py\", line 1, in <module>\n",
      "    from .data.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\data\\all.py\", line 1, in <module>\n",
      "    from ..torch_basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\torch_basics.py\", line 1, in <module>\n",
      "    from torch import multiprocessing\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\cudnn_cnn_infer64_8.dll\" or one of its dependencies.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 457, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 425, in after_all_folds_scheduled\n",
      "    time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\worker.py\", line 1831, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(OSError): \u001b[36mray::_ray_fit()\u001b[39m (pid=13908, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py\", line 171, in _fit\n",
      "    try_import_fastai()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 191, in try_import_fastai\n",
      "    import autogluon.tabular.models.fastainn.imports_helper\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\imports_helper.py\", line 1, in <module>\n",
      "    from fastai.tabular.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\tabular\\all.py\", line 1, in <module>\n",
      "    from ..basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\basics.py\", line 1, in <module>\n",
      "    from .data.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\data\\all.py\", line 1, in <module>\n",
      "    from ..torch_basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\torch_basics.py\", line 1, in <module>\n",
      "    from torch import multiprocessing\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\cudnn_cnn_infer64_8.dll\" or one of its dependencies.\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 3068.91s of the 4869.31s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=9452, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 121, in _fit\n",
      "    try_import_xgboost()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 167, in try_import_xgboost\n",
      "    import xgboost\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\xgboost\\__init__.py\", line 9, in <module>\n",
      "    from .core import DMatrix, DeviceQuantileDMatrix, Booster\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\xgboost\\core.py\", line 195, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\xgboost\\core.py\", line 178, in _load_lib\n",
      "    raise XGBoostError(\n",
      "xgboost.core.XGBoostError: XGBoost Library (xgboost.dll) could not be loaded.\n",
      "Likely causes:\n",
      "  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n",
      "  * You are running 32-bit Python on a 64-bit OS\n",
      "Error message(s): ['[WinError 1455] Файл подкачки слишком мал для завершения операции']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 457, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 425, in after_all_folds_scheduled\n",
      "    time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\worker.py\", line 1831, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(XGBoostError): \u001b[36mray::_ray_fit()\u001b[39m (pid=9452, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 121, in _fit\n",
      "    try_import_xgboost()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 167, in try_import_xgboost\n",
      "    import xgboost\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\xgboost\\__init__.py\", line 9, in <module>\n",
      "    from .core import DMatrix, DeviceQuantileDMatrix, Booster\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\xgboost\\core.py\", line 195, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\xgboost\\core.py\", line 178, in _load_lib\n",
      "    raise XGBoostError(\n",
      "xgboost.core.XGBoostError: XGBoost Library (xgboost.dll) could not be loaded.\n",
      "Likely causes:\n",
      "  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n",
      "  * You are running 32-bit Python on a 64-bit OS\n",
      "Error message(s): ['[WinError 1455] Файл подкачки слишком мал для завершения операции']\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 3056.42s of the 4856.82s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\NeuralNetTorch_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\NeuralNetTorch_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=3272, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 142, in _fit\n",
      "    try_import_torch()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 199, in try_import_torch\n",
      "    import torch\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\cudnn_cnn_infer64_8.dll\" or one of its dependencies.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 457, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 425, in after_all_folds_scheduled\n",
      "    time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\worker.py\", line 1831, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(OSError): \u001b[36mray::_ray_fit()\u001b[39m (pid=3272, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 142, in _fit\n",
      "    try_import_torch()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 199, in try_import_torch\n",
      "    import torch\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\cudnn_cnn_infer64_8.dll\" or one of its dependencies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3044.91s of the 4845.31s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2658878.1752\t = Validation score   (-root_mean_squared_error)\n",
      "\t97.04s\t = Training   runtime\n",
      "\t2.23s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 2/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2939.47s of the 4739.86s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2553215.2018\t = Validation score   (-root_mean_squared_error)\n",
      "\t114.53s\t = Training   runtime\n",
      "\t4.99s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2878.88s of the 4679.28s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2592899.0879\t = Validation score   (-root_mean_squared_error)\n",
      "\t101.67s\t = Training   runtime\n",
      "\t3.59s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2831.4s of the 4631.79s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2631858.79\t = Validation score   (-root_mean_squared_error)\n",
      "\t182.71s\t = Training   runtime\n",
      "\t4.61s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 3/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2742.25s of the 4542.65s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2540522.0733\t = Validation score   (-root_mean_squared_error)\n",
      "\t178.27s\t = Training   runtime\n",
      "\t7.77s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2674.84s of the 4475.24s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2586213.7535\t = Validation score   (-root_mean_squared_error)\n",
      "\t137.38s\t = Training   runtime\n",
      "\t5.04s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2635.96s of the 4436.36s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2615170.3865\t = Validation score   (-root_mean_squared_error)\n",
      "\t273.63s\t = Training   runtime\n",
      "\t6.93s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 4/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2541.48s of the 4341.87s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2542250.9754\t = Validation score   (-root_mean_squared_error)\n",
      "\t234.2s\t = Training   runtime\n",
      "\t10.26s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2481.9s of the 4282.3s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2586263.974\t = Validation score   (-root_mean_squared_error)\n",
      "\t182.23s\t = Training   runtime\n",
      "\t6.68s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2433.79s of the 4234.19s of remaining time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2609675.8675\t = Validation score   (-root_mean_squared_error)\n",
      "\t380.6s\t = Training   runtime\n",
      "\t9.37s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 5/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2323.03s of the 4123.42s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2541101.4247\t = Validation score   (-root_mean_squared_error)\n",
      "\t287.9s\t = Training   runtime\n",
      "\t12.85s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2265.81s of the 4066.21s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2582432.8667\t = Validation score   (-root_mean_squared_error)\n",
      "\t224.22s\t = Training   runtime\n",
      "\t8.35s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2220.52s of the 4020.92s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2606454.6025\t = Validation score   (-root_mean_squared_error)\n",
      "\t495.48s\t = Training   runtime\n",
      "\t11.79s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 6/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2101.76s of the 3902.16s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2539558.4288\t = Validation score   (-root_mean_squared_error)\n",
      "\t335.74s\t = Training   runtime\n",
      "\t15.14s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2050.47s of the 3850.87s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2580816.7182\t = Validation score   (-root_mean_squared_error)\n",
      "\t266.97s\t = Training   runtime\n",
      "\t9.96s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2004.48s of the 3804.88s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2605654.6708\t = Validation score   (-root_mean_squared_error)\n",
      "\t586.42s\t = Training   runtime\n",
      "\t13.98s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 7/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1909.91s of the 3710.3s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2542189.1008\t = Validation score   (-root_mean_squared_error)\n",
      "\t386.4s\t = Training   runtime\n",
      "\t17.59s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1855.78s of the 3656.17s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2584456.1896\t = Validation score   (-root_mean_squared_error)\n",
      "\t306.13s\t = Training   runtime\n",
      "\t11.64s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1813.0s of the 3613.4s of remaining time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2610267.4506\t = Validation score   (-root_mean_squared_error)\n",
      "\t690.62s\t = Training   runtime\n",
      "\t16.07s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 8/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1704.96s of the 3505.36s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S8F1 - S8F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2541302.5472\t = Validation score   (-root_mean_squared_error)\n",
      "\t429.93s\t = Training   runtime\n",
      "\t19.85s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1658.04s of the 3458.43s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S8F1 - S8F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2584123.7034\t = Validation score   (-root_mean_squared_error)\n",
      "\t347.56s\t = Training   runtime\n",
      "\t13.36s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1613.32s of the 3413.72s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S8F1 - S8F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2608758.9331\t = Validation score   (-root_mean_squared_error)\n",
      "\t782.16s\t = Training   runtime\n",
      "\t18.18s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 9/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1518.12s of the 3318.51s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S9F1 - S9F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2542310.069\t = Validation score   (-root_mean_squared_error)\n",
      "\t474.46s\t = Training   runtime\n",
      "\t22.21s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1470.18s of the 3270.58s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S9F1 - S9F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2584369.9544\t = Validation score   (-root_mean_squared_error)\n",
      "\t388.31s\t = Training   runtime\n",
      "\t14.99s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1426.15s of the 3226.55s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S9F1 - S9F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2608315.2153\t = Validation score   (-root_mean_squared_error)\n",
      "\t872.76s\t = Training   runtime\n",
      "\t20.27s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 10/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1331.95s of the 3132.33s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S10F1 - S10F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2543740.6142\t = Validation score   (-root_mean_squared_error)\n",
      "\t528.37s\t = Training   runtime\n",
      "\t24.56s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1274.46s of the 3074.85s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S10F1 - S10F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2584819.624\t = Validation score   (-root_mean_squared_error)\n",
      "\t434.55s\t = Training   runtime\n",
      "\t16.8s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1224.88s of the 3025.28s of remaining time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S10F1 - S10F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2606612.9123\t = Validation score   (-root_mean_squared_error)\n",
      "\t957.17s\t = Training   runtime\n",
      "\t22.24s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 11/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1136.9s of the 2937.3s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S11F1 - S11F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2544085.4169\t = Validation score   (-root_mean_squared_error)\n",
      "\t577.7s\t = Training   runtime\n",
      "\t26.92s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1084.07s of the 2884.46s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S11F1 - S11F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2584530.2055\t = Validation score   (-root_mean_squared_error)\n",
      "\t480.05s\t = Training   runtime\n",
      "\t18.63s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1035.22s of the 2835.62s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S11F1 - S11F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2606451.258\t = Validation score   (-root_mean_squared_error)\n",
      "\t1060.23s\t = Training   runtime\n",
      "\t24.79s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 12/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 928.43s of the 2728.83s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S12F1 - S12F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2544376.2828\t = Validation score   (-root_mean_squared_error)\n",
      "\t635.9s\t = Training   runtime\n",
      "\t29.45s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 866.57s of the 2666.97s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S12F1 - S12F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2584447.1095\t = Validation score   (-root_mean_squared_error)\n",
      "\t526.45s\t = Training   runtime\n",
      "\t20.29s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 816.79s of the 2617.19s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S12F1 - S12F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2606698.5546\t = Validation score   (-root_mean_squared_error)\n",
      "\t1161.96s\t = Training   runtime\n",
      "\t27.13s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 13/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 711.35s of the 2511.73s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S13F1 - S13F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2543419.952\t = Validation score   (-root_mean_squared_error)\n",
      "\t688.24s\t = Training   runtime\n",
      "\t31.93s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 655.47s of the 2455.87s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S13F1 - S13F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2583432.6069\t = Validation score   (-root_mean_squared_error)\n",
      "\t567.89s\t = Training   runtime\n",
      "\t21.82s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 610.71s of the 2411.11s of remaining time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S13F1 - S13F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2606757.633\t = Validation score   (-root_mean_squared_error)\n",
      "\t1245.77s\t = Training   runtime\n",
      "\t29.24s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 14/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 523.33s of the 2323.73s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S14F1 - S14F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2544254.5666\t = Validation score   (-root_mean_squared_error)\n",
      "\t735.28s\t = Training   runtime\n",
      "\t34.34s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 472.8s of the 2273.2s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S14F1 - S14F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2583075.4615\t = Validation score   (-root_mean_squared_error)\n",
      "\t613.89s\t = Training   runtime\n",
      "\t23.52s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 423.42s of the 2223.82s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S14F1 - S14F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2608209.6612\t = Validation score   (-root_mean_squared_error)\n",
      "\t1324.43s\t = Training   runtime\n",
      "\t31.38s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 15/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 341.27s of the 2141.66s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S15F1 - S15F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-2544954.0612\t = Validation score   (-root_mean_squared_error)\n",
      "\t781.59s\t = Training   runtime\n",
      "\t36.46s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 291.47s of the 2091.86s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S15F1 - S15F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-2583306.4552\t = Validation score   (-root_mean_squared_error)\n",
      "\t662.26s\t = Training   runtime\n",
      "\t25.15s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 239.68s of the 2040.08s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S15F1 - S15F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "2022-10-03 23:54:00,416\tWARNING worker.py:1404 -- The log monitor on node XiaoLong failed with the following error:\n",
      "OSError: [WinError 87] Параметр задан неверно\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\log_monitor.py\", line 451, in <module>\n",
      "    log_monitor.run()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\log_monitor.py\", line 376, in run\n",
      "    self.open_closed_files()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\log_monitor.py\", line 218, in open_closed_files\n",
      "    self.close_all_files()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\log_monitor.py\", line 130, in close_all_files\n",
      "    os.kill(file_info.worker_pid, 0)\n",
      "SystemError: <class 'OSError'> returned a result with an error set\n",
      "\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t-2608271.6562\t = Validation score   (-root_mean_squared_error)\n",
      "\t1415.99s\t = Training   runtime\n",
      "\t33.61s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Not enough time left to finish repeated k-fold bagging, stopping early ...\n",
      "Completed 15/20 k-fold bagging repeats ...\n",
      "Loading: AutoGluonSber/\\models\\KNeighborsUnif_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\KNeighborsDist_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\RandomForestMSE_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\ExtraTreesMSE_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1944.8s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\WeightedEnsemble_L2\\utils\\model_template.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutoGluonSber/\\models\\WeightedEnsemble_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "Ensemble size: 95\n",
      "Ensemble weights: \n",
      "[0.         0.         0.84210526 0.         0.08421053 0.07368421\n",
      " 0.        ]\n",
      "\t0.0s\t= Estimated out-of-fold prediction time...\n",
      "Saving AutoGluonSber/\\models\\WeightedEnsemble_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "\t-2541513.4875\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tLightGBMXT_BAG_L2: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tLightGBM_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tRandomForestMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
      "\tExtraTreesMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n",
      "\tXGBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
      "\tNeuralNetTorch_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
      "\tLightGBMLarge_BAG_L2: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}}\n",
      "Fitting 9 L2 models ...\n",
      "Loading: AutoGluonSber/\\models\\KNeighborsUnif_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\KNeighborsDist_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\RandomForestMSE_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\ExtraTreesMSE_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1944.16s of the 1944.1s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-2601817.4758\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.93s\t = Training   runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1912.84s of the 1912.8s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-2642726.5645\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.9s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1891.84s of the 1891.8s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\RandomForestMSE_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\RandomForestMSE_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t3.96s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonSber/\\models\\RandomForestMSE_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\RandomForestMSE_BAG_L2\\model.pkl\n",
      "\t-2640544.9081\t = Validation score   (-root_mean_squared_error)\n",
      "\t332.86s\t = Training   runtime\n",
      "\t4.96s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 1552.03s of the 1551.99s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\CatBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\CatBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Consider decrease folds trained in parallel                              by passing num_fold_parallel to ag_args_ensemble                              when calling tabular.fit.\n",
      "                             If none working, use sequential folding by passing                              SequentialLocalFoldFittingStrategy to ag_args_ensemble                              when calling tabular.fit and try again.\n",
      "\tNot enough memory to train CatBoost_BAG_L2... Skipping this model.\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 1543.13s of the 1543.1s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\ExtraTreesMSE_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\ExtraTreesMSE_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t3.47s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonSber/\\models\\ExtraTreesMSE_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\ExtraTreesMSE_BAG_L2\\model.pkl\n",
      "\t-2603659.4483\t = Validation score   (-root_mean_squared_error)\n",
      "\t126.94s\t = Training   runtime\n",
      "\t4.65s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1410.52s of the 1410.48s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\NeuralNetFastAI_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\NeuralNetFastAI_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=9024, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py\", line 171, in _fit\n",
      "    try_import_fastai()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 191, in try_import_fastai\n",
      "    import autogluon.tabular.models.fastainn.imports_helper\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\imports_helper.py\", line 1, in <module>\n",
      "    from fastai.tabular.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\tabular\\all.py\", line 1, in <module>\n",
      "    from ..basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\basics.py\", line 1, in <module>\n",
      "    from .data.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\data\\all.py\", line 1, in <module>\n",
      "    from ..torch_basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\torch_basics.py\", line 1, in <module>\n",
      "    from torch import multiprocessing\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\cudnn_adv_train64_8.dll\" or one of its dependencies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 457, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 425, in after_all_folds_scheduled\n",
      "    time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\worker.py\", line 1831, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(OSError): \u001b[36mray::_ray_fit()\u001b[39m (pid=9024, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py\", line 171, in _fit\n",
      "    try_import_fastai()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 191, in try_import_fastai\n",
      "    import autogluon.tabular.models.fastainn.imports_helper\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\imports_helper.py\", line 1, in <module>\n",
      "    from fastai.tabular.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\tabular\\all.py\", line 1, in <module>\n",
      "    from ..basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\basics.py\", line 1, in <module>\n",
      "    from .data.all import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\data\\all.py\", line 1, in <module>\n",
      "    from ..torch_basics import *\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\torch_basics.py\", line 1, in <module>\n",
      "    from torch import multiprocessing\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\cudnn_adv_train64_8.dll\" or one of its dependencies.\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 1399.73s of the 1399.68s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=13152, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 121, in _fit\n",
      "    try_import_xgboost()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 167, in try_import_xgboost\n",
      "    import xgboost\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\xgboost\\__init__.py\", line 9, in <module>\n",
      "    from .core import DMatrix, DeviceQuantileDMatrix, Booster\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\xgboost\\core.py\", line 195, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\xgboost\\core.py\", line 178, in _load_lib\n",
      "    raise XGBoostError(\n",
      "xgboost.core.XGBoostError: XGBoost Library (xgboost.dll) could not be loaded.\n",
      "Likely causes:\n",
      "  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n",
      "  * You are running 32-bit Python on a 64-bit OS\n",
      "Error message(s): ['[WinError 1455] Файл подкачки слишком мал для завершения операции']\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 457, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 425, in after_all_folds_scheduled\n",
      "    time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\worker.py\", line 1831, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(XGBoostError): \u001b[36mray::_ray_fit()\u001b[39m (pid=13152, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 121, in _fit\n",
      "    try_import_xgboost()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 167, in try_import_xgboost\n",
      "    import xgboost\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\xgboost\\__init__.py\", line 9, in <module>\n",
      "    from .core import DMatrix, DeviceQuantileDMatrix, Booster\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\xgboost\\core.py\", line 195, in <module>\n",
      "    _LIB = _load_lib()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\xgboost\\core.py\", line 178, in _load_lib\n",
      "    raise XGBoostError(\n",
      "xgboost.core.XGBoostError: XGBoost Library (xgboost.dll) could not be loaded.\n",
      "Likely causes:\n",
      "  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n",
      "  * You are running 32-bit Python on a 64-bit OS\n",
      "Error message(s): ['[WinError 1455] Файл подкачки слишком мал для завершения операции']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1387.47s of the 1387.44s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\NeuralNetTorch_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\NeuralNetTorch_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=11236, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 142, in _fit\n",
      "    try_import_torch()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 199, in try_import_torch\n",
      "    import torch\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\cudnn_adv_infer64_8.dll\" or one of its dependencies.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 457, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 425, in after_all_folds_scheduled\n",
      "    time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\worker.py\", line 1831, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(OSError): \u001b[36mray::_ray_fit()\u001b[39m (pid=11236, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 142, in _fit\n",
      "    try_import_torch()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 199, in try_import_torch\n",
      "    import torch\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\cudnn_adv_infer64_8.dll\" or one of its dependencies.\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1376.89s of the 1376.86s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-2700393.4445\t = Validation score   (-root_mean_squared_error)\n",
      "\t48.69s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 2/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1320.43s of the 1320.39s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-2599597.2997\t = Validation score   (-root_mean_squared_error)\n",
      "\t45.46s\t = Training   runtime\n",
      "\t1.42s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1299.89s of the 1299.86s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-2645350.8782\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.75s\t = Training   runtime\n",
      "\t0.78s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1278.03s of the 1277.99s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-2681292.7315\t = Validation score   (-root_mean_squared_error)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t101.29s\t = Training   runtime\n",
      "\t1.57s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 3/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1222.18s of the 1222.15s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-2595430.4191\t = Validation score   (-root_mean_squared_error)\n",
      "\t67.39s\t = Training   runtime\n",
      "\t2.06s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1197.16s of the 1197.12s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-2635245.6971\t = Validation score   (-root_mean_squared_error)\n",
      "\t58.06s\t = Training   runtime\n",
      "\t1.2s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1172.86s of the 1172.82s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-2662865.8278\t = Validation score   (-root_mean_squared_error)\n",
      "\t152.88s\t = Training   runtime\n",
      "\t2.25s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 4/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1118.03s of the 1117.99s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-2600288.6338\t = Validation score   (-root_mean_squared_error)\n",
      "\t86.88s\t = Training   runtime\n",
      "\t2.73s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1095.46s of the 1095.42s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-2629448.9792\t = Validation score   (-root_mean_squared_error)\n",
      "\t78.97s\t = Training   runtime\n",
      "\t1.69s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1071.48s of the 1071.44s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-2660754.5967\t = Validation score   (-root_mean_squared_error)\n",
      "\t200.41s\t = Training   runtime\n",
      "\t2.97s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 5/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1020.73s of the 1020.69s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-2598044.7321\t = Validation score   (-root_mean_squared_error)\n",
      "\t109.94s\t = Training   runtime\n",
      "\t3.4s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 994.56s of the 994.52s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-2629154.2407\t = Validation score   (-root_mean_squared_error)\n",
      "\t98.49s\t = Training   runtime\n",
      "\t2.09s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 971.99s of the 971.95s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S5F1 - S5F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-2655313.4907\t = Validation score   (-root_mean_squared_error)\n",
      "\t246.86s\t = Training   runtime\n",
      "\t3.61s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 6/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 922.32s of the 922.29s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-2597013.2766\t = Validation score   (-root_mean_squared_error)\n",
      "\t135.23s\t = Training   runtime\n",
      "\t4.16s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 893.73s of the 893.69s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-2629311.6831\t = Validation score   (-root_mean_squared_error)\n",
      "\t116.64s\t = Training   runtime\n",
      "\t2.5s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 872.63s of the 872.58s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S6F1 - S6F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-2657189.9335\t = Validation score   (-root_mean_squared_error)\n",
      "\t293.68s\t = Training   runtime\n",
      "\t4.34s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 7/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 822.53s of the 822.49s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-2595321.7429\t = Validation score   (-root_mean_squared_error)\n",
      "\t160.56s\t = Training   runtime\n",
      "\t4.91s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 793.95s of the 793.91s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-2628312.0307\t = Validation score   (-root_mean_squared_error)\n",
      "\t135.84s\t = Training   runtime\n",
      "\t2.91s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 771.77s of the 771.73s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S7F1 - S7F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-2651873.5422\t = Validation score   (-root_mean_squared_error)\n",
      "\t338.98s\t = Training   runtime\n",
      "\t5.03s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 8/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 723.18s of the 723.14s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S8F1 - S8F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-2595299.0704\t = Validation score   (-root_mean_squared_error)\n",
      "\t185.87s\t = Training   runtime\n",
      "\t5.66s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 694.67s of the 694.64s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S8F1 - S8F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-2627336.9984\t = Validation score   (-root_mean_squared_error)\n",
      "\t155.8s\t = Training   runtime\n",
      "\t3.36s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 671.71s of the 671.67s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S8F1 - S8F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-2648741.4469\t = Validation score   (-root_mean_squared_error)\n",
      "\t388.83s\t = Training   runtime\n",
      "\t5.7s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 9/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 618.63s of the 618.6s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S9F1 - S9F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-2596412.2846\t = Validation score   (-root_mean_squared_error)\n",
      "\t208.61s\t = Training   runtime\n",
      "\t6.35s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 592.79s of the 592.75s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S9F1 - S9F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-2627209.7036\t = Validation score   (-root_mean_squared_error)\n",
      "\t175.77s\t = Training   runtime\n",
      "\t3.72s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 569.84s of the 569.8s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S9F1 - S9F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-2647560.8012\t = Validation score   (-root_mean_squared_error)\n",
      "\t442.37s\t = Training   runtime\n",
      "\t6.46s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 10/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 513.05s of the 513.01s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S10F1 - S10F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-2597433.3209\t = Validation score   (-root_mean_squared_error)\n",
      "\t227.94s\t = Training   runtime\n",
      "\t6.93s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 490.64s of the 490.61s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S10F1 - S10F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-2626755.4242\t = Validation score   (-root_mean_squared_error)\n",
      "\t197.76s\t = Training   runtime\n",
      "\t4.19s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 465.64s of the 465.61s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S10F1 - S10F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-2648780.8607\t = Validation score   (-root_mean_squared_error)\n",
      "\t490.24s\t = Training   runtime\n",
      "\t7.2s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 11/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 414.6s of the 414.57s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S11F1 - S11F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-2595377.7064\t = Validation score   (-root_mean_squared_error)\n",
      "\t250.17s\t = Training   runtime\n",
      "\t7.77s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 389.22s of the 389.19s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S11F1 - S11F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-2624495.6735\t = Validation score   (-root_mean_squared_error)\n",
      "\t216.85s\t = Training   runtime\n",
      "\t4.59s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 367.04s of the 366.99s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S11F1 - S11F8) | Fitting with ParallelLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-2645260.7955\t = Validation score   (-root_mean_squared_error)\n",
      "\t535.91s\t = Training   runtime\n",
      "\t7.92s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 12/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 318.17s of the 318.13s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S12F1 - S12F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-2593731.9499\t = Validation score   (-root_mean_squared_error)\n",
      "\t271.41s\t = Training   runtime\n",
      "\t8.43s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 293.75s of the 293.71s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S12F1 - S12F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-2622913.339\t = Validation score   (-root_mean_squared_error)\n",
      "\t235.96s\t = Training   runtime\n",
      "\t5.04s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 271.61s of the 271.57s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S12F1 - S12F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-2642985.0098\t = Validation score   (-root_mean_squared_error)\n",
      "\t582.5s\t = Training   runtime\n",
      "\t8.64s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 13/20\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 221.81s of the 221.77s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S13F1 - S13F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-2592769.2288\t = Validation score   (-root_mean_squared_error)\n",
      "\t296.04s\t = Training   runtime\n",
      "\t9.17s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 193.92s of the 193.88s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S13F1 - S13F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-2623920.7103\t = Validation score   (-root_mean_squared_error)\n",
      "\t254.35s\t = Training   runtime\n",
      "\t5.53s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 172.46s of the 172.42s of remaining time.\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S13F1 - S13F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "\t-2641468.7811\t = Validation score   (-root_mean_squared_error)\n",
      "\t631.23s\t = Training   runtime\n",
      "\t9.38s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Not enough time left to finish repeated k-fold bagging, stopping early ...\n",
      "Completed 13/20 k-fold bagging repeats ...\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\RandomForestMSE_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\ExtraTreesMSE_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\utils\\oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L3: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 120.47s of remaining time.\n",
      "Saving AutoGluonSber/\\models\\WeightedEnsemble_L3\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSber/\\models\\WeightedEnsemble_L3\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "Ensemble size: 100\n",
      "Ensemble weights: \n",
      "[0.57 0.   0.01 0.42 0.  ]\n",
      "\t0.0s\t= Estimated out-of-fold prediction time...\n",
      "Saving AutoGluonSber/\\models\\WeightedEnsemble_L3\\utils\\oof.pkl\n",
      "Saving AutoGluonSber/\\models\\WeightedEnsemble_L3\\model.pkl\n",
      "\t-2578387.5523\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "AutoGluon training complete, total runtime = 5280.32s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Loading: AutoGluonSber/\\models\\trainer.pkl\n",
      "Saving AutoGluonSber/\\models\\trainer.pkl\n",
      "Saving AutoGluonSber/\\learner.pkl\n",
      "Saving AutoGluonSber/\\predictor.pkl\n",
      "Saving AutoGluonSber/\\__version__ with contents \"0.5.2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutoGluonSber/\\\")\n",
      "Loading: AutoGluonSber/\\models\\KNeighborsUnif_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\KNeighborsDist_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\RandomForestMSE_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\ExtraTreesMSE_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\WeightedEnsemble_L2\\model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\RandomForestMSE_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\ExtraTreesMSE_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMLarge_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\WeightedEnsemble_L3\\model.pkl\n",
      "D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:138: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                     model     score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L2 -2.541513e+06      45.573395  1097.056351                0.000000           0.552828            2       True          8\n",
      "1        LightGBMXT_BAG_L1 -2.544954e+06      36.460903   781.587556               36.460903         781.587556            1       True          3\n",
      "2      WeightedEnsemble_L3 -2.578388e+06     180.606732  3931.677097                0.000000           0.459125            3       True         14\n",
      "3          LightGBM_BAG_L1 -2.583306e+06      25.151915   662.256418               25.151915         662.256418            1       True          4\n",
      "4        LightGBMXT_BAG_L2 -2.592769e+06     170.989487  3471.416218                9.165503         296.035949            2       True          9\n",
      "5     ExtraTreesMSE_BAG_L2 -2.603659e+06     166.476976  3302.323584                4.652992         126.943314            2       True         12\n",
      "6     LightGBMLarge_BAG_L1 -2.608272e+06      33.606518  1415.993124               33.606518        1415.993124            1       True          7\n",
      "7          LightGBM_BAG_L2 -2.623921e+06     167.349183  3429.729520                5.525199         254.349250            2       True         10\n",
      "8   RandomForestMSE_BAG_L2 -2.640545e+06     166.788237  3508.238710                4.964253         332.858440            2       True         11\n",
      "9     LightGBMLarge_BAG_L2 -2.641469e+06     171.204218  3806.605460                9.380234         631.225190            2       True         13\n",
      "10    ExtraTreesMSE_BAG_L1 -2.645660e+06       4.456810    96.548375                4.456810          96.548375            1       True          6\n",
      "11  RandomForestMSE_BAG_L1 -2.675103e+06       4.655681   218.367592                4.655681         218.367592            1       True          5\n",
      "12   KNeighborsUnif_BAG_L1 -5.138587e+06      28.790065     0.316677               28.790065           0.316677            1       True          1\n",
      "13   KNeighborsDist_BAG_L1 -5.140267e+06      28.702092     0.310527               28.702092           0.310527            1       True          2\n",
      "Number of models trained: 14\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'StackerEnsembleModel_XT', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_RF', 'StackerEnsembleModel_LGB'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])             :   2 | ['sub_area', 'ecology']\n",
      "('float', [])                : 119 | ['life_sq', 'floor', 'max_floor', 'material', 'build_year', ...]\n",
      "('int', [])                  : 152 | ['id', 'full_sq', 'raion_popul', 'children_preschool', 'preschool_education_centers_raion', ...]\n",
      "('int', ['bool'])            :  17 | ['product_type', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', ...]\n",
      "('int', ['datetime_as_int']) :   5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutoGluonSber/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\ExtraTreesMSE_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSber/\\models\\RandomForestMSE_BAG_L1\\model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "label = 'price_doc'  # name of target variable to predict in this competition\n",
    "eval_metric = 'rmse'  # Optional: specify that competition evaluation metric is AUC\n",
    "save_path = 'AutoGluonSber/'  # where to store trained models\n",
    "\n",
    "train_data = pd.read_csv('train_sber.csv', low_memory=False)\n",
    "predictor = TabularPredictor(label=label, eval_metric=eval_metric, path=save_path, verbosity=3).fit(\n",
    "    train_data, presets='best_quality', time_limit=5400\n",
    ")\n",
    "\n",
    "results = predictor.fit_summary()\n",
    "test_data = pd.read_csv('test_sber.csv', low_memory=False)\n",
    "y_predproba = predictor.predict_proba(test_data)\n",
    "\n",
    "test_data['price_doc'] = y_predproba\n",
    "test_data[['id', 'price_doc']].to_csv('sber_autogluon.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a636df4",
   "metadata": {},
   "source": [
    "# Santander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9879f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutoGluonSantander/\"\n",
      "Presets specified: ['best_quality']\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'auto_stack': True}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': None,\n",
      " 'auto_stack': True,\n",
      " 'calibrate': 'auto',\n",
      " 'excluded_model_types': None,\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'keep_only_best': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': None,\n",
      " 'num_bag_sets': None,\n",
      " 'num_stack_levels': None,\n",
      " 'pseudo_data': None,\n",
      " 'quantile_levels': None,\n",
      " 'refit_full': False,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 3}\n",
      "========================================\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=20\n",
      "Saving AutoGluonSantander/\\learner.pkl\n",
      "Saving AutoGluonSantander/\\predictor.pkl\n",
      "Beginning AutoGluon training ... Time limit = 5400s\n",
      "AutoGluon will save models to \"AutoGluonSantander/\\\"\n",
      "AutoGluon Version:  0.5.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Train Data Rows:    76020\n",
      "Train Data Columns: 370\n",
      "Label Column: TARGET\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    10173.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 225.02 MB (2.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 105 features to boolean dtype as they only contain 2 unique values.\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float64', 'float') : 111 | ['imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', ...]\n",
      "\t\t\t\t('int64', 'int')     : 225 | ['ID', 'var3', 'var15', 'ind_var1_0', 'ind_var1', ...]\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 111 | ['imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', ...]\n",
      "\t\t\t\t('int', [])   : 225 | ['ID', 'var3', 'var15', 'ind_var1_0', 'ind_var1', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 109 | ['imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', ...]\n",
      "\t\t\t\t('int', [])       : 122 | ['ID', 'var3', 'var15', 'num_var1_0', 'num_var1', ...]\n",
      "\t\t\t\t('int', ['bool']) : 105 | ['ind_var1_0', 'ind_var1', 'ind_var5_0', 'ind_var5', 'ind_var6_0', ...]\n",
      "\t\t\t3.7s = Fit runtime\n",
      "\t\t\t336 features in original data used to generate 336 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 109 | ['imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', ...]\n",
      "\t\t\t\t('int', [])       : 122 | ['ID', 'var3', 'var15', 'num_var1_0', 'num_var1', ...]\n",
      "\t\t\t\t('int', ['bool']) : 105 | ['ind_var1_0', 'ind_var1', 'ind_var5_0', 'ind_var5', 'ind_var6_0', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 109 | ['imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', ...]\n",
      "\t\t\t\t('int', [])       : 122 | ['ID', 'var3', 'var15', 'num_var1_0', 'num_var1', ...]\n",
      "\t\t\t\t('int', ['bool']) : 105 | ['ind_var1_0', 'ind_var1', 'ind_var5_0', 'ind_var5', 'ind_var6_0', ...]\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t336 features in original data used to generate 336 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 109 | ['imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', ...]\n",
      "\t\t\t\t('int', [])       : 122 | ['ID', 'var3', 'var15', 'num_var1_0', 'num_var1', ...]\n",
      "\t\t\t\t('int', ['bool']) : 105 | ['ind_var1_0', 'ind_var1', 'ind_var5_0', 'ind_var5', 'ind_var6_0', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 109 | ['imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', ...]\n",
      "\t\t\t\t('int', [])       : 122 | ['ID', 'var3', 'var15', 'num_var1_0', 'num_var1', ...]\n",
      "\t\t\t\t('int', ['bool']) : 105 | ['ind_var1_0', 'ind_var1', 'ind_var5_0', 'ind_var5', 'ind_var6_0', ...]\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t336 features in original data used to generate 336 features in processed data.\n",
      "\t\tSkipping CategoryFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 109 | ['imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', ...]\n",
      "\t\t\t\t('int', [])       : 122 | ['ID', 'var3', 'var15', 'num_var1_0', 'num_var1', ...]\n",
      "\t\t\t\t('int', ['bool']) : 105 | ['ind_var1_0', 'ind_var1', 'ind_var5_0', 'ind_var5', 'ind_var6_0', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 109 | ['imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', ...]\n",
      "\t\t\t\t('int', [])       : 122 | ['ID', 'var3', 'var15', 'num_var1_0', 'num_var1', ...]\n",
      "\t\t\t\t('int', ['bool']) : 105 | ['ind_var1_0', 'ind_var1', 'ind_var5_0', 'ind_var5', 'ind_var6_0', ...]\n",
      "\t\t\t0.3s = Fit runtime\n",
      "\t\t\t336 features in original data used to generate 336 features in processed data.\n",
      "\tUseless Original Features (Count: 34): ['ind_var2_0', 'ind_var2', 'ind_var27_0', 'ind_var28_0', 'ind_var28', 'ind_var27', 'ind_var41', 'ind_var46_0', 'ind_var46', 'num_var27_0', 'num_var28_0', 'num_var28', 'num_var27', 'num_var41', 'num_var46_0', 'num_var46', 'saldo_var28', 'saldo_var27', 'saldo_var41', 'saldo_var46', 'imp_amort_var18_hace3', 'imp_amort_var34_hace3', 'imp_reemb_var13_hace3', 'imp_reemb_var33_hace3', 'imp_trasp_var17_out_hace3', 'imp_trasp_var33_out_hace3', 'num_var2_0_ult1', 'num_var2_ult1', 'num_reemb_var13_hace3', 'num_reemb_var33_hace3', 'num_trasp_var17_out_hace3', 'num_trasp_var33_out_hace3', 'saldo_var2_ult1', 'saldo_medio_var13_medio_hace3']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('float64', 'float') : 111 | ['imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', ...]\n",
      "\t\t('int64', 'int')     : 225 | ['ID', 'var3', 'var15', 'ind_var1_0', 'ind_var1', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 111 | ['imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', ...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t('int', [])   : 225 | ['ID', 'var3', 'var15', 'ind_var1_0', 'ind_var1', ...]\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('float64', 'float') : 109 | ['imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', ...]\n",
      "\t\t('int64', 'int')     : 122 | ['ID', 'var3', 'var15', 'num_var1_0', 'num_var1', ...]\n",
      "\t\t('int8', 'int')      : 105 | ['ind_var1_0', 'ind_var1', 'ind_var5_0', 'ind_var5', 'ind_var6_0', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 109 | ['imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', ...]\n",
      "\t\t('int', [])       : 122 | ['ID', 'var3', 'var15', 'num_var1_0', 'num_var1', ...]\n",
      "\t\t('int', ['bool']) : 105 | ['ind_var1_0', 'ind_var1', 'ind_var5_0', 'ind_var5', 'ind_var6_0', ...]\n",
      "\t4.8s = Fit runtime\n",
      "\t336 features in original data used to generate 336 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 148.47 MB (1.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving AutoGluonSantander/\\learner.pkl\n",
      "Saving AutoGluonSantander/\\utils\\data\\X.pkl\n",
      "Saving AutoGluonSantander/\\utils\\data\\y.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tKNeighborsUnif_BAG_L1: \t{'weights': 'uniform', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Unif', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tKNeighborsDist_BAG_L1: \t{'weights': 'distance', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Dist', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBMXT_BAG_L1: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tLightGBM_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tRandomForestGini_BAG_L1: \t{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tRandomForestEntr_BAG_L1: \t{'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
      "\tExtraTreesGini_BAG_L1: \t{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tExtraTreesEntr_BAG_L1: \t{'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n",
      "\tXGBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
      "\tNeuralNetTorch_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
      "\tLightGBMLarge_BAG_L1: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 5394.89s of the 5394.87s of remaining time.\n",
      "Saving AutoGluonSantander/\\models\\KNeighborsUnif_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\KNeighborsUnif_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t0.06s \t= Train Time (Using 10000/76020 rows) (5394.72s remaining time)\n",
      "\t0.05s \t= Train Time (Using 20000/76020 rows) (5394.67s remaining time)\n",
      "\t0.11s \t= Train Time (Using 40000/76020 rows) (5394.56s remaining time)\n",
      "\t0.02s \t= Train Time (Using 76020/76020 rows) (5394.54s remaining time)\n",
      "\t131.25s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonSantander/\\models\\KNeighborsUnif_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\KNeighborsUnif_BAG_L1\\model.pkl\n",
      "\t0.5288\t = Validation score   (roc_auc)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t141.04s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 5250.14s of the 5250.12s of remaining time.\n",
      "Saving AutoGluonSantander/\\models\\KNeighborsDist_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\KNeighborsDist_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t0.03s \t= Train Time (Using 10000/76020 rows) (5250.01s remaining time)\n",
      "\t0.07s \t= Train Time (Using 20000/76020 rows) (5249.94s remaining time)\n",
      "\t0.12s \t= Train Time (Using 40000/76020 rows) (5249.82s remaining time)\n",
      "\t0.02s \t= Train Time (Using 76020/76020 rows) (5249.8s remaining time)\n",
      "\t121.54s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonSantander/\\models\\KNeighborsDist_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\KNeighborsDist_BAG_L1\\model.pkl\n",
      "\t0.5297\t = Validation score   (roc_auc)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t140.4s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5106.1s of the 5106.07s of remaining time.\n",
      "Saving AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138597\n",
      "[100]\tvalid_set's binary_logloss: 0.13599\n",
      "[150]\tvalid_set's binary_logloss: 0.135457\n",
      "[200]\tvalid_set's binary_logloss: 0.135347\n",
      "[250]\tvalid_set's binary_logloss: 0.135389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.13705\n",
      "[100]\tvalid_set's binary_logloss: 0.1339\n",
      "[150]\tvalid_set's binary_logloss: 0.133163\n",
      "[200]\tvalid_set's binary_logloss: 0.132895\n",
      "[250]\tvalid_set's binary_logloss: 0.132588\n",
      "[300]\tvalid_set's binary_logloss: 0.13251\n",
      "[350]\tvalid_set's binary_logloss: 0.132888\n",
      "[400]\tvalid_set's binary_logloss: 0.132866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138299\n",
      "[100]\tvalid_set's binary_logloss: 0.136635\n",
      "[150]\tvalid_set's binary_logloss: 0.13675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.13778\n",
      "[100]\tvalid_set's binary_logloss: 0.135088\n",
      "[150]\tvalid_set's binary_logloss: 0.134706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.134659\n",
      "[100]\tvalid_set's binary_logloss: 0.13194\n",
      "[150]\tvalid_set's binary_logloss: 0.131082\n",
      "[200]\tvalid_set's binary_logloss: 0.131134\n",
      "[250]\tvalid_set's binary_logloss: 0.130956\n",
      "[300]\tvalid_set's binary_logloss: 0.130939\n",
      "[350]\tvalid_set's binary_logloss: 0.13104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.1397\n",
      "[100]\tvalid_set's binary_logloss: 0.136713\n",
      "[150]\tvalid_set's binary_logloss: 0.135852\n",
      "[200]\tvalid_set's binary_logloss: 0.135202\n",
      "[250]\tvalid_set's binary_logloss: 0.135192\n",
      "[300]\tvalid_set's binary_logloss: 0.135172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138211\n",
      "[100]\tvalid_set's binary_logloss: 0.134299\n",
      "[150]\tvalid_set's binary_logloss: 0.13357\n",
      "[200]\tvalid_set's binary_logloss: 0.133584\n",
      "[250]\tvalid_set's binary_logloss: 0.133562\n",
      "[300]\tvalid_set's binary_logloss: 0.133799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138468\n",
      "[100]\tvalid_set's binary_logloss: 0.137499\n",
      "[150]\tvalid_set's binary_logloss: 0.137512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t0.8373\t = Validation score   (roc_auc)\n",
      "\t18.18s\t = Training   runtime\n",
      "\t0.63s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 5086.23s of the 5086.21s of remaining time.\n",
      "Saving AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135312\n",
      "[100]\tvalid_set's binary_logloss: 0.134532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.132997\n",
      "[100]\tvalid_set's binary_logloss: 0.131918\n",
      "[150]\tvalid_set's binary_logloss: 0.131988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.136313\n",
      "[100]\tvalid_set's binary_logloss: 0.135998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.136269\n",
      "[100]\tvalid_set's binary_logloss: 0.135813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.131138\n",
      "[100]\tvalid_set's binary_logloss: 0.130459\n",
      "[150]\tvalid_set's binary_logloss: 0.130564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135158\n",
      "[100]\tvalid_set's binary_logloss: 0.133979\n",
      "[150]\tvalid_set's binary_logloss: 0.13397\n",
      "[200]\tvalid_set's binary_logloss: 0.13429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.134748\n",
      "[100]\tvalid_set's binary_logloss: 0.134036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138181\n",
      "[100]\tvalid_set's binary_logloss: 0.137954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t0.8386\t = Validation score   (roc_auc)\n",
      "\t13.65s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 5071.33s of the 5071.32s of remaining time.\n",
      "Saving AutoGluonSantander/\\models\\RandomForestGini_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\RandomForestGini_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t9.33s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonSantander/\\models\\RandomForestGini_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\RandomForestGini_BAG_L1\\model.pkl\n",
      "\t0.7831\t = Validation score   (roc_auc)\n",
      "\t24.61s\t = Training   runtime\n",
      "\t9.53s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 5036.61s of the 5036.59s of remaining time.\n",
      "Saving AutoGluonSantander/\\models\\RandomForestEntr_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\RandomForestEntr_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t8.65s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonSantander/\\models\\RandomForestEntr_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\RandomForestEntr_BAG_L1\\model.pkl\n",
      "\t0.7833\t = Validation score   (roc_auc)\n",
      "\t23.41s\t = Training   runtime\n",
      "\t9.59s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 5003.07s of the 5003.05s of remaining time.\n",
      "Saving AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\xgboost\\compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6171424\ttest: 0.6171950\tbest: 0.6171950 (0)\ttotal: 173ms\tremaining: 28m 52s\n",
      "20:\tlearn: 0.1806080\ttest: 0.1807542\tbest: 0.1807542 (20)\ttotal: 637ms\tremaining: 5m 2s\n",
      "40:\tlearn: 0.1422727\ttest: 0.1436868\tbest: 0.1436868 (40)\ttotal: 1.17s\tremaining: 4m 45s\n",
      "60:\tlearn: 0.1363215\ttest: 0.1383493\tbest: 0.1383493 (60)\ttotal: 1.71s\tremaining: 4m 38s\n",
      "80:\tlearn: 0.1342470\ttest: 0.1367241\tbest: 0.1367231 (79)\ttotal: 2.22s\tremaining: 4m 32s\n",
      "100:\tlearn: 0.1335281\ttest: 0.1362655\tbest: 0.1362655 (100)\ttotal: 2.69s\tremaining: 4m 23s\n",
      "120:\tlearn: 0.1328439\ttest: 0.1358195\tbest: 0.1358195 (120)\ttotal: 3.17s\tremaining: 4m 18s\n",
      "140:\tlearn: 0.1322115\ttest: 0.1355141\tbest: 0.1355115 (139)\ttotal: 3.64s\tremaining: 4m 14s\n",
      "160:\tlearn: 0.1315592\ttest: 0.1352267\tbest: 0.1352265 (159)\ttotal: 4.12s\tremaining: 4m 11s\n",
      "180:\tlearn: 0.1308734\ttest: 0.1349727\tbest: 0.1349727 (180)\ttotal: 4.59s\tremaining: 4m 9s\n",
      "200:\tlearn: 0.1304108\ttest: 0.1348381\tbest: 0.1348381 (200)\ttotal: 5.07s\tremaining: 4m 7s\n",
      "220:\tlearn: 0.1298906\ttest: 0.1346720\tbest: 0.1346552 (217)\ttotal: 5.53s\tremaining: 4m 4s\n",
      "240:\tlearn: 0.1292700\ttest: 0.1346318\tbest: 0.1346279 (238)\ttotal: 5.97s\tremaining: 4m 1s\n",
      "260:\tlearn: 0.1287921\ttest: 0.1345008\tbest: 0.1345008 (260)\ttotal: 6.41s\tremaining: 3m 59s\n",
      "280:\tlearn: 0.1282928\ttest: 0.1343537\tbest: 0.1343430 (276)\ttotal: 6.85s\tremaining: 3m 57s\n",
      "300:\tlearn: 0.1278820\ttest: 0.1343063\tbest: 0.1342993 (299)\ttotal: 7.3s\tremaining: 3m 55s\n",
      "320:\tlearn: 0.1275034\ttest: 0.1343292\tbest: 0.1342787 (311)\ttotal: 7.74s\tremaining: 3m 53s\n",
      "340:\tlearn: 0.1270548\ttest: 0.1343605\tbest: 0.1342787 (311)\ttotal: 8.2s\tremaining: 3m 52s\n",
      "360:\tlearn: 0.1265941\ttest: 0.1343520\tbest: 0.1342787 (311)\ttotal: 8.67s\tremaining: 3m 51s\n",
      "380:\tlearn: 0.1262478\ttest: 0.1343869\tbest: 0.1342787 (311)\ttotal: 9.13s\tremaining: 3m 50s\n",
      "400:\tlearn: 0.1258171\ttest: 0.1343440\tbest: 0.1342787 (311)\ttotal: 9.59s\tremaining: 3m 49s\n",
      "420:\tlearn: 0.1254915\ttest: 0.1343641\tbest: 0.1342787 (311)\ttotal: 10s\tremaining: 3m 47s\n",
      "\n",
      "bestTest = 0.1342787051\n",
      "bestIteration = 311\n",
      "\n",
      "Shrink model to first 312 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6223185\ttest: 0.6223144\tbest: 0.6223144 (0)\ttotal: 23ms\tremaining: 3m 49s\n",
      "20:\tlearn: 0.1797675\ttest: 0.1798624\tbest: 0.1798624 (20)\ttotal: 490ms\tremaining: 3m 52s\n",
      "40:\tlearn: 0.1423584\ttest: 0.1423110\tbest: 0.1423110 (40)\ttotal: 1000ms\tremaining: 4m 2s\n",
      "60:\tlearn: 0.1364032\ttest: 0.1367781\tbest: 0.1367781 (60)\ttotal: 1.49s\tremaining: 4m 3s\n",
      "80:\tlearn: 0.1344960\ttest: 0.1352875\tbest: 0.1352875 (80)\ttotal: 1.96s\tremaining: 4m\n",
      "100:\tlearn: 0.1336972\ttest: 0.1348563\tbest: 0.1348563 (100)\ttotal: 2.4s\tremaining: 3m 55s\n",
      "120:\tlearn: 0.1328348\ttest: 0.1342680\tbest: 0.1342680 (120)\ttotal: 2.85s\tremaining: 3m 53s\n",
      "140:\tlearn: 0.1323147\ttest: 0.1340467\tbest: 0.1340467 (140)\ttotal: 3.29s\tremaining: 3m 49s\n",
      "160:\tlearn: 0.1314877\ttest: 0.1336794\tbest: 0.1336794 (160)\ttotal: 3.77s\tremaining: 3m 50s\n",
      "180:\tlearn: 0.1310082\ttest: 0.1334948\tbest: 0.1334948 (180)\ttotal: 4.22s\tremaining: 3m 49s\n",
      "200:\tlearn: 0.1304578\ttest: 0.1332055\tbest: 0.1332055 (200)\ttotal: 4.69s\tremaining: 3m 48s\n",
      "220:\tlearn: 0.1298682\ttest: 0.1329658\tbest: 0.1329658 (220)\ttotal: 5.13s\tremaining: 3m 47s\n",
      "240:\tlearn: 0.1292997\ttest: 0.1327637\tbest: 0.1327637 (240)\ttotal: 5.59s\tremaining: 3m 46s\n",
      "260:\tlearn: 0.1287434\ttest: 0.1326531\tbest: 0.1326418 (255)\ttotal: 6.05s\tremaining: 3m 45s\n",
      "280:\tlearn: 0.1282873\ttest: 0.1325494\tbest: 0.1325494 (280)\ttotal: 6.53s\tremaining: 3m 45s\n",
      "300:\tlearn: 0.1279391\ttest: 0.1325168\tbest: 0.1325153 (289)\ttotal: 6.96s\tremaining: 3m 44s\n",
      "320:\tlearn: 0.1275679\ttest: 0.1324272\tbest: 0.1324203 (319)\ttotal: 7.42s\tremaining: 3m 43s\n",
      "340:\tlearn: 0.1271708\ttest: 0.1323539\tbest: 0.1323539 (340)\ttotal: 7.95s\tremaining: 3m 45s\n",
      "360:\tlearn: 0.1267820\ttest: 0.1323134\tbest: 0.1323100 (354)\ttotal: 8.44s\tremaining: 3m 45s\n",
      "380:\tlearn: 0.1264252\ttest: 0.1323029\tbest: 0.1322914 (374)\ttotal: 8.89s\tremaining: 3m 44s\n",
      "400:\tlearn: 0.1260704\ttest: 0.1322246\tbest: 0.1322131 (397)\ttotal: 9.33s\tremaining: 3m 43s\n",
      "420:\tlearn: 0.1257829\ttest: 0.1322429\tbest: 0.1322084 (413)\ttotal: 9.78s\tremaining: 3m 42s\n",
      "440:\tlearn: 0.1253810\ttest: 0.1321761\tbest: 0.1321735 (439)\ttotal: 10.2s\tremaining: 3m 42s\n",
      "460:\tlearn: 0.1249517\ttest: 0.1320541\tbest: 0.1320541 (460)\ttotal: 10.7s\tremaining: 3m 41s\n",
      "480:\tlearn: 0.1246175\ttest: 0.1320082\tbest: 0.1319980 (477)\ttotal: 11.1s\tremaining: 3m 40s\n",
      "500:\tlearn: 0.1242305\ttest: 0.1319673\tbest: 0.1319673 (500)\ttotal: 11.7s\tremaining: 3m 41s\n",
      "520:\tlearn: 0.1238412\ttest: 0.1319024\tbest: 0.1318941 (517)\ttotal: 12.2s\tremaining: 3m 41s\n",
      "540:\tlearn: 0.1234259\ttest: 0.1318671\tbest: 0.1318671 (540)\ttotal: 12.7s\tremaining: 3m 41s\n",
      "560:\tlearn: 0.1230824\ttest: 0.1318612\tbest: 0.1318304 (548)\ttotal: 13.2s\tremaining: 3m 41s\n",
      "580:\tlearn: 0.1226942\ttest: 0.1319217\tbest: 0.1318304 (548)\ttotal: 13.7s\tremaining: 3m 41s\n",
      "600:\tlearn: 0.1223429\ttest: 0.1318980\tbest: 0.1318304 (548)\ttotal: 14.2s\tremaining: 3m 41s\n",
      "620:\tlearn: 0.1219974\ttest: 0.1318669\tbest: 0.1318304 (548)\ttotal: 14.6s\tremaining: 3m 40s\n",
      "640:\tlearn: 0.1216289\ttest: 0.1319142\tbest: 0.1318304 (548)\ttotal: 15.1s\tremaining: 3m 40s\n",
      "660:\tlearn: 0.1212560\ttest: 0.1319519\tbest: 0.1318304 (548)\ttotal: 15.5s\tremaining: 3m 39s\n",
      "680:\tlearn: 0.1209517\ttest: 0.1319198\tbest: 0.1318304 (548)\ttotal: 16s\tremaining: 3m 39s\n",
      "700:\tlearn: 0.1206114\ttest: 0.1319377\tbest: 0.1318304 (548)\ttotal: 16.5s\tremaining: 3m 38s\n",
      "720:\tlearn: 0.1201824\ttest: 0.1319159\tbest: 0.1318304 (548)\ttotal: 16.9s\tremaining: 3m 37s\n",
      "\n",
      "bestTest = 0.1318304392\n",
      "bestIteration = 548\n",
      "\n",
      "Shrink model to first 549 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6176021\ttest: 0.6177398\tbest: 0.6177398 (0)\ttotal: 23.5ms\tremaining: 3m 55s\n",
      "20:\tlearn: 0.1762613\ttest: 0.1765967\tbest: 0.1765967 (20)\ttotal: 469ms\tremaining: 3m 43s\n",
      "40:\tlearn: 0.1423484\ttest: 0.1434558\tbest: 0.1434558 (40)\ttotal: 945ms\tremaining: 3m 49s\n",
      "60:\tlearn: 0.1359192\ttest: 0.1381971\tbest: 0.1381971 (60)\ttotal: 1.45s\tremaining: 3m 55s\n",
      "80:\tlearn: 0.1340612\ttest: 0.1370823\tbest: 0.1370823 (80)\ttotal: 1.92s\tremaining: 3m 55s\n",
      "100:\tlearn: 0.1330836\ttest: 0.1365923\tbest: 0.1365835 (99)\ttotal: 2.37s\tremaining: 3m 51s\n",
      "120:\tlearn: 0.1324099\ttest: 0.1363064\tbest: 0.1363064 (120)\ttotal: 2.79s\tremaining: 3m 47s\n",
      "140:\tlearn: 0.1319081\ttest: 0.1361966\tbest: 0.1361777 (133)\ttotal: 3.21s\tremaining: 3m 44s\n",
      "160:\tlearn: 0.1312733\ttest: 0.1359196\tbest: 0.1359196 (160)\ttotal: 3.67s\tremaining: 3m 44s\n",
      "180:\tlearn: 0.1306765\ttest: 0.1357789\tbest: 0.1357789 (180)\ttotal: 4.11s\tremaining: 3m 42s\n",
      "200:\tlearn: 0.1300637\ttest: 0.1356589\tbest: 0.1356540 (194)\ttotal: 4.54s\tremaining: 3m 41s\n",
      "220:\tlearn: 0.1296214\ttest: 0.1355486\tbest: 0.1355300 (216)\ttotal: 4.97s\tremaining: 3m 40s\n",
      "240:\tlearn: 0.1290555\ttest: 0.1354779\tbest: 0.1354779 (240)\ttotal: 5.44s\tremaining: 3m 40s\n",
      "260:\tlearn: 0.1285823\ttest: 0.1354312\tbest: 0.1354312 (260)\ttotal: 5.87s\tremaining: 3m 39s\n",
      "280:\tlearn: 0.1280871\ttest: 0.1353728\tbest: 0.1353728 (280)\ttotal: 6.31s\tremaining: 3m 38s\n",
      "300:\tlearn: 0.1276257\ttest: 0.1353340\tbest: 0.1353295 (287)\ttotal: 6.74s\tremaining: 3m 37s\n",
      "320:\tlearn: 0.1271592\ttest: 0.1353369\tbest: 0.1353197 (304)\ttotal: 7.17s\tremaining: 3m 36s\n",
      "340:\tlearn: 0.1267848\ttest: 0.1353001\tbest: 0.1352984 (339)\ttotal: 7.63s\tremaining: 3m 36s\n",
      "360:\tlearn: 0.1263057\ttest: 0.1353137\tbest: 0.1352863 (352)\ttotal: 8.08s\tremaining: 3m 35s\n",
      "380:\tlearn: 0.1260045\ttest: 0.1352763\tbest: 0.1352676 (371)\ttotal: 8.49s\tremaining: 3m 34s\n",
      "400:\tlearn: 0.1256350\ttest: 0.1352357\tbest: 0.1352357 (400)\ttotal: 8.92s\tremaining: 3m 33s\n",
      "420:\tlearn: 0.1252766\ttest: 0.1352531\tbest: 0.1352121 (409)\ttotal: 9.35s\tremaining: 3m 32s\n",
      "440:\tlearn: 0.1249222\ttest: 0.1352269\tbest: 0.1352121 (409)\ttotal: 9.79s\tremaining: 3m 32s\n",
      "460:\tlearn: 0.1246364\ttest: 0.1352295\tbest: 0.1352121 (409)\ttotal: 10.2s\tremaining: 3m 31s\n",
      "480:\tlearn: 0.1244003\ttest: 0.1352082\tbest: 0.1351900 (471)\ttotal: 10.6s\tremaining: 3m 30s\n",
      "500:\tlearn: 0.1239869\ttest: 0.1351986\tbest: 0.1351742 (486)\ttotal: 11s\tremaining: 3m 29s\n",
      "520:\tlearn: 0.1235691\ttest: 0.1352340\tbest: 0.1351742 (486)\ttotal: 11.5s\tremaining: 3m 29s\n",
      "540:\tlearn: 0.1232078\ttest: 0.1352167\tbest: 0.1351742 (486)\ttotal: 11.9s\tremaining: 3m 28s\n",
      "560:\tlearn: 0.1229835\ttest: 0.1352374\tbest: 0.1351742 (486)\ttotal: 12.3s\tremaining: 3m 27s\n",
      "580:\tlearn: 0.1225953\ttest: 0.1352137\tbest: 0.1351742 (486)\ttotal: 12.8s\tremaining: 3m 27s\n",
      "600:\tlearn: 0.1222616\ttest: 0.1352210\tbest: 0.1351742 (486)\ttotal: 13.2s\tremaining: 3m 26s\n",
      "620:\tlearn: 0.1218949\ttest: 0.1351918\tbest: 0.1351742 (486)\ttotal: 13.6s\tremaining: 3m 25s\n",
      "640:\tlearn: 0.1215130\ttest: 0.1352701\tbest: 0.1351742 (486)\ttotal: 14.1s\tremaining: 3m 25s\n",
      "\n",
      "bestTest = 0.1351741739\n",
      "bestIteration = 486\n",
      "\n",
      "Shrink model to first 487 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6225066\ttest: 0.6225880\tbest: 0.6225880 (0)\ttotal: 23ms\tremaining: 3m 49s\n",
      "20:\tlearn: 0.1766028\ttest: 0.1779585\tbest: 0.1779585 (20)\ttotal: 479ms\tremaining: 3m 47s\n",
      "40:\tlearn: 0.1421851\ttest: 0.1442285\tbest: 0.1442285 (40)\ttotal: 972ms\tremaining: 3m 56s\n",
      "60:\tlearn: 0.1360982\ttest: 0.1388539\tbest: 0.1388539 (60)\ttotal: 1.45s\tremaining: 3m 56s\n",
      "80:\tlearn: 0.1343511\ttest: 0.1376519\tbest: 0.1376519 (80)\ttotal: 1.91s\tremaining: 3m 53s\n",
      "100:\tlearn: 0.1334814\ttest: 0.1372125\tbest: 0.1372125 (100)\ttotal: 2.34s\tremaining: 3m 49s\n",
      "120:\tlearn: 0.1328407\ttest: 0.1367876\tbest: 0.1367876 (120)\ttotal: 2.78s\tremaining: 3m 46s\n",
      "140:\tlearn: 0.1320669\ttest: 0.1363845\tbest: 0.1363845 (140)\ttotal: 3.21s\tremaining: 3m 44s\n",
      "160:\tlearn: 0.1314418\ttest: 0.1360879\tbest: 0.1360870 (158)\ttotal: 3.63s\tremaining: 3m 41s\n",
      "180:\tlearn: 0.1308737\ttest: 0.1358273\tbest: 0.1358273 (180)\ttotal: 4.08s\tremaining: 3m 41s\n",
      "200:\tlearn: 0.1303681\ttest: 0.1356757\tbest: 0.1356604 (199)\ttotal: 4.52s\tremaining: 3m 40s\n",
      "220:\tlearn: 0.1298816\ttest: 0.1355382\tbest: 0.1355382 (220)\ttotal: 4.95s\tremaining: 3m 39s\n",
      "240:\tlearn: 0.1293796\ttest: 0.1353809\tbest: 0.1353672 (237)\ttotal: 5.37s\tremaining: 3m 37s\n",
      "260:\tlearn: 0.1288921\ttest: 0.1353001\tbest: 0.1352796 (255)\ttotal: 5.79s\tremaining: 3m 35s\n",
      "280:\tlearn: 0.1283623\ttest: 0.1352598\tbest: 0.1352586 (279)\ttotal: 6.22s\tremaining: 3m 35s\n",
      "300:\tlearn: 0.1278711\ttest: 0.1351690\tbest: 0.1351624 (295)\ttotal: 6.65s\tremaining: 3m 34s\n",
      "320:\tlearn: 0.1274791\ttest: 0.1350967\tbest: 0.1350965 (319)\ttotal: 7.07s\tremaining: 3m 33s\n",
      "340:\tlearn: 0.1271190\ttest: 0.1350201\tbest: 0.1350201 (340)\ttotal: 7.47s\tremaining: 3m 31s\n",
      "360:\tlearn: 0.1265985\ttest: 0.1350893\tbest: 0.1349957 (344)\ttotal: 7.91s\tremaining: 3m 31s\n",
      "380:\tlearn: 0.1262777\ttest: 0.1351108\tbest: 0.1349957 (344)\ttotal: 8.32s\tremaining: 3m 30s\n",
      "400:\tlearn: 0.1258576\ttest: 0.1350949\tbest: 0.1349957 (344)\ttotal: 8.76s\tremaining: 3m 29s\n",
      "420:\tlearn: 0.1253892\ttest: 0.1350800\tbest: 0.1349957 (344)\ttotal: 9.19s\tremaining: 3m 28s\n",
      "440:\tlearn: 0.1249703\ttest: 0.1351057\tbest: 0.1349957 (344)\ttotal: 9.61s\tremaining: 3m 28s\n",
      "460:\tlearn: 0.1244999\ttest: 0.1351304\tbest: 0.1349957 (344)\ttotal: 10.1s\tremaining: 3m 28s\n",
      "\n",
      "bestTest = 0.1349956619\n",
      "bestIteration = 344\n",
      "\n",
      "Shrink model to first 345 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6190515\ttest: 0.6191467\tbest: 0.6191467 (0)\ttotal: 24.7ms\tremaining: 4m 6s\n",
      "20:\tlearn: 0.1779169\ttest: 0.1770945\tbest: 0.1770945 (20)\ttotal: 462ms\tremaining: 3m 39s\n",
      "40:\tlearn: 0.1431746\ttest: 0.1410837\tbest: 0.1410837 (40)\ttotal: 927ms\tremaining: 3m 45s\n",
      "60:\tlearn: 0.1369866\ttest: 0.1346315\tbest: 0.1346315 (60)\ttotal: 1.42s\tremaining: 3m 51s\n",
      "80:\tlearn: 0.1349151\ttest: 0.1329140\tbest: 0.1329140 (80)\ttotal: 1.88s\tremaining: 3m 50s\n",
      "100:\tlearn: 0.1339139\ttest: 0.1323972\tbest: 0.1323972 (100)\ttotal: 2.31s\tremaining: 3m 46s\n",
      "120:\tlearn: 0.1330420\ttest: 0.1319461\tbest: 0.1319461 (120)\ttotal: 2.73s\tremaining: 3m 42s\n",
      "140:\tlearn: 0.1323553\ttest: 0.1316550\tbest: 0.1316550 (140)\ttotal: 3.17s\tremaining: 3m 41s\n",
      "160:\tlearn: 0.1316253\ttest: 0.1314578\tbest: 0.1314578 (160)\ttotal: 3.62s\tremaining: 3m 41s\n",
      "180:\tlearn: 0.1310678\ttest: 0.1312463\tbest: 0.1312408 (179)\ttotal: 4.05s\tremaining: 3m 40s\n",
      "200:\tlearn: 0.1305206\ttest: 0.1311091\tbest: 0.1311091 (200)\ttotal: 4.48s\tremaining: 3m 38s\n",
      "220:\tlearn: 0.1298834\ttest: 0.1309282\tbest: 0.1309282 (220)\ttotal: 4.92s\tremaining: 3m 37s\n",
      "240:\tlearn: 0.1294158\ttest: 0.1309257\tbest: 0.1309196 (238)\ttotal: 5.36s\tremaining: 3m 36s\n",
      "260:\tlearn: 0.1288863\ttest: 0.1308845\tbest: 0.1308842 (259)\ttotal: 5.8s\tremaining: 3m 36s\n",
      "280:\tlearn: 0.1284012\ttest: 0.1307973\tbest: 0.1307939 (278)\ttotal: 6.22s\tremaining: 3m 35s\n",
      "300:\tlearn: 0.1279112\ttest: 0.1307138\tbest: 0.1307138 (300)\ttotal: 6.65s\tremaining: 3m 34s\n",
      "320:\tlearn: 0.1273661\ttest: 0.1306199\tbest: 0.1306199 (320)\ttotal: 7.1s\tremaining: 3m 33s\n",
      "340:\tlearn: 0.1269237\ttest: 0.1305684\tbest: 0.1305684 (340)\ttotal: 7.53s\tremaining: 3m 33s\n",
      "360:\tlearn: 0.1264203\ttest: 0.1305213\tbest: 0.1305148 (359)\ttotal: 7.97s\tremaining: 3m 32s\n",
      "380:\tlearn: 0.1258955\ttest: 0.1304766\tbest: 0.1304669 (379)\ttotal: 8.41s\tremaining: 3m 32s\n",
      "400:\tlearn: 0.1255360\ttest: 0.1304573\tbest: 0.1304541 (395)\ttotal: 8.83s\tremaining: 3m 31s\n",
      "420:\tlearn: 0.1250813\ttest: 0.1304544\tbest: 0.1304247 (411)\ttotal: 9.27s\tremaining: 3m 30s\n",
      "440:\tlearn: 0.1246649\ttest: 0.1304139\tbest: 0.1304130 (439)\ttotal: 9.71s\tremaining: 3m 30s\n",
      "460:\tlearn: 0.1242561\ttest: 0.1304413\tbest: 0.1304108 (441)\ttotal: 10.1s\tremaining: 3m 29s\n",
      "480:\tlearn: 0.1238147\ttest: 0.1303804\tbest: 0.1303726 (478)\ttotal: 10.6s\tremaining: 3m 29s\n",
      "500:\tlearn: 0.1234058\ttest: 0.1303572\tbest: 0.1303490 (493)\ttotal: 11s\tremaining: 3m 29s\n",
      "520:\tlearn: 0.1230781\ttest: 0.1304030\tbest: 0.1303490 (493)\ttotal: 11.5s\tremaining: 3m 28s\n",
      "540:\tlearn: 0.1226873\ttest: 0.1304314\tbest: 0.1303490 (493)\ttotal: 11.9s\tremaining: 3m 28s\n",
      "560:\tlearn: 0.1223075\ttest: 0.1304675\tbest: 0.1303490 (493)\ttotal: 12.3s\tremaining: 3m 27s\n",
      "580:\tlearn: 0.1218899\ttest: 0.1305449\tbest: 0.1303490 (493)\ttotal: 12.8s\tremaining: 3m 27s\n",
      "600:\tlearn: 0.1215293\ttest: 0.1305557\tbest: 0.1303490 (493)\ttotal: 13.2s\tremaining: 3m 26s\n",
      "620:\tlearn: 0.1210322\ttest: 0.1305548\tbest: 0.1303490 (493)\ttotal: 13.7s\tremaining: 3m 26s\n",
      "640:\tlearn: 0.1206236\ttest: 0.1305597\tbest: 0.1303490 (493)\ttotal: 14.1s\tremaining: 3m 26s\n",
      "660:\tlearn: 0.1201103\ttest: 0.1305554\tbest: 0.1303490 (493)\ttotal: 14.6s\tremaining: 3m 26s\n",
      "\n",
      "bestTest = 0.1303490154\n",
      "bestIteration = 493\n",
      "\n",
      "Shrink model to first 494 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6175353\ttest: 0.6176811\tbest: 0.6176811 (0)\ttotal: 19.5ms\tremaining: 3m 15s\n",
      "20:\tlearn: 0.1779768\ttest: 0.1783183\tbest: 0.1783183 (20)\ttotal: 460ms\tremaining: 3m 38s\n",
      "40:\tlearn: 0.1414038\ttest: 0.1426350\tbest: 0.1426350 (40)\ttotal: 938ms\tremaining: 3m 47s\n",
      "60:\tlearn: 0.1358987\ttest: 0.1381578\tbest: 0.1381578 (60)\ttotal: 1.4s\tremaining: 3m 48s\n",
      "80:\tlearn: 0.1340314\ttest: 0.1370432\tbest: 0.1370432 (80)\ttotal: 1.86s\tremaining: 3m 48s\n",
      "100:\tlearn: 0.1333516\ttest: 0.1367411\tbest: 0.1367411 (99)\ttotal: 2.28s\tremaining: 3m 43s\n",
      "120:\tlearn: 0.1325808\ttest: 0.1364097\tbest: 0.1364097 (120)\ttotal: 2.7s\tremaining: 3m 40s\n",
      "140:\tlearn: 0.1318997\ttest: 0.1361338\tbest: 0.1361320 (139)\ttotal: 3.12s\tremaining: 3m 38s\n",
      "160:\tlearn: 0.1314141\ttest: 0.1360193\tbest: 0.1360193 (160)\ttotal: 3.54s\tremaining: 3m 36s\n",
      "180:\tlearn: 0.1308825\ttest: 0.1357618\tbest: 0.1357592 (179)\ttotal: 3.98s\tremaining: 3m 36s\n",
      "200:\tlearn: 0.1303279\ttest: 0.1356211\tbest: 0.1356211 (200)\ttotal: 4.41s\tremaining: 3m 34s\n",
      "220:\tlearn: 0.1297436\ttest: 0.1354279\tbest: 0.1354279 (220)\ttotal: 4.83s\tremaining: 3m 33s\n",
      "240:\tlearn: 0.1292207\ttest: 0.1352525\tbest: 0.1352525 (240)\ttotal: 5.26s\tremaining: 3m 32s\n",
      "260:\tlearn: 0.1288199\ttest: 0.1351536\tbest: 0.1351454 (259)\ttotal: 5.68s\tremaining: 3m 31s\n",
      "280:\tlearn: 0.1282607\ttest: 0.1349638\tbest: 0.1349638 (280)\ttotal: 6.12s\tremaining: 3m 31s\n",
      "300:\tlearn: 0.1277676\ttest: 0.1348342\tbest: 0.1348342 (300)\ttotal: 6.53s\tremaining: 3m 30s\n",
      "320:\tlearn: 0.1273788\ttest: 0.1348063\tbest: 0.1347960 (310)\ttotal: 6.94s\tremaining: 3m 29s\n",
      "340:\tlearn: 0.1269792\ttest: 0.1349035\tbest: 0.1347960 (310)\ttotal: 7.37s\tremaining: 3m 28s\n",
      "360:\tlearn: 0.1264110\ttest: 0.1348538\tbest: 0.1347960 (310)\ttotal: 7.82s\tremaining: 3m 28s\n",
      "380:\tlearn: 0.1259686\ttest: 0.1347129\tbest: 0.1347053 (377)\ttotal: 8.25s\tremaining: 3m 28s\n",
      "400:\tlearn: 0.1256591\ttest: 0.1347369\tbest: 0.1347014 (385)\ttotal: 8.66s\tremaining: 3m 27s\n",
      "420:\tlearn: 0.1251009\ttest: 0.1346989\tbest: 0.1346984 (419)\ttotal: 9.1s\tremaining: 3m 27s\n",
      "440:\tlearn: 0.1247368\ttest: 0.1346622\tbest: 0.1346550 (436)\ttotal: 9.53s\tremaining: 3m 26s\n",
      "460:\tlearn: 0.1244295\ttest: 0.1346496\tbest: 0.1346326 (452)\ttotal: 9.95s\tremaining: 3m 25s\n",
      "480:\tlearn: 0.1240970\ttest: 0.1346770\tbest: 0.1346326 (452)\ttotal: 10.4s\tremaining: 3m 25s\n",
      "500:\tlearn: 0.1237528\ttest: 0.1346530\tbest: 0.1346326 (452)\ttotal: 10.8s\tremaining: 3m 24s\n",
      "520:\tlearn: 0.1234036\ttest: 0.1346905\tbest: 0.1346326 (452)\ttotal: 11.2s\tremaining: 3m 24s\n",
      "540:\tlearn: 0.1229574\ttest: 0.1347085\tbest: 0.1346326 (452)\ttotal: 11.7s\tremaining: 3m 24s\n",
      "560:\tlearn: 0.1225411\ttest: 0.1346891\tbest: 0.1346326 (452)\ttotal: 12.1s\tremaining: 3m 24s\n",
      "580:\tlearn: 0.1222228\ttest: 0.1346759\tbest: 0.1346326 (452)\ttotal: 12.6s\tremaining: 3m 23s\n",
      "600:\tlearn: 0.1218890\ttest: 0.1346470\tbest: 0.1346239 (588)\ttotal: 13s\tremaining: 3m 23s\n",
      "620:\tlearn: 0.1216320\ttest: 0.1346442\tbest: 0.1346239 (588)\ttotal: 13.4s\tremaining: 3m 22s\n",
      "640:\tlearn: 0.1213030\ttest: 0.1346175\tbest: 0.1345699 (630)\ttotal: 13.9s\tremaining: 3m 22s\n",
      "660:\tlearn: 0.1208965\ttest: 0.1346241\tbest: 0.1345699 (630)\ttotal: 14.3s\tremaining: 3m 21s\n",
      "680:\tlearn: 0.1204926\ttest: 0.1345908\tbest: 0.1345699 (630)\ttotal: 14.7s\tremaining: 3m 21s\n",
      "700:\tlearn: 0.1200817\ttest: 0.1346483\tbest: 0.1345699 (630)\ttotal: 15.2s\tremaining: 3m 21s\n",
      "720:\tlearn: 0.1197287\ttest: 0.1346126\tbest: 0.1345699 (630)\ttotal: 15.6s\tremaining: 3m 21s\n",
      "740:\tlearn: 0.1194318\ttest: 0.1347193\tbest: 0.1345699 (630)\ttotal: 16.1s\tremaining: 3m 20s\n",
      "760:\tlearn: 0.1190826\ttest: 0.1347194\tbest: 0.1345699 (630)\ttotal: 16.5s\tremaining: 3m 20s\n",
      "780:\tlearn: 0.1187633\ttest: 0.1347865\tbest: 0.1345699 (630)\ttotal: 16.9s\tremaining: 3m 19s\n",
      "800:\tlearn: 0.1184157\ttest: 0.1348488\tbest: 0.1345699 (630)\ttotal: 17.4s\tremaining: 3m 19s\n",
      "820:\tlearn: 0.1181284\ttest: 0.1349164\tbest: 0.1345699 (630)\ttotal: 17.8s\tremaining: 3m 19s\n",
      "\n",
      "bestTest = 0.1345699347\n",
      "bestIteration = 630\n",
      "\n",
      "Shrink model to first 631 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6227616\ttest: 0.6228670\tbest: 0.6228670 (0)\ttotal: 19.3ms\tremaining: 3m 12s\n",
      "20:\tlearn: 0.1774669\ttest: 0.1773098\tbest: 0.1773098 (20)\ttotal: 456ms\tremaining: 3m 36s\n",
      "40:\tlearn: 0.1429826\ttest: 0.1429100\tbest: 0.1429100 (40)\ttotal: 926ms\tremaining: 3m 44s\n",
      "60:\tlearn: 0.1361603\ttest: 0.1364511\tbest: 0.1364511 (60)\ttotal: 1.41s\tremaining: 3m 49s\n",
      "80:\tlearn: 0.1343389\ttest: 0.1355430\tbest: 0.1355430 (80)\ttotal: 1.86s\tremaining: 3m 48s\n",
      "100:\tlearn: 0.1334006\ttest: 0.1350940\tbest: 0.1350752 (99)\ttotal: 2.3s\tremaining: 3m 45s\n",
      "120:\tlearn: 0.1324749\ttest: 0.1347418\tbest: 0.1347417 (119)\ttotal: 2.75s\tremaining: 3m 44s\n",
      "140:\tlearn: 0.1316770\ttest: 0.1343341\tbest: 0.1343317 (139)\ttotal: 3.18s\tremaining: 3m 42s\n",
      "160:\tlearn: 0.1311646\ttest: 0.1341617\tbest: 0.1341578 (158)\ttotal: 3.61s\tremaining: 3m 40s\n",
      "180:\tlearn: 0.1305396\ttest: 0.1339816\tbest: 0.1339816 (180)\ttotal: 4.06s\tremaining: 3m 40s\n",
      "200:\tlearn: 0.1300468\ttest: 0.1338983\tbest: 0.1338983 (200)\ttotal: 4.5s\tremaining: 3m 39s\n",
      "220:\tlearn: 0.1295596\ttest: 0.1337686\tbest: 0.1337478 (219)\ttotal: 4.93s\tremaining: 3m 38s\n",
      "240:\tlearn: 0.1289700\ttest: 0.1336881\tbest: 0.1336836 (232)\ttotal: 5.38s\tremaining: 3m 37s\n",
      "260:\tlearn: 0.1285481\ttest: 0.1336093\tbest: 0.1336093 (260)\ttotal: 5.79s\tremaining: 3m 36s\n",
      "280:\tlearn: 0.1280202\ttest: 0.1335826\tbest: 0.1335305 (275)\ttotal: 6.23s\tremaining: 3m 35s\n",
      "300:\tlearn: 0.1276378\ttest: 0.1335505\tbest: 0.1335261 (298)\ttotal: 6.67s\tremaining: 3m 34s\n",
      "320:\tlearn: 0.1272063\ttest: 0.1334389\tbest: 0.1334389 (320)\ttotal: 7.09s\tremaining: 3m 33s\n",
      "340:\tlearn: 0.1268420\ttest: 0.1334705\tbest: 0.1333908 (327)\ttotal: 7.51s\tremaining: 3m 32s\n",
      "360:\tlearn: 0.1265152\ttest: 0.1334463\tbest: 0.1333908 (327)\ttotal: 7.94s\tremaining: 3m 32s\n",
      "380:\tlearn: 0.1260396\ttest: 0.1334418\tbest: 0.1333908 (327)\ttotal: 8.38s\tremaining: 3m 31s\n",
      "400:\tlearn: 0.1256989\ttest: 0.1334476\tbest: 0.1333908 (327)\ttotal: 8.84s\tremaining: 3m 31s\n",
      "420:\tlearn: 0.1253343\ttest: 0.1335152\tbest: 0.1333908 (327)\ttotal: 9.26s\tremaining: 3m 30s\n",
      "440:\tlearn: 0.1249791\ttest: 0.1335476\tbest: 0.1333908 (327)\ttotal: 9.69s\tremaining: 3m 30s\n",
      "\n",
      "bestTest = 0.1333907777\n",
      "bestIteration = 327\n",
      "\n",
      "Shrink model to first 328 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6171074\ttest: 0.6168728\tbest: 0.6168728 (0)\ttotal: 22.3ms\tremaining: 3m 42s\n",
      "20:\tlearn: 0.1762118\ttest: 0.1769210\tbest: 0.1769210 (20)\ttotal: 448ms\tremaining: 3m 32s\n",
      "40:\tlearn: 0.1426281\ttest: 0.1447851\tbest: 0.1447851 (40)\ttotal: 905ms\tremaining: 3m 39s\n",
      "60:\tlearn: 0.1360859\ttest: 0.1396933\tbest: 0.1396933 (60)\ttotal: 1.4s\tremaining: 3m 47s\n",
      "80:\tlearn: 0.1341207\ttest: 0.1387100\tbest: 0.1387100 (80)\ttotal: 1.85s\tremaining: 3m 47s\n",
      "100:\tlearn: 0.1333397\ttest: 0.1382674\tbest: 0.1382620 (98)\ttotal: 2.28s\tremaining: 3m 43s\n",
      "120:\tlearn: 0.1325070\ttest: 0.1378810\tbest: 0.1378810 (120)\ttotal: 2.72s\tremaining: 3m 41s\n",
      "140:\tlearn: 0.1317563\ttest: 0.1376022\tbest: 0.1376022 (140)\ttotal: 3.15s\tremaining: 3m 40s\n",
      "160:\tlearn: 0.1309554\ttest: 0.1373188\tbest: 0.1373188 (160)\ttotal: 3.6s\tremaining: 3m 40s\n",
      "180:\tlearn: 0.1305500\ttest: 0.1371692\tbest: 0.1371520 (179)\ttotal: 4.03s\tremaining: 3m 38s\n",
      "200:\tlearn: 0.1300854\ttest: 0.1370014\tbest: 0.1369876 (198)\ttotal: 4.46s\tremaining: 3m 37s\n",
      "220:\tlearn: 0.1295930\ttest: 0.1368359\tbest: 0.1368253 (219)\ttotal: 4.88s\tremaining: 3m 36s\n",
      "240:\tlearn: 0.1291815\ttest: 0.1367330\tbest: 0.1367313 (237)\ttotal: 5.31s\tremaining: 3m 35s\n",
      "260:\tlearn: 0.1287350\ttest: 0.1366788\tbest: 0.1366750 (256)\ttotal: 5.76s\tremaining: 3m 34s\n",
      "280:\tlearn: 0.1282462\ttest: 0.1366234\tbest: 0.1366124 (275)\ttotal: 6.21s\tremaining: 3m 34s\n",
      "300:\tlearn: 0.1278397\ttest: 0.1366291\tbest: 0.1366124 (275)\ttotal: 6.64s\tremaining: 3m 33s\n",
      "320:\tlearn: 0.1275111\ttest: 0.1366229\tbest: 0.1366124 (275)\ttotal: 7.04s\tremaining: 3m 32s\n",
      "340:\tlearn: 0.1269112\ttest: 0.1365707\tbest: 0.1365586 (338)\ttotal: 7.49s\tremaining: 3m 32s\n",
      "360:\tlearn: 0.1263009\ttest: 0.1365315\tbest: 0.1365178 (358)\ttotal: 7.95s\tremaining: 3m 32s\n",
      "380:\tlearn: 0.1258179\ttest: 0.1365767\tbest: 0.1365139 (366)\ttotal: 8.39s\tremaining: 3m 31s\n",
      "400:\tlearn: 0.1254255\ttest: 0.1365645\tbest: 0.1365139 (366)\ttotal: 8.81s\tremaining: 3m 30s\n",
      "420:\tlearn: 0.1248391\ttest: 0.1365834\tbest: 0.1365139 (366)\ttotal: 9.25s\tremaining: 3m 30s\n",
      "440:\tlearn: 0.1243064\ttest: 0.1366833\tbest: 0.1365139 (366)\ttotal: 9.71s\tremaining: 3m 30s\n",
      "460:\tlearn: 0.1239050\ttest: 0.1367047\tbest: 0.1365139 (366)\ttotal: 10.1s\tremaining: 3m 29s\n",
      "480:\tlearn: 0.1234656\ttest: 0.1367817\tbest: 0.1365139 (366)\ttotal: 10.6s\tremaining: 3m 29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "\t0.8399\t = Validation score   (roc_auc)\n",
      "\t113.47s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 4888.54s of the 4888.52s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bestTest = 0.1365139011\n",
      "bestIteration = 366\n",
      "\n",
      "Shrink model to first 367 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\ExtraTreesGini_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\ExtraTreesGini_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t8.65s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonSantander/\\models\\ExtraTreesGini_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\ExtraTreesGini_BAG_L1\\model.pkl\n",
      "\t0.7744\t = Validation score   (roc_auc)\n",
      "\t33.23s\t = Training   runtime\n",
      "\t10.49s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 4844.05s of the 4844.04s of remaining time.\n",
      "Saving AutoGluonSantander/\\models\\ExtraTreesEntr_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\ExtraTreesEntr_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t8.65s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonSantander/\\models\\ExtraTreesEntr_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\ExtraTreesEntr_BAG_L1\\model.pkl\n",
      "\t0.7753\t = Validation score   (roc_auc)\n",
      "\t33.76s\t = Training   runtime\n",
      "\t10.86s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 4798.75s of the 4798.73s of remaining time.\n",
      "Saving AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit NNFastAiTabularModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 109.37 / 475.94 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.1707865297794342.\n",
      "Better model found at epoch 1 with valid_loss value: 0.1479191780090332.\n",
      "Better model found at epoch 2 with valid_loss value: 0.14597547054290771.\n",
      "Better model found at epoch 4 with valid_loss value: 0.14350537955760956.\n",
      "Better model found at epoch 5 with valid_loss value: 0.14081725478172302.\n",
      "Better model found at epoch 7 with valid_loss value: 0.14054135978221893.\n",
      "Better model found at epoch 8 with valid_loss value: 0.14048729836940765.\n",
      "Better model found at epoch 11 with valid_loss value: 0.14026805758476257.\n",
      "Model validation metrics: 0.14026805758476257\n",
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 102.08 / 533.39 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.17284052073955536.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14053687453269958.\n",
      "Better model found at epoch 4 with valid_loss value: 0.13743799924850464.\n",
      "Better model found at epoch 5 with valid_loss value: 0.13703902065753937.\n",
      "No improvement since epoch 5: early stopping\n",
      "Model validation metrics: 0.13703902065753937\n",
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 331 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=331, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 124.65 / 611.76 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.7419346570968628.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14212392270565033.\n",
      "Better model found at epoch 3 with valid_loss value: 0.13844174146652222.\n",
      "No improvement since epoch 3: early stopping\n",
      "Model validation metrics: 0.13844174146652222\n",
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 334 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(334, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=334, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 125.93 / 721.64 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.17950274050235748.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14485055208206177.\n",
      "Better model found at epoch 8 with valid_loss value: 0.14290884137153625.\n",
      "No improvement since epoch 8: early stopping\n",
      "Model validation metrics: 0.14290884137153625\n",
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 112.25 / 884.97 sec\n",
      "Better model found at epoch 0 with valid_loss value: 1465335.0.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14154300093650818.\n",
      "Better model found at epoch 3 with valid_loss value: 0.13935036957263947.\n",
      "Better model found at epoch 4 with valid_loss value: 0.13777869939804077.\n",
      "Better model found at epoch 5 with valid_loss value: 0.13611984252929688.\n",
      "No improvement since epoch 5: early stopping\n",
      "Model validation metrics: 0.13611984252929688\n",
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 102.38 / 1158.76 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.17344802618026733.\n",
      "Better model found at epoch 1 with valid_loss value: 0.1711321771144867.\n",
      "Better model found at epoch 2 with valid_loss value: 0.14220619201660156.\n",
      "Better model found at epoch 3 with valid_loss value: 0.1415940672159195.\n",
      "Better model found at epoch 5 with valid_loss value: 0.1406211256980896.\n",
      "Better model found at epoch 7 with valid_loss value: 0.13746775686740875.\n",
      "No improvement since epoch 7: early stopping\n",
      "Model validation metrics: 0.13746775686740875\n",
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 98.81 / 1704.39 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.18258841335773468.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14795705676078796.\n",
      "Better model found at epoch 6 with valid_loss value: 0.14605732262134552.\n",
      "No improvement since epoch 6: early stopping\n",
      "Model validation metrics: 0.14605732262134552\n",
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 332 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=332, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 108.27 / 3343.23 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.259965181350708.\n",
      "Better model found at epoch 1 with valid_loss value: 0.15019965171813965.\n",
      "Better model found at epoch 2 with valid_loss value: 0.14119039475917816.\n",
      "Better model found at epoch 3 with valid_loss value: 0.14063124358654022.\n",
      "Better model found at epoch 5 with valid_loss value: 0.14032268524169922.\n",
      "Better model found at epoch 7 with valid_loss value: 0.13862447440624237.\n",
      "No improvement since epoch 7: early stopping\n",
      "Model validation metrics: 0.13862447440624237\n",
      "Saving AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\model.pkl\n",
      "\t0.8229\t = Validation score   (roc_auc)\n",
      "\t696.14s\t = Training   runtime\n",
      "\t4.51s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 4093.07s of the 4093.05s of remaining time.\n",
      "Saving AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit XGBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61138\n",
      "[50]\tvalidation_0-logloss:0.13485\n",
      "[100]\tvalidation_0-logloss:0.13493\n",
      "[107]\tvalidation_0-logloss:0.13502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61141\n",
      "[50]\tvalidation_0-logloss:0.13312\n",
      "[100]\tvalidation_0-logloss:0.13222\n",
      "[140]\tvalidation_0-logloss:0.13234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61145\n",
      "[50]\tvalidation_0-logloss:0.13558\n",
      "[100]\tvalidation_0-logloss:0.13538\n",
      "[114]\tvalidation_0-logloss:0.13554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61151\n",
      "[50]\tvalidation_0-logloss:0.13588\n",
      "[100]\tvalidation_0-logloss:0.13576\n",
      "[113]\tvalidation_0-logloss:0.13608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61151\n",
      "[50]\tvalidation_0-logloss:0.13182\n",
      "[100]\tvalidation_0-logloss:0.13175\n",
      "[104]\tvalidation_0-logloss:0.13173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61142\n",
      "[50]\tvalidation_0-logloss:0.13506\n",
      "[100]\tvalidation_0-logloss:0.13465\n",
      "[113]\tvalidation_0-logloss:0.13450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61151\n",
      "[50]\tvalidation_0-logloss:0.13465\n",
      "[100]\tvalidation_0-logloss:0.13460\n",
      "[105]\tvalidation_0-logloss:0.13466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61142\n",
      "[50]\tvalidation_0-logloss:0.13720\n",
      "[100]\tvalidation_0-logloss:0.13737\n",
      "[118]\tvalidation_0-logloss:0.13748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\XGBoost_BAG_L1\\model.pkl\n",
      "\t0.8381\t = Validation score   (roc_auc)\n",
      "\t22.54s\t = Training   runtime\n",
      "\t0.79s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 4068.91s of the 4068.89s of remaining time.\n",
      "Saving AutoGluonSantander/\\models\\NeuralNetTorch_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetTorch_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=5496, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 142, in _fit\n",
      "    try_import_torch()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 199, in try_import_torch\n",
      "    import torch\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 457, in after_all_folds_scheduled\n",
      "    raise processed_exception\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 425, in after_all_folds_scheduled\n",
      "    time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\ray\\worker.py\", line 1831, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(OSError): \u001b[36mray::_ray_fit()\u001b[39m (pid=5496, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 288, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 142, in _fit\n",
      "    try_import_torch()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\try_import.py\", line 199, in try_import_torch\n",
      "    import torch\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\__init__.py\", line 129, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 4057.69s of the 4057.68s of remaining time.\n",
      "Saving AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138327\n",
      "[100]\tvalid_set's binary_logloss: 0.13572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.13574\n",
      "[100]\tvalid_set's binary_logloss: 0.133117\n",
      "[150]\tvalid_set's binary_logloss: 0.133239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138579\n",
      "[100]\tvalid_set's binary_logloss: 0.136511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138393\n",
      "[100]\tvalid_set's binary_logloss: 0.137137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.133916\n",
      "[100]\tvalid_set's binary_logloss: 0.13137\n",
      "[150]\tvalid_set's binary_logloss: 0.131407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138089\n",
      "[100]\tvalid_set's binary_logloss: 0.135976\n",
      "[150]\tvalid_set's binary_logloss: 0.136092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.13718\n",
      "[100]\tvalid_set's binary_logloss: 0.135489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.140167\n",
      "[100]\tvalid_set's binary_logloss: 0.138994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t0.8355\t = Validation score   (roc_auc)\n",
      "\t24.64s\t = Training   runtime\n",
      "\t0.55s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 2/20\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4031.48s of the 4031.45s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S2F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.141234\n",
      "[100]\tvalid_set's binary_logloss: 0.139266\n",
      "[150]\tvalid_set's binary_logloss: 0.139279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135116\n",
      "[100]\tvalid_set's binary_logloss: 0.131714\n",
      "[150]\tvalid_set's binary_logloss: 0.131199\n",
      "[200]\tvalid_set's binary_logloss: 0.130952\n",
      "[250]\tvalid_set's binary_logloss: 0.131183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135776\n",
      "[100]\tvalid_set's binary_logloss: 0.133559\n",
      "[150]\tvalid_set's binary_logloss: 0.133017\n",
      "[200]\tvalid_set's binary_logloss: 0.132971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.137964\n",
      "[100]\tvalid_set's binary_logloss: 0.135572\n",
      "[150]\tvalid_set's binary_logloss: 0.135244\n",
      "[200]\tvalid_set's binary_logloss: 0.135215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.13709\n",
      "[100]\tvalid_set's binary_logloss: 0.135109\n",
      "[150]\tvalid_set's binary_logloss: 0.134484\n",
      "[200]\tvalid_set's binary_logloss: 0.134526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.136794\n",
      "[100]\tvalid_set's binary_logloss: 0.134402\n",
      "[150]\tvalid_set's binary_logloss: 0.133759\n",
      "[200]\tvalid_set's binary_logloss: 0.133902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135469\n",
      "[100]\tvalid_set's binary_logloss: 0.132576\n",
      "[150]\tvalid_set's binary_logloss: 0.132001\n",
      "[200]\tvalid_set's binary_logloss: 0.131929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.139019\n",
      "[100]\tvalid_set's binary_logloss: 0.137066\n",
      "[150]\tvalid_set's binary_logloss: 0.136668\n",
      "[200]\tvalid_set's binary_logloss: 0.136615\n",
      "[250]\tvalid_set's binary_logloss: 0.136617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t0.8393\t = Validation score   (roc_auc)\n",
      "\t36.33s\t = Training   runtime\n",
      "\t1.27s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 4012.09s of the 4012.07s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S2F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138511\n",
      "[100]\tvalid_set's binary_logloss: 0.138282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.13359\n",
      "[100]\tvalid_set's binary_logloss: 0.1327\n",
      "[150]\tvalid_set's binary_logloss: 0.132785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.134523\n",
      "[100]\tvalid_set's binary_logloss: 0.133571\n",
      "[150]\tvalid_set's binary_logloss: 0.133717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.13609\n",
      "[100]\tvalid_set's binary_logloss: 0.13539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135486\n",
      "[100]\tvalid_set's binary_logloss: 0.13507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.134871\n",
      "[100]\tvalid_set's binary_logloss: 0.134063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.132591\n",
      "[100]\tvalid_set's binary_logloss: 0.131314\n",
      "[150]\tvalid_set's binary_logloss: 0.131227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.136314\n",
      "[100]\tvalid_set's binary_logloss: 0.135599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t0.8397\t = Validation score   (roc_auc)\n",
      "\t28.73s\t = Training   runtime\n",
      "\t0.96s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 3995.92s of the 3995.9s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S2F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6173595\ttest: 0.6176606\tbest: 0.6176606 (0)\ttotal: 15.8ms\tremaining: 2m 38s\n",
      "20:\tlearn: 0.1759425\ttest: 0.1779268\tbest: 0.1779268 (20)\ttotal: 495ms\tremaining: 3m 55s\n",
      "40:\tlearn: 0.1415677\ttest: 0.1451804\tbest: 0.1451804 (40)\ttotal: 1.03s\tremaining: 4m 9s\n",
      "60:\tlearn: 0.1354719\ttest: 0.1402200\tbest: 0.1402200 (60)\ttotal: 1.56s\tremaining: 4m 14s\n",
      "80:\tlearn: 0.1339178\ttest: 0.1393547\tbest: 0.1393547 (80)\ttotal: 2.04s\tremaining: 4m 10s\n",
      "100:\tlearn: 0.1330744\ttest: 0.1389244\tbest: 0.1389032 (99)\ttotal: 2.55s\tremaining: 4m 9s\n",
      "120:\tlearn: 0.1323558\ttest: 0.1385272\tbest: 0.1385272 (120)\ttotal: 3s\tremaining: 4m 5s\n",
      "140:\tlearn: 0.1315170\ttest: 0.1382210\tbest: 0.1382190 (139)\ttotal: 3.51s\tremaining: 4m 5s\n",
      "160:\tlearn: 0.1308596\ttest: 0.1379657\tbest: 0.1379657 (160)\ttotal: 3.98s\tremaining: 4m 3s\n",
      "180:\tlearn: 0.1303544\ttest: 0.1378568\tbest: 0.1378568 (180)\ttotal: 4.46s\tremaining: 4m 1s\n",
      "200:\tlearn: 0.1298383\ttest: 0.1376443\tbest: 0.1376443 (200)\ttotal: 4.91s\tremaining: 3m 59s\n",
      "220:\tlearn: 0.1292677\ttest: 0.1374863\tbest: 0.1374863 (220)\ttotal: 5.38s\tremaining: 3m 58s\n",
      "240:\tlearn: 0.1287239\ttest: 0.1373840\tbest: 0.1373778 (231)\ttotal: 5.85s\tremaining: 3m 56s\n",
      "260:\tlearn: 0.1281655\ttest: 0.1374358\tbest: 0.1373711 (243)\ttotal: 6.32s\tremaining: 3m 55s\n",
      "280:\tlearn: 0.1274824\ttest: 0.1371938\tbest: 0.1371938 (280)\ttotal: 6.79s\tremaining: 3m 54s\n",
      "300:\tlearn: 0.1269065\ttest: 0.1372709\tbest: 0.1371688 (286)\ttotal: 7.26s\tremaining: 3m 53s\n",
      "320:\tlearn: 0.1265410\ttest: 0.1373349\tbest: 0.1371688 (286)\ttotal: 7.72s\tremaining: 3m 52s\n",
      "340:\tlearn: 0.1260760\ttest: 0.1373988\tbest: 0.1371688 (286)\ttotal: 8.19s\tremaining: 3m 51s\n",
      "360:\tlearn: 0.1256344\ttest: 0.1374651\tbest: 0.1371688 (286)\ttotal: 8.64s\tremaining: 3m 50s\n",
      "380:\tlearn: 0.1252601\ttest: 0.1374214\tbest: 0.1371688 (286)\ttotal: 9.1s\tremaining: 3m 49s\n",
      "\n",
      "bestTest = 0.1371688483\n",
      "bestIteration = 286\n",
      "\n",
      "Shrink model to first 287 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6172751\ttest: 0.6170434\tbest: 0.6170434 (0)\ttotal: 24.5ms\tremaining: 4m 4s\n",
      "20:\tlearn: 0.1789222\ttest: 0.1786925\tbest: 0.1786925 (20)\ttotal: 492ms\tremaining: 3m 53s\n",
      "40:\tlearn: 0.1426157\ttest: 0.1423775\tbest: 0.1423775 (40)\ttotal: 1s\tremaining: 4m 4s\n",
      "60:\tlearn: 0.1366093\ttest: 0.1368916\tbest: 0.1368916 (60)\ttotal: 1.53s\tremaining: 4m 9s\n",
      "80:\tlearn: 0.1345763\ttest: 0.1352251\tbest: 0.1352251 (80)\ttotal: 2.05s\tremaining: 4m 10s\n",
      "100:\tlearn: 0.1337252\ttest: 0.1345816\tbest: 0.1345816 (100)\ttotal: 2.51s\tremaining: 4m 5s\n",
      "120:\tlearn: 0.1330164\ttest: 0.1342123\tbest: 0.1342123 (120)\ttotal: 2.97s\tremaining: 4m 2s\n",
      "140:\tlearn: 0.1323524\ttest: 0.1339094\tbest: 0.1339094 (140)\ttotal: 3.44s\tremaining: 4m\n",
      "160:\tlearn: 0.1317715\ttest: 0.1336889\tbest: 0.1336889 (160)\ttotal: 3.92s\tremaining: 3m 59s\n",
      "180:\tlearn: 0.1311551\ttest: 0.1334453\tbest: 0.1334453 (180)\ttotal: 4.4s\tremaining: 3m 58s\n",
      "200:\tlearn: 0.1306014\ttest: 0.1332676\tbest: 0.1332591 (199)\ttotal: 4.87s\tremaining: 3m 57s\n",
      "220:\tlearn: 0.1299336\ttest: 0.1329807\tbest: 0.1329807 (220)\ttotal: 5.36s\tremaining: 3m 57s\n",
      "240:\tlearn: 0.1294659\ttest: 0.1328813\tbest: 0.1328813 (240)\ttotal: 5.81s\tremaining: 3m 55s\n",
      "260:\tlearn: 0.1287889\ttest: 0.1326128\tbest: 0.1326128 (260)\ttotal: 6.31s\tremaining: 3m 55s\n",
      "280:\tlearn: 0.1283035\ttest: 0.1325077\tbest: 0.1325077 (280)\ttotal: 6.77s\tremaining: 3m 54s\n",
      "300:\tlearn: 0.1278415\ttest: 0.1323769\tbest: 0.1323769 (300)\ttotal: 7.24s\tremaining: 3m 53s\n",
      "320:\tlearn: 0.1272402\ttest: 0.1322137\tbest: 0.1322137 (320)\ttotal: 7.73s\tremaining: 3m 53s\n",
      "340:\tlearn: 0.1268569\ttest: 0.1321719\tbest: 0.1321636 (339)\ttotal: 8.19s\tremaining: 3m 51s\n",
      "360:\tlearn: 0.1265258\ttest: 0.1322093\tbest: 0.1321517 (343)\ttotal: 8.65s\tremaining: 3m 50s\n",
      "380:\tlearn: 0.1262097\ttest: 0.1321436\tbest: 0.1321436 (380)\ttotal: 9.14s\tremaining: 3m 50s\n",
      "400:\tlearn: 0.1257361\ttest: 0.1321153\tbest: 0.1321009 (396)\ttotal: 9.64s\tremaining: 3m 50s\n",
      "420:\tlearn: 0.1252160\ttest: 0.1321210\tbest: 0.1320807 (409)\ttotal: 10.1s\tremaining: 3m 50s\n",
      "440:\tlearn: 0.1248133\ttest: 0.1321933\tbest: 0.1320807 (409)\ttotal: 10.6s\tremaining: 3m 49s\n",
      "460:\tlearn: 0.1244515\ttest: 0.1322529\tbest: 0.1320807 (409)\ttotal: 11.1s\tremaining: 3m 49s\n",
      "480:\tlearn: 0.1239998\ttest: 0.1323377\tbest: 0.1320807 (409)\ttotal: 11.6s\tremaining: 3m 49s\n",
      "500:\tlearn: 0.1236343\ttest: 0.1323525\tbest: 0.1320807 (409)\ttotal: 12.1s\tremaining: 3m 48s\n",
      "520:\tlearn: 0.1232815\ttest: 0.1323578\tbest: 0.1320807 (409)\ttotal: 12.5s\tremaining: 3m 48s\n",
      "540:\tlearn: 0.1229557\ttest: 0.1323748\tbest: 0.1320807 (409)\ttotal: 13s\tremaining: 3m 47s\n",
      "\n",
      "bestTest = 0.1320807125\n",
      "bestIteration = 409\n",
      "\n",
      "Shrink model to first 410 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6226508\ttest: 0.6227368\tbest: 0.6227368 (0)\ttotal: 23.5ms\tremaining: 3m 55s\n",
      "20:\tlearn: 0.1794040\ttest: 0.1800087\tbest: 0.1800087 (20)\ttotal: 480ms\tremaining: 3m 48s\n",
      "40:\tlearn: 0.1426535\ttest: 0.1435092\tbest: 0.1435092 (40)\ttotal: 1.03s\tremaining: 4m 10s\n",
      "60:\tlearn: 0.1366414\ttest: 0.1378750\tbest: 0.1378750 (60)\ttotal: 1.56s\tremaining: 4m 13s\n",
      "80:\tlearn: 0.1345814\ttest: 0.1363136\tbest: 0.1363128 (79)\ttotal: 2.09s\tremaining: 4m 16s\n",
      "100:\tlearn: 0.1337502\ttest: 0.1358038\tbest: 0.1358036 (99)\ttotal: 2.54s\tremaining: 4m 9s\n",
      "120:\tlearn: 0.1330192\ttest: 0.1353129\tbest: 0.1353126 (119)\ttotal: 3s\tremaining: 4m 4s\n",
      "140:\tlearn: 0.1323898\ttest: 0.1348815\tbest: 0.1348815 (140)\ttotal: 3.47s\tremaining: 4m 2s\n",
      "160:\tlearn: 0.1318393\ttest: 0.1345638\tbest: 0.1345638 (160)\ttotal: 3.95s\tremaining: 4m 1s\n",
      "180:\tlearn: 0.1313264\ttest: 0.1343556\tbest: 0.1343556 (180)\ttotal: 4.4s\tremaining: 3m 58s\n",
      "200:\tlearn: 0.1307932\ttest: 0.1341248\tbest: 0.1341000 (195)\ttotal: 4.85s\tremaining: 3m 56s\n",
      "220:\tlearn: 0.1300818\ttest: 0.1337736\tbest: 0.1337708 (219)\ttotal: 5.34s\tremaining: 3m 56s\n",
      "240:\tlearn: 0.1295292\ttest: 0.1335498\tbest: 0.1335498 (240)\ttotal: 5.88s\tremaining: 3m 57s\n",
      "260:\tlearn: 0.1290658\ttest: 0.1334177\tbest: 0.1334177 (260)\ttotal: 6.39s\tremaining: 3m 58s\n",
      "280:\tlearn: 0.1287506\ttest: 0.1333480\tbest: 0.1333440 (275)\ttotal: 6.9s\tremaining: 3m 58s\n",
      "300:\tlearn: 0.1282226\ttest: 0.1332260\tbest: 0.1331974 (296)\ttotal: 7.38s\tremaining: 3m 57s\n",
      "320:\tlearn: 0.1276819\ttest: 0.1331577\tbest: 0.1331431 (315)\ttotal: 7.86s\tremaining: 3m 57s\n",
      "340:\tlearn: 0.1272585\ttest: 0.1331218\tbest: 0.1331011 (333)\ttotal: 8.31s\tremaining: 3m 55s\n",
      "360:\tlearn: 0.1267997\ttest: 0.1330084\tbest: 0.1330084 (360)\ttotal: 8.78s\tremaining: 3m 54s\n",
      "380:\tlearn: 0.1263972\ttest: 0.1329060\tbest: 0.1329031 (379)\ttotal: 9.28s\tremaining: 3m 54s\n",
      "400:\tlearn: 0.1259768\ttest: 0.1328148\tbest: 0.1328053 (399)\ttotal: 9.73s\tremaining: 3m 53s\n",
      "420:\tlearn: 0.1255525\ttest: 0.1327826\tbest: 0.1327826 (420)\ttotal: 10.2s\tremaining: 3m 52s\n",
      "440:\tlearn: 0.1251885\ttest: 0.1327001\tbest: 0.1326996 (439)\ttotal: 10.7s\tremaining: 3m 51s\n",
      "460:\tlearn: 0.1248100\ttest: 0.1327413\tbest: 0.1326872 (452)\ttotal: 11.2s\tremaining: 3m 51s\n",
      "480:\tlearn: 0.1244806\ttest: 0.1327194\tbest: 0.1326872 (452)\ttotal: 11.7s\tremaining: 3m 50s\n",
      "500:\tlearn: 0.1241033\ttest: 0.1326892\tbest: 0.1326755 (492)\ttotal: 12.1s\tremaining: 3m 49s\n",
      "520:\tlearn: 0.1235628\ttest: 0.1326511\tbest: 0.1326504 (519)\ttotal: 12.6s\tremaining: 3m 49s\n",
      "540:\tlearn: 0.1230269\ttest: 0.1326824\tbest: 0.1326435 (521)\ttotal: 13.1s\tremaining: 3m 49s\n",
      "560:\tlearn: 0.1227592\ttest: 0.1326253\tbest: 0.1326198 (559)\ttotal: 13.6s\tremaining: 3m 48s\n",
      "580:\tlearn: 0.1223449\ttest: 0.1325711\tbest: 0.1325664 (577)\ttotal: 14.1s\tremaining: 3m 47s\n",
      "600:\tlearn: 0.1219630\ttest: 0.1325170\tbest: 0.1325170 (600)\ttotal: 14.5s\tremaining: 3m 47s\n",
      "620:\tlearn: 0.1215097\ttest: 0.1325341\tbest: 0.1325170 (600)\ttotal: 15s\tremaining: 3m 46s\n",
      "640:\tlearn: 0.1210876\ttest: 0.1324488\tbest: 0.1324391 (635)\ttotal: 15.6s\tremaining: 3m 47s\n",
      "660:\tlearn: 0.1207977\ttest: 0.1324878\tbest: 0.1324137 (650)\ttotal: 16.1s\tremaining: 3m 47s\n",
      "680:\tlearn: 0.1204772\ttest: 0.1324657\tbest: 0.1324137 (650)\ttotal: 16.5s\tremaining: 3m 46s\n",
      "700:\tlearn: 0.1200707\ttest: 0.1325122\tbest: 0.1324137 (650)\ttotal: 17.1s\tremaining: 3m 47s\n",
      "720:\tlearn: 0.1197383\ttest: 0.1325159\tbest: 0.1324137 (650)\ttotal: 17.6s\tremaining: 3m 46s\n",
      "740:\tlearn: 0.1193984\ttest: 0.1325032\tbest: 0.1324137 (650)\ttotal: 18.1s\tremaining: 3m 46s\n",
      "760:\tlearn: 0.1189911\ttest: 0.1325899\tbest: 0.1324137 (650)\ttotal: 18.7s\tremaining: 3m 46s\n",
      "780:\tlearn: 0.1187627\ttest: 0.1325644\tbest: 0.1324137 (650)\ttotal: 19.3s\tremaining: 3m 47s\n",
      "800:\tlearn: 0.1184962\ttest: 0.1326356\tbest: 0.1324137 (650)\ttotal: 19.9s\tremaining: 3m 48s\n",
      "820:\tlearn: 0.1181980\ttest: 0.1326738\tbest: 0.1324137 (650)\ttotal: 20.5s\tremaining: 3m 49s\n",
      "840:\tlearn: 0.1178193\ttest: 0.1327192\tbest: 0.1324137 (650)\ttotal: 21.1s\tremaining: 3m 49s\n",
      "860:\tlearn: 0.1174481\ttest: 0.1328255\tbest: 0.1324137 (650)\ttotal: 21.7s\tremaining: 3m 50s\n",
      "\n",
      "bestTest = 0.1324137199\n",
      "bestIteration = 650\n",
      "\n",
      "Shrink model to first 651 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6120029\ttest: 0.6119057\tbest: 0.6119057 (0)\ttotal: 29.6ms\tremaining: 4m 55s\n",
      "20:\tlearn: 0.1757325\ttest: 0.1749925\tbest: 0.1749925 (20)\ttotal: 558ms\tremaining: 4m 25s\n",
      "40:\tlearn: 0.1421080\ttest: 0.1418305\tbest: 0.1418305 (40)\ttotal: 1.1s\tremaining: 4m 27s\n",
      "60:\tlearn: 0.1362431\ttest: 0.1371132\tbest: 0.1371132 (60)\ttotal: 1.71s\tremaining: 4m 38s\n",
      "80:\tlearn: 0.1342439\ttest: 0.1361577\tbest: 0.1361421 (79)\ttotal: 2.27s\tremaining: 4m 38s\n",
      "100:\tlearn: 0.1335471\ttest: 0.1358541\tbest: 0.1358541 (100)\ttotal: 2.77s\tremaining: 4m 31s\n",
      "120:\tlearn: 0.1326371\ttest: 0.1356188\tbest: 0.1356188 (120)\ttotal: 3.27s\tremaining: 4m 26s\n",
      "140:\tlearn: 0.1319261\ttest: 0.1354128\tbest: 0.1354128 (140)\ttotal: 3.76s\tremaining: 4m 22s\n",
      "160:\tlearn: 0.1311835\ttest: 0.1352455\tbest: 0.1352436 (149)\ttotal: 4.27s\tremaining: 4m 20s\n",
      "180:\tlearn: 0.1306275\ttest: 0.1352416\tbest: 0.1351919 (175)\ttotal: 4.79s\tremaining: 4m 19s\n",
      "200:\tlearn: 0.1302114\ttest: 0.1351485\tbest: 0.1351485 (200)\ttotal: 5.3s\tremaining: 4m 18s\n",
      "220:\tlearn: 0.1297247\ttest: 0.1351648\tbest: 0.1351146 (204)\ttotal: 5.79s\tremaining: 4m 16s\n",
      "240:\tlearn: 0.1291428\ttest: 0.1350596\tbest: 0.1350567 (239)\ttotal: 6.29s\tremaining: 4m 14s\n",
      "260:\tlearn: 0.1285362\ttest: 0.1349732\tbest: 0.1349732 (260)\ttotal: 6.77s\tremaining: 4m 12s\n",
      "280:\tlearn: 0.1282002\ttest: 0.1350053\tbest: 0.1349585 (269)\ttotal: 7.22s\tremaining: 4m 9s\n",
      "300:\tlearn: 0.1277818\ttest: 0.1349909\tbest: 0.1349585 (269)\ttotal: 7.68s\tremaining: 4m 7s\n",
      "320:\tlearn: 0.1273190\ttest: 0.1349567\tbest: 0.1349550 (312)\ttotal: 8.16s\tremaining: 4m 6s\n",
      "340:\tlearn: 0.1269623\ttest: 0.1349468\tbest: 0.1348913 (334)\ttotal: 8.61s\tremaining: 4m 3s\n",
      "360:\tlearn: 0.1264628\ttest: 0.1349126\tbest: 0.1348913 (334)\ttotal: 9.09s\tremaining: 4m 2s\n",
      "380:\tlearn: 0.1260495\ttest: 0.1349023\tbest: 0.1348778 (377)\ttotal: 9.55s\tremaining: 4m 1s\n",
      "400:\tlearn: 0.1256045\ttest: 0.1349809\tbest: 0.1348778 (377)\ttotal: 10s\tremaining: 3m 59s\n",
      "420:\tlearn: 0.1251725\ttest: 0.1349282\tbest: 0.1348778 (377)\ttotal: 10.5s\tremaining: 3m 59s\n",
      "440:\tlearn: 0.1247899\ttest: 0.1348859\tbest: 0.1348778 (377)\ttotal: 11s\tremaining: 3m 57s\n",
      "460:\tlearn: 0.1244371\ttest: 0.1349045\tbest: 0.1348778 (377)\ttotal: 11.4s\tremaining: 3m 56s\n",
      "480:\tlearn: 0.1241044\ttest: 0.1349502\tbest: 0.1348778 (377)\ttotal: 11.9s\tremaining: 3m 55s\n",
      "500:\tlearn: 0.1236079\ttest: 0.1349182\tbest: 0.1348778 (377)\ttotal: 12.4s\tremaining: 3m 55s\n",
      "\n",
      "bestTest = 0.1348777944\n",
      "bestIteration = 377\n",
      "\n",
      "Shrink model to first 378 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6217531\ttest: 0.6218884\tbest: 0.6218884 (0)\ttotal: 22.6ms\tremaining: 3m 45s\n",
      "20:\tlearn: 0.1789621\ttest: 0.1788967\tbest: 0.1788967 (20)\ttotal: 498ms\tremaining: 3m 56s\n",
      "40:\tlearn: 0.1422417\ttest: 0.1424378\tbest: 0.1424378 (40)\ttotal: 1.04s\tremaining: 4m 12s\n",
      "60:\tlearn: 0.1360086\ttest: 0.1367465\tbest: 0.1367465 (60)\ttotal: 1.56s\tremaining: 4m 14s\n",
      "80:\tlearn: 0.1341539\ttest: 0.1354385\tbest: 0.1354385 (80)\ttotal: 2.05s\tremaining: 4m 11s\n",
      "100:\tlearn: 0.1332901\ttest: 0.1349975\tbest: 0.1349975 (99)\ttotal: 2.52s\tremaining: 4m 7s\n",
      "120:\tlearn: 0.1325924\ttest: 0.1346755\tbest: 0.1346755 (120)\ttotal: 3.01s\tremaining: 4m 5s\n",
      "140:\tlearn: 0.1319537\ttest: 0.1344853\tbest: 0.1344853 (140)\ttotal: 3.46s\tremaining: 4m 2s\n",
      "160:\tlearn: 0.1314241\ttest: 0.1343781\tbest: 0.1343768 (159)\ttotal: 3.92s\tremaining: 3m 59s\n",
      "180:\tlearn: 0.1307624\ttest: 0.1341967\tbest: 0.1341967 (180)\ttotal: 4.42s\tremaining: 3m 59s\n",
      "200:\tlearn: 0.1302444\ttest: 0.1340889\tbest: 0.1340781 (198)\ttotal: 4.91s\tremaining: 3m 59s\n",
      "220:\tlearn: 0.1296497\ttest: 0.1339516\tbest: 0.1339414 (219)\ttotal: 5.42s\tremaining: 3m 59s\n",
      "240:\tlearn: 0.1291821\ttest: 0.1338629\tbest: 0.1338566 (235)\ttotal: 5.9s\tremaining: 3m 58s\n",
      "260:\tlearn: 0.1285928\ttest: 0.1338597\tbest: 0.1338523 (241)\ttotal: 6.39s\tremaining: 3m 58s\n",
      "280:\tlearn: 0.1280855\ttest: 0.1339119\tbest: 0.1338363 (262)\ttotal: 6.88s\tremaining: 3m 58s\n",
      "300:\tlearn: 0.1275323\ttest: 0.1338782\tbest: 0.1338363 (262)\ttotal: 7.35s\tremaining: 3m 56s\n",
      "320:\tlearn: 0.1270700\ttest: 0.1339632\tbest: 0.1338363 (262)\ttotal: 7.8s\tremaining: 3m 55s\n",
      "340:\tlearn: 0.1266114\ttest: 0.1339879\tbest: 0.1338363 (262)\ttotal: 8.29s\tremaining: 3m 54s\n",
      "360:\tlearn: 0.1262224\ttest: 0.1339646\tbest: 0.1338363 (262)\ttotal: 8.75s\tremaining: 3m 53s\n",
      "\n",
      "bestTest = 0.1338362764\n",
      "bestIteration = 262\n",
      "\n",
      "Shrink model to first 263 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6228370\ttest: 0.6229039\tbest: 0.6229039 (0)\ttotal: 21.3ms\tremaining: 3m 32s\n",
      "20:\tlearn: 0.1773624\ttest: 0.1776986\tbest: 0.1776986 (20)\ttotal: 480ms\tremaining: 3m 48s\n",
      "40:\tlearn: 0.1423333\ttest: 0.1433479\tbest: 0.1433479 (40)\ttotal: 1.02s\tremaining: 4m 8s\n",
      "60:\tlearn: 0.1361909\ttest: 0.1378679\tbest: 0.1378679 (60)\ttotal: 1.56s\tremaining: 4m 14s\n",
      "80:\tlearn: 0.1340415\ttest: 0.1362341\tbest: 0.1362341 (80)\ttotal: 2.06s\tremaining: 4m 11s\n",
      "100:\tlearn: 0.1334353\ttest: 0.1357721\tbest: 0.1357721 (100)\ttotal: 2.49s\tremaining: 4m 4s\n",
      "120:\tlearn: 0.1327593\ttest: 0.1354270\tbest: 0.1354270 (120)\ttotal: 2.96s\tremaining: 4m 1s\n",
      "140:\tlearn: 0.1320318\ttest: 0.1350306\tbest: 0.1350221 (138)\ttotal: 3.43s\tremaining: 3m 59s\n",
      "160:\tlearn: 0.1313356\ttest: 0.1347715\tbest: 0.1347715 (160)\ttotal: 3.89s\tremaining: 3m 57s\n",
      "180:\tlearn: 0.1307979\ttest: 0.1345583\tbest: 0.1345555 (179)\ttotal: 4.35s\tremaining: 3m 56s\n",
      "200:\tlearn: 0.1302223\ttest: 0.1343350\tbest: 0.1343247 (198)\ttotal: 4.82s\tremaining: 3m 55s\n",
      "220:\tlearn: 0.1297181\ttest: 0.1341934\tbest: 0.1341934 (220)\ttotal: 5.29s\tremaining: 3m 54s\n",
      "240:\tlearn: 0.1292405\ttest: 0.1340854\tbest: 0.1340759 (236)\ttotal: 5.74s\tremaining: 3m 52s\n",
      "260:\tlearn: 0.1287296\ttest: 0.1339153\tbest: 0.1339153 (260)\ttotal: 6.18s\tremaining: 3m 50s\n",
      "280:\tlearn: 0.1283045\ttest: 0.1338626\tbest: 0.1338624 (279)\ttotal: 6.63s\tremaining: 3m 49s\n",
      "300:\tlearn: 0.1279021\ttest: 0.1337997\tbest: 0.1337907 (298)\ttotal: 7.1s\tremaining: 3m 48s\n",
      "320:\tlearn: 0.1273980\ttest: 0.1336661\tbest: 0.1336661 (320)\ttotal: 7.59s\tremaining: 3m 48s\n",
      "340:\tlearn: 0.1269517\ttest: 0.1336178\tbest: 0.1336178 (340)\ttotal: 8.06s\tremaining: 3m 48s\n",
      "360:\tlearn: 0.1265641\ttest: 0.1335773\tbest: 0.1335743 (358)\ttotal: 8.55s\tremaining: 3m 48s\n",
      "380:\tlearn: 0.1262185\ttest: 0.1336134\tbest: 0.1335632 (361)\ttotal: 9.03s\tremaining: 3m 48s\n",
      "400:\tlearn: 0.1258224\ttest: 0.1336061\tbest: 0.1335632 (361)\ttotal: 9.54s\tremaining: 3m 48s\n",
      "420:\tlearn: 0.1254425\ttest: 0.1335899\tbest: 0.1335608 (419)\ttotal: 10.1s\tremaining: 3m 49s\n",
      "440:\tlearn: 0.1250744\ttest: 0.1335682\tbest: 0.1335477 (428)\ttotal: 10.7s\tremaining: 3m 50s\n",
      "460:\tlearn: 0.1247046\ttest: 0.1335669\tbest: 0.1335477 (428)\ttotal: 11.2s\tremaining: 3m 51s\n",
      "480:\tlearn: 0.1243310\ttest: 0.1335696\tbest: 0.1335477 (428)\ttotal: 11.9s\tremaining: 3m 55s\n",
      "500:\tlearn: 0.1239862\ttest: 0.1335798\tbest: 0.1335477 (428)\ttotal: 12.5s\tremaining: 3m 56s\n",
      "520:\tlearn: 0.1236274\ttest: 0.1335768\tbest: 0.1335477 (428)\ttotal: 13.1s\tremaining: 3m 59s\n",
      "540:\tlearn: 0.1232524\ttest: 0.1335322\tbest: 0.1335273 (536)\ttotal: 13.8s\tremaining: 4m\n",
      "560:\tlearn: 0.1228952\ttest: 0.1334691\tbest: 0.1334691 (560)\ttotal: 14.3s\tremaining: 4m\n",
      "580:\tlearn: 0.1225932\ttest: 0.1334505\tbest: 0.1334477 (579)\ttotal: 15s\tremaining: 4m 2s\n",
      "600:\tlearn: 0.1222083\ttest: 0.1335043\tbest: 0.1334277 (584)\ttotal: 15.6s\tremaining: 4m 4s\n",
      "620:\tlearn: 0.1217029\ttest: 0.1335418\tbest: 0.1334277 (584)\ttotal: 16.2s\tremaining: 4m 4s\n",
      "640:\tlearn: 0.1213341\ttest: 0.1335953\tbest: 0.1334277 (584)\ttotal: 16.7s\tremaining: 4m 4s\n",
      "660:\tlearn: 0.1210218\ttest: 0.1335982\tbest: 0.1334277 (584)\ttotal: 17.3s\tremaining: 4m 4s\n",
      "680:\tlearn: 0.1207514\ttest: 0.1335574\tbest: 0.1334277 (584)\ttotal: 17.9s\tremaining: 4m 5s\n",
      "700:\tlearn: 0.1204627\ttest: 0.1335619\tbest: 0.1334277 (584)\ttotal: 18.6s\tremaining: 4m 6s\n",
      "720:\tlearn: 0.1201880\ttest: 0.1335598\tbest: 0.1334277 (584)\ttotal: 19.2s\tremaining: 4m 7s\n",
      "740:\tlearn: 0.1198920\ttest: 0.1336018\tbest: 0.1334277 (584)\ttotal: 19.8s\tremaining: 4m 7s\n",
      "760:\tlearn: 0.1195086\ttest: 0.1336837\tbest: 0.1334277 (584)\ttotal: 20.4s\tremaining: 4m 7s\n",
      "780:\tlearn: 0.1190685\ttest: 0.1337553\tbest: 0.1334277 (584)\ttotal: 21s\tremaining: 4m 7s\n",
      "\n",
      "bestTest = 0.1334276738\n",
      "bestIteration = 584\n",
      "\n",
      "Shrink model to first 585 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6166208\ttest: 0.6167040\tbest: 0.6167040 (0)\ttotal: 25.1ms\tremaining: 4m 10s\n",
      "20:\tlearn: 0.1794642\ttest: 0.1793907\tbest: 0.1793907 (20)\ttotal: 516ms\tremaining: 4m 5s\n",
      "40:\tlearn: 0.1428036\ttest: 0.1425874\tbest: 0.1425874 (40)\ttotal: 1.11s\tremaining: 4m 28s\n",
      "60:\tlearn: 0.1366578\ttest: 0.1366345\tbest: 0.1366345 (60)\ttotal: 1.66s\tremaining: 4m 31s\n",
      "80:\tlearn: 0.1345782\ttest: 0.1348645\tbest: 0.1348645 (80)\ttotal: 2.21s\tremaining: 4m 31s\n",
      "100:\tlearn: 0.1338270\ttest: 0.1342939\tbest: 0.1342939 (100)\ttotal: 2.69s\tremaining: 4m 24s\n",
      "120:\tlearn: 0.1331561\ttest: 0.1337855\tbest: 0.1337855 (120)\ttotal: 3.21s\tremaining: 4m 21s\n",
      "140:\tlearn: 0.1323917\ttest: 0.1334218\tbest: 0.1334157 (139)\ttotal: 3.71s\tremaining: 4m 19s\n",
      "160:\tlearn: 0.1317276\ttest: 0.1331008\tbest: 0.1331008 (160)\ttotal: 4.23s\tremaining: 4m 18s\n",
      "180:\tlearn: 0.1310554\ttest: 0.1327399\tbest: 0.1327399 (180)\ttotal: 4.8s\tremaining: 4m 20s\n",
      "200:\tlearn: 0.1306782\ttest: 0.1326263\tbest: 0.1326236 (199)\ttotal: 5.28s\tremaining: 4m 17s\n",
      "220:\tlearn: 0.1301714\ttest: 0.1325615\tbest: 0.1325421 (218)\ttotal: 5.77s\tremaining: 4m 15s\n",
      "240:\tlearn: 0.1296866\ttest: 0.1324390\tbest: 0.1324390 (240)\ttotal: 6.27s\tremaining: 4m 13s\n",
      "260:\tlearn: 0.1290718\ttest: 0.1322560\tbest: 0.1322560 (260)\ttotal: 6.79s\tremaining: 4m 13s\n",
      "280:\tlearn: 0.1286327\ttest: 0.1321910\tbest: 0.1321737 (272)\ttotal: 7.28s\tremaining: 4m 11s\n",
      "300:\tlearn: 0.1281996\ttest: 0.1320827\tbest: 0.1320827 (300)\ttotal: 7.77s\tremaining: 4m 10s\n",
      "320:\tlearn: 0.1276860\ttest: 0.1320686\tbest: 0.1320674 (319)\ttotal: 8.29s\tremaining: 4m 10s\n",
      "340:\tlearn: 0.1273053\ttest: 0.1320239\tbest: 0.1320239 (340)\ttotal: 8.79s\tremaining: 4m 8s\n",
      "360:\tlearn: 0.1267797\ttest: 0.1320304\tbest: 0.1320015 (353)\ttotal: 9.37s\tremaining: 4m 10s\n",
      "380:\tlearn: 0.1264345\ttest: 0.1320175\tbest: 0.1320015 (353)\ttotal: 9.88s\tremaining: 4m 9s\n",
      "400:\tlearn: 0.1260173\ttest: 0.1319382\tbest: 0.1319300 (399)\ttotal: 10.4s\tremaining: 4m 8s\n",
      "420:\tlearn: 0.1256342\ttest: 0.1319629\tbest: 0.1319300 (399)\ttotal: 10.9s\tremaining: 4m 7s\n",
      "440:\tlearn: 0.1251938\ttest: 0.1320344\tbest: 0.1319300 (399)\ttotal: 11.4s\tremaining: 4m 6s\n",
      "460:\tlearn: 0.1246512\ttest: 0.1320372\tbest: 0.1319300 (399)\ttotal: 11.9s\tremaining: 4m 6s\n",
      "480:\tlearn: 0.1240986\ttest: 0.1320728\tbest: 0.1319300 (399)\ttotal: 12.4s\tremaining: 4m 5s\n",
      "500:\tlearn: 0.1236884\ttest: 0.1320788\tbest: 0.1319300 (399)\ttotal: 12.9s\tremaining: 4m 5s\n",
      "520:\tlearn: 0.1233499\ttest: 0.1320489\tbest: 0.1319300 (399)\ttotal: 13.4s\tremaining: 4m 4s\n",
      "\n",
      "bestTest = 0.1319300286\n",
      "bestIteration = 399\n",
      "\n",
      "Shrink model to first 400 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6206391\ttest: 0.6207366\tbest: 0.6207366 (0)\ttotal: 28ms\tremaining: 4m 40s\n",
      "20:\tlearn: 0.1749892\ttest: 0.1756581\tbest: 0.1756581 (20)\ttotal: 517ms\tremaining: 4m 5s\n",
      "40:\tlearn: 0.1425182\ttest: 0.1441959\tbest: 0.1441959 (40)\ttotal: 1.08s\tremaining: 4m 22s\n",
      "60:\tlearn: 0.1361444\ttest: 0.1390339\tbest: 0.1390339 (60)\ttotal: 1.65s\tremaining: 4m 28s\n",
      "80:\tlearn: 0.1340673\ttest: 0.1379707\tbest: 0.1379707 (80)\ttotal: 2.26s\tremaining: 4m 36s\n",
      "100:\tlearn: 0.1329660\ttest: 0.1374459\tbest: 0.1374459 (100)\ttotal: 2.76s\tremaining: 4m 30s\n",
      "120:\tlearn: 0.1322603\ttest: 0.1372559\tbest: 0.1372559 (120)\ttotal: 3.24s\tremaining: 4m 24s\n",
      "140:\tlearn: 0.1314757\ttest: 0.1370835\tbest: 0.1370835 (140)\ttotal: 3.75s\tremaining: 4m 21s\n",
      "160:\tlearn: 0.1308178\ttest: 0.1369279\tbest: 0.1369279 (160)\ttotal: 4.25s\tremaining: 4m 20s\n",
      "180:\tlearn: 0.1302356\ttest: 0.1369200\tbest: 0.1369200 (180)\ttotal: 4.73s\tremaining: 4m 16s\n",
      "200:\tlearn: 0.1297180\ttest: 0.1368782\tbest: 0.1368743 (198)\ttotal: 5.21s\tremaining: 4m 14s\n",
      "220:\tlearn: 0.1292653\ttest: 0.1368762\tbest: 0.1368497 (212)\ttotal: 5.79s\tremaining: 4m 15s\n",
      "240:\tlearn: 0.1287114\ttest: 0.1368316\tbest: 0.1368316 (240)\ttotal: 6.36s\tremaining: 4m 17s\n",
      "260:\tlearn: 0.1282520\ttest: 0.1367279\tbest: 0.1367239 (253)\ttotal: 6.91s\tremaining: 4m 17s\n",
      "280:\tlearn: 0.1277962\ttest: 0.1366344\tbest: 0.1366344 (280)\ttotal: 7.42s\tremaining: 4m 16s\n",
      "300:\tlearn: 0.1271801\ttest: 0.1365785\tbest: 0.1365704 (299)\ttotal: 7.93s\tremaining: 4m 15s\n",
      "320:\tlearn: 0.1267529\ttest: 0.1365778\tbest: 0.1365704 (299)\ttotal: 8.42s\tremaining: 4m 13s\n",
      "340:\tlearn: 0.1263098\ttest: 0.1365334\tbest: 0.1365159 (331)\ttotal: 8.93s\tremaining: 4m 13s\n",
      "360:\tlearn: 0.1259161\ttest: 0.1365370\tbest: 0.1365159 (331)\ttotal: 9.44s\tremaining: 4m 12s\n",
      "380:\tlearn: 0.1255092\ttest: 0.1365544\tbest: 0.1365114 (368)\ttotal: 9.94s\tremaining: 4m 11s\n",
      "400:\tlearn: 0.1250560\ttest: 0.1364935\tbest: 0.1364599 (393)\ttotal: 10.5s\tremaining: 4m 11s\n",
      "420:\tlearn: 0.1244836\ttest: 0.1364432\tbest: 0.1363953 (407)\ttotal: 11.1s\tremaining: 4m 11s\n",
      "440:\tlearn: 0.1240056\ttest: 0.1364099\tbest: 0.1363953 (407)\ttotal: 11.6s\tremaining: 4m 10s\n",
      "460:\tlearn: 0.1235855\ttest: 0.1364317\tbest: 0.1363953 (407)\ttotal: 12.1s\tremaining: 4m 9s\n",
      "480:\tlearn: 0.1231550\ttest: 0.1364882\tbest: 0.1363953 (407)\ttotal: 12.6s\tremaining: 4m 8s\n",
      "500:\tlearn: 0.1227380\ttest: 0.1364040\tbest: 0.1363953 (407)\ttotal: 13.1s\tremaining: 4m 7s\n",
      "520:\tlearn: 0.1222683\ttest: 0.1364291\tbest: 0.1363953 (407)\ttotal: 13.6s\tremaining: 4m 7s\n",
      "540:\tlearn: 0.1218868\ttest: 0.1363512\tbest: 0.1363512 (540)\ttotal: 14.1s\tremaining: 4m 6s\n",
      "560:\tlearn: 0.1213670\ttest: 0.1363127\tbest: 0.1363035 (545)\ttotal: 14.6s\tremaining: 4m 6s\n",
      "580:\tlearn: 0.1210121\ttest: 0.1363373\tbest: 0.1363035 (545)\ttotal: 15.2s\tremaining: 4m 5s\n",
      "600:\tlearn: 0.1205945\ttest: 0.1362462\tbest: 0.1361931 (594)\ttotal: 15.7s\tremaining: 4m 5s\n",
      "620:\tlearn: 0.1202464\ttest: 0.1362517\tbest: 0.1361931 (594)\ttotal: 16.2s\tremaining: 4m 4s\n",
      "640:\tlearn: 0.1198375\ttest: 0.1361803\tbest: 0.1361803 (640)\ttotal: 16.8s\tremaining: 4m 4s\n",
      "660:\tlearn: 0.1194367\ttest: 0.1361626\tbest: 0.1361497 (649)\ttotal: 17.4s\tremaining: 4m 5s\n",
      "680:\tlearn: 0.1192074\ttest: 0.1361779\tbest: 0.1361497 (649)\ttotal: 17.9s\tremaining: 4m 5s\n",
      "700:\tlearn: 0.1189125\ttest: 0.1362317\tbest: 0.1361497 (649)\ttotal: 18.5s\tremaining: 4m 5s\n",
      "720:\tlearn: 0.1185152\ttest: 0.1361876\tbest: 0.1361497 (649)\ttotal: 19.1s\tremaining: 4m 5s\n",
      "740:\tlearn: 0.1182297\ttest: 0.1361663\tbest: 0.1361497 (649)\ttotal: 19.6s\tremaining: 4m 5s\n",
      "760:\tlearn: 0.1177949\ttest: 0.1362148\tbest: 0.1361497 (649)\ttotal: 20.2s\tremaining: 4m 5s\n",
      "780:\tlearn: 0.1173717\ttest: 0.1363559\tbest: 0.1361497 (649)\ttotal: 20.8s\tremaining: 4m 5s\n",
      "800:\tlearn: 0.1169800\ttest: 0.1363519\tbest: 0.1361497 (649)\ttotal: 21.3s\tremaining: 4m 4s\n",
      "820:\tlearn: 0.1167998\ttest: 0.1364095\tbest: 0.1361497 (649)\ttotal: 21.8s\tremaining: 4m 3s\n",
      "840:\tlearn: 0.1165221\ttest: 0.1364067\tbest: 0.1361497 (649)\ttotal: 22.3s\tremaining: 4m 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "860:\tlearn: 0.1162079\ttest: 0.1364969\tbest: 0.1361497 (649)\ttotal: 22.8s\tremaining: 4m 1s\n",
      "\n",
      "bestTest = 0.1361496614\n",
      "bestIteration = 649\n",
      "\n",
      "Shrink model to first 650 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "\t0.8406\t = Validation score   (roc_auc)\n",
      "\t245.65s\t = Training   runtime\n",
      "\t0.66s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\model.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 3862.85s of the 3862.83s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit NNFastAiTabularModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S2F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 112.22 / 382.33 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.3534407317638397.\n",
      "Better model found at epoch 1 with valid_loss value: 0.15639828145503998.\n",
      "Better model found at epoch 2 with valid_loss value: 0.14377093315124512.\n",
      "Better model found at epoch 17 with valid_loss value: 0.14252854883670807.\n",
      "Model validation metrics: 0.14252854883670807\n",
      "\tFitting S2F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 332 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=332, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 98.99 / 425.80 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.17584647238254547.\n",
      "Better model found at epoch 1 with valid_loss value: 0.1419985443353653.\n",
      "Better model found at epoch 3 with valid_loss value: 0.13810798525810242.\n",
      "Better model found at epoch 9 with valid_loss value: 0.13731810450553894.\n",
      "No improvement since epoch 9: early stopping\n",
      "Model validation metrics: 0.13731810450553894\n",
      "\tFitting S2F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 108.23 / 484.53 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.17957955598831177.\n",
      "Better model found at epoch 1 with valid_loss value: 0.15327461063861847.\n",
      "Better model found at epoch 4 with valid_loss value: 0.14230524003505707.\n",
      "Better model found at epoch 5 with valid_loss value: 0.13859042525291443.\n",
      "Better model found at epoch 10 with valid_loss value: 0.13835681974887848.\n",
      "Better model found at epoch 12 with valid_loss value: 0.13668088614940643.\n",
      "Model validation metrics: 0.13668088614940643\n",
      "\tFitting S2F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 331 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=331, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 109.02 / 566.52 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.17133699357509613.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14657962322235107.\n",
      "Better model found at epoch 4 with valid_loss value: 0.14216016232967377.\n",
      "Better model found at epoch 5 with valid_loss value: 0.14074422419071198.\n",
      "Better model found at epoch 7 with valid_loss value: 0.1384613960981369.\n",
      "Better model found at epoch 12 with valid_loss value: 0.13743221759796143.\n",
      "Model validation metrics: 0.13743221759796143\n",
      "\tFitting S2F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 107.81 / 688.73 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.19081784784793854.\n",
      "Better model found at epoch 1 with valid_loss value: 0.1494741290807724.\n",
      "Better model found at epoch 2 with valid_loss value: 0.14273816347122192.\n",
      "Better model found at epoch 4 with valid_loss value: 0.14033159613609314.\n",
      "Better model found at epoch 5 with valid_loss value: 0.1402582824230194.\n",
      "Better model found at epoch 6 with valid_loss value: 0.13882951438426971.\n",
      "Better model found at epoch 12 with valid_loss value: 0.13778936862945557.\n",
      "Better model found at epoch 15 with valid_loss value: 0.13700532913208008.\n",
      "Model validation metrics: 0.13700532913208008\n",
      "\tFitting S2F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 111.51 / 892.32 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.17135493457317352.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14666976034641266.\n",
      "Better model found at epoch 2 with valid_loss value: 0.14317536354064941.\n",
      "Better model found at epoch 3 with valid_loss value: 0.1388997584581375.\n",
      "Better model found at epoch 13 with valid_loss value: 0.1371951848268509.\n",
      "Model validation metrics: 0.1371951848268509\n",
      "\tFitting S2F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 334 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(334, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=334, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 115.41 / 1299.10 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.2754596769809723.\n",
      "Better model found at epoch 1 with valid_loss value: 0.17208237946033478.\n",
      "Better model found at epoch 3 with valid_loss value: 0.1610356718301773.\n",
      "Better model found at epoch 4 with valid_loss value: 0.14180244505405426.\n",
      "Better model found at epoch 11 with valid_loss value: 0.13939279317855835.\n",
      "Better model found at epoch 15 with valid_loss value: 0.13844414055347443.\n",
      "Better model found at epoch 16 with valid_loss value: 0.1377469152212143.\n",
      "Model validation metrics: 0.1377469152212143\n",
      "\tFitting S2F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 106.23 / 2521.38 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.1798326075077057.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14757809042930603.\n",
      "Better model found at epoch 3 with valid_loss value: 0.1423959881067276.\n",
      "No improvement since epoch 3: early stopping\n",
      "Model validation metrics: 0.1423959881067276\n",
      "Saving AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\model.pkl\n",
      "\t0.8292\t = Validation score   (roc_auc)\n",
      "\t1466.73s\t = Training   runtime\n",
      "\t9.35s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 3077.75s of the 3077.74s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit XGBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S2F1 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61168\n",
      "[50]\tvalidation_0-logloss:0.13814\n",
      "[97]\tvalidation_0-logloss:0.13866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F2 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61150\n",
      "[50]\tvalidation_0-logloss:0.13311\n",
      "[100]\tvalidation_0-logloss:0.13264\n",
      "[150]\tvalidation_0-logloss:0.13294\n",
      "[173]\tvalidation_0-logloss:0.13332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F3 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61147\n",
      "[50]\tvalidation_0-logloss:0.13371\n",
      "[100]\tvalidation_0-logloss:0.13301\n",
      "[131]\tvalidation_0-logloss:0.13340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F4 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61140\n",
      "[50]\tvalidation_0-logloss:0.13547\n",
      "[100]\tvalidation_0-logloss:0.13539\n",
      "[114]\tvalidation_0-logloss:0.13533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F5 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61146\n",
      "[50]\tvalidation_0-logloss:0.13474\n",
      "[100]\tvalidation_0-logloss:0.13460\n",
      "[107]\tvalidation_0-logloss:0.13467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F6 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61142\n",
      "[50]\tvalidation_0-logloss:0.13435\n",
      "[100]\tvalidation_0-logloss:0.13393\n",
      "[117]\tvalidation_0-logloss:0.13420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F7 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61143\n",
      "[50]\tvalidation_0-logloss:0.13267\n",
      "[100]\tvalidation_0-logloss:0.13181\n",
      "[122]\tvalidation_0-logloss:0.13192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F8 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61143\n",
      "[50]\tvalidation_0-logloss:0.13614\n",
      "[100]\tvalidation_0-logloss:0.13607\n",
      "[114]\tvalidation_0-logloss:0.13606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\XGBoost_BAG_L1\\model.pkl\n",
      "\t0.8396\t = Validation score   (roc_auc)\n",
      "\t45.51s\t = Training   runtime\n",
      "\t1.57s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3053.44s of the 3053.42s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S2F1 - S2F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S2F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.139847\n",
      "[100]\tvalid_set's binary_logloss: 0.138571\n",
      "[150]\tvalid_set's binary_logloss: 0.139295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.136028\n",
      "[100]\tvalid_set's binary_logloss: 0.133809\n",
      "[150]\tvalid_set's binary_logloss: 0.13434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.136509\n",
      "[100]\tvalid_set's binary_logloss: 0.134714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.137644\n",
      "[100]\tvalid_set's binary_logloss: 0.13587\n",
      "[150]\tvalid_set's binary_logloss: 0.13606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.13788\n",
      "[100]\tvalid_set's binary_logloss: 0.136318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.137322\n",
      "[100]\tvalid_set's binary_logloss: 0.135029\n",
      "[150]\tvalid_set's binary_logloss: 0.135366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135555\n",
      "[100]\tvalid_set's binary_logloss: 0.132743\n",
      "[150]\tvalid_set's binary_logloss: 0.132804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S2F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.139758\n",
      "[100]\tvalid_set's binary_logloss: 0.137819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t0.8366\t = Validation score   (roc_auc)\n",
      "\t49.68s\t = Training   runtime\n",
      "\t1.12s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 3/20\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150]\tvalid_set's binary_logloss: 0.138029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3027.02s of the 3027.0s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S3F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.139892\n",
      "[100]\tvalid_set's binary_logloss: 0.137269\n",
      "[150]\tvalid_set's binary_logloss: 0.13645\n",
      "[200]\tvalid_set's binary_logloss: 0.136653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.134831\n",
      "[100]\tvalid_set's binary_logloss: 0.131316\n",
      "[150]\tvalid_set's binary_logloss: 0.130781\n",
      "[200]\tvalid_set's binary_logloss: 0.130546\n",
      "[250]\tvalid_set's binary_logloss: 0.130319\n",
      "[300]\tvalid_set's binary_logloss: 0.130234\n",
      "[350]\tvalid_set's binary_logloss: 0.130409\n",
      "[400]\tvalid_set's binary_logloss: 0.130473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.136494\n",
      "[100]\tvalid_set's binary_logloss: 0.133642\n",
      "[150]\tvalid_set's binary_logloss: 0.133268\n",
      "[200]\tvalid_set's binary_logloss: 0.132934\n",
      "[250]\tvalid_set's binary_logloss: 0.132985\n",
      "[300]\tvalid_set's binary_logloss: 0.132973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.134526\n",
      "[100]\tvalid_set's binary_logloss: 0.132006\n",
      "[150]\tvalid_set's binary_logloss: 0.131264\n",
      "[200]\tvalid_set's binary_logloss: 0.1309\n",
      "[250]\tvalid_set's binary_logloss: 0.13076\n",
      "[300]\tvalid_set's binary_logloss: 0.130678\n",
      "[350]\tvalid_set's binary_logloss: 0.130866\n",
      "[400]\tvalid_set's binary_logloss: 0.130999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.13971\n",
      "[100]\tvalid_set's binary_logloss: 0.137991\n",
      "[150]\tvalid_set's binary_logloss: 0.137695\n",
      "[200]\tvalid_set's binary_logloss: 0.137902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.139141\n",
      "[100]\tvalid_set's binary_logloss: 0.137404\n",
      "[150]\tvalid_set's binary_logloss: 0.136831\n",
      "[200]\tvalid_set's binary_logloss: 0.136743\n",
      "[250]\tvalid_set's binary_logloss: 0.136947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135608\n",
      "[100]\tvalid_set's binary_logloss: 0.133136\n",
      "[150]\tvalid_set's binary_logloss: 0.132165\n",
      "[200]\tvalid_set's binary_logloss: 0.132105\n",
      "[250]\tvalid_set's binary_logloss: 0.132147\n",
      "[300]\tvalid_set's binary_logloss: 0.131971\n",
      "[350]\tvalid_set's binary_logloss: 0.132019\n",
      "[400]\tvalid_set's binary_logloss: 0.132112\n",
      "[450]\tvalid_set's binary_logloss: 0.132197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.13732\n",
      "[100]\tvalid_set's binary_logloss: 0.135592\n",
      "[150]\tvalid_set's binary_logloss: 0.13563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t0.8403\t = Validation score   (roc_auc)\n",
      "\t56.6s\t = Training   runtime\n",
      "\t1.95s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 3005.41s of the 3005.4s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S3F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.136653\n",
      "[100]\tvalid_set's binary_logloss: 0.136183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.131367\n",
      "[100]\tvalid_set's binary_logloss: 0.129921\n",
      "[150]\tvalid_set's binary_logloss: 0.130135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.133347\n",
      "[100]\tvalid_set's binary_logloss: 0.132407\n",
      "[150]\tvalid_set's binary_logloss: 0.132602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.133097\n",
      "[100]\tvalid_set's binary_logloss: 0.131975\n",
      "[150]\tvalid_set's binary_logloss: 0.131748\n",
      "[200]\tvalid_set's binary_logloss: 0.131815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138936\n",
      "[100]\tvalid_set's binary_logloss: 0.139022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.136996\n",
      "[100]\tvalid_set's binary_logloss: 0.136698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.13313\n",
      "[100]\tvalid_set's binary_logloss: 0.131809\n",
      "[150]\tvalid_set's binary_logloss: 0.131769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135706\n",
      "[100]\tvalid_set's binary_logloss: 0.135081\n",
      "[150]\tvalid_set's binary_logloss: 0.135247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t0.8408\t = Validation score   (roc_auc)\n",
      "\t44.14s\t = Training   runtime\n",
      "\t1.47s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2988.89s of the 2988.87s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S3F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6220502\ttest: 0.6221436\tbest: 0.6221436 (0)\ttotal: 23.2ms\tremaining: 3m 51s\n",
      "20:\tlearn: 0.1788171\ttest: 0.1796769\tbest: 0.1796769 (20)\ttotal: 492ms\tremaining: 3m 53s\n",
      "40:\tlearn: 0.1423052\ttest: 0.1441039\tbest: 0.1441039 (40)\ttotal: 1.03s\tremaining: 4m 10s\n",
      "60:\tlearn: 0.1358676\ttest: 0.1386375\tbest: 0.1386375 (60)\ttotal: 1.57s\tremaining: 4m 15s\n",
      "80:\tlearn: 0.1341104\ttest: 0.1376574\tbest: 0.1376563 (79)\ttotal: 2.06s\tremaining: 4m 12s\n",
      "100:\tlearn: 0.1332446\ttest: 0.1373224\tbest: 0.1373224 (100)\ttotal: 2.55s\tremaining: 4m 10s\n",
      "120:\tlearn: 0.1325128\ttest: 0.1370045\tbest: 0.1370045 (120)\ttotal: 3.02s\tremaining: 4m 6s\n",
      "140:\tlearn: 0.1318997\ttest: 0.1367939\tbest: 0.1367902 (137)\ttotal: 3.48s\tremaining: 4m 3s\n",
      "160:\tlearn: 0.1312449\ttest: 0.1365693\tbest: 0.1365693 (160)\ttotal: 3.96s\tremaining: 4m 1s\n",
      "180:\tlearn: 0.1306384\ttest: 0.1363625\tbest: 0.1363625 (180)\ttotal: 4.44s\tremaining: 4m 1s\n",
      "200:\tlearn: 0.1300655\ttest: 0.1362575\tbest: 0.1362511 (196)\ttotal: 4.91s\tremaining: 3m 59s\n",
      "220:\tlearn: 0.1295924\ttest: 0.1361127\tbest: 0.1361127 (220)\ttotal: 5.36s\tremaining: 3m 57s\n",
      "240:\tlearn: 0.1291241\ttest: 0.1359462\tbest: 0.1359319 (238)\ttotal: 5.83s\tremaining: 3m 56s\n",
      "260:\tlearn: 0.1286455\ttest: 0.1358138\tbest: 0.1358138 (260)\ttotal: 6.3s\tremaining: 3m 55s\n",
      "280:\tlearn: 0.1281426\ttest: 0.1357260\tbest: 0.1357173 (278)\ttotal: 6.77s\tremaining: 3m 54s\n",
      "300:\tlearn: 0.1275621\ttest: 0.1356064\tbest: 0.1356064 (300)\ttotal: 7.25s\tremaining: 3m 53s\n",
      "320:\tlearn: 0.1269569\ttest: 0.1355841\tbest: 0.1355619 (318)\ttotal: 7.75s\tremaining: 3m 53s\n",
      "340:\tlearn: 0.1266102\ttest: 0.1355471\tbest: 0.1355383 (331)\ttotal: 8.22s\tremaining: 3m 52s\n",
      "360:\tlearn: 0.1260898\ttest: 0.1355751\tbest: 0.1354998 (354)\ttotal: 8.71s\tremaining: 3m 52s\n",
      "380:\tlearn: 0.1255937\ttest: 0.1356099\tbest: 0.1354998 (354)\ttotal: 9.18s\tremaining: 3m 51s\n",
      "400:\tlearn: 0.1251023\ttest: 0.1356080\tbest: 0.1354998 (354)\ttotal: 9.65s\tremaining: 3m 51s\n",
      "420:\tlearn: 0.1247266\ttest: 0.1355868\tbest: 0.1354998 (354)\ttotal: 10.1s\tremaining: 3m 50s\n",
      "440:\tlearn: 0.1243322\ttest: 0.1355595\tbest: 0.1354998 (354)\ttotal: 10.6s\tremaining: 3m 49s\n",
      "460:\tlearn: 0.1238687\ttest: 0.1355735\tbest: 0.1354998 (354)\ttotal: 11.1s\tremaining: 3m 49s\n",
      "480:\tlearn: 0.1233780\ttest: 0.1356383\tbest: 0.1354998 (354)\ttotal: 11.6s\tremaining: 3m 48s\n",
      "\n",
      "bestTest = 0.1354998241\n",
      "bestIteration = 354\n",
      "\n",
      "Shrink model to first 355 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6226720\ttest: 0.6227698\tbest: 0.6227698 (0)\ttotal: 21.3ms\tremaining: 3m 33s\n",
      "20:\tlearn: 0.1768462\ttest: 0.1763171\tbest: 0.1763171 (20)\ttotal: 488ms\tremaining: 3m 51s\n",
      "40:\tlearn: 0.1431246\ttest: 0.1413605\tbest: 0.1413605 (40)\ttotal: 990ms\tremaining: 4m\n",
      "60:\tlearn: 0.1368936\ttest: 0.1348351\tbest: 0.1348351 (60)\ttotal: 1.51s\tremaining: 4m 6s\n",
      "80:\tlearn: 0.1348869\ttest: 0.1332315\tbest: 0.1332315 (80)\ttotal: 2.01s\tremaining: 4m 6s\n",
      "100:\tlearn: 0.1341316\ttest: 0.1325978\tbest: 0.1325978 (100)\ttotal: 2.46s\tremaining: 4m 1s\n",
      "120:\tlearn: 0.1334749\ttest: 0.1321423\tbest: 0.1321423 (120)\ttotal: 2.91s\tremaining: 3m 57s\n",
      "140:\tlearn: 0.1330076\ttest: 0.1319352\tbest: 0.1319352 (140)\ttotal: 3.35s\tremaining: 3m 54s\n",
      "160:\tlearn: 0.1322246\ttest: 0.1314658\tbest: 0.1314658 (160)\ttotal: 3.83s\tremaining: 3m 54s\n",
      "180:\tlearn: 0.1315697\ttest: 0.1311168\tbest: 0.1311168 (180)\ttotal: 4.31s\tremaining: 3m 53s\n",
      "200:\tlearn: 0.1310518\ttest: 0.1309954\tbest: 0.1309954 (200)\ttotal: 4.77s\tremaining: 3m 52s\n",
      "220:\tlearn: 0.1304374\ttest: 0.1307432\tbest: 0.1307432 (220)\ttotal: 5.24s\tremaining: 3m 51s\n",
      "240:\tlearn: 0.1299021\ttest: 0.1305799\tbest: 0.1305799 (240)\ttotal: 5.7s\tremaining: 3m 51s\n",
      "260:\tlearn: 0.1293425\ttest: 0.1303765\tbest: 0.1303703 (258)\ttotal: 6.18s\tremaining: 3m 50s\n",
      "280:\tlearn: 0.1288824\ttest: 0.1303153\tbest: 0.1303153 (280)\ttotal: 6.64s\tremaining: 3m 49s\n",
      "300:\tlearn: 0.1283935\ttest: 0.1302213\tbest: 0.1302213 (300)\ttotal: 7.13s\tremaining: 3m 49s\n",
      "320:\tlearn: 0.1278879\ttest: 0.1301611\tbest: 0.1301611 (320)\ttotal: 7.6s\tremaining: 3m 49s\n",
      "340:\tlearn: 0.1275373\ttest: 0.1300599\tbest: 0.1300499 (339)\ttotal: 8.07s\tremaining: 3m 48s\n",
      "360:\tlearn: 0.1271638\ttest: 0.1300443\tbest: 0.1300062 (348)\ttotal: 8.51s\tremaining: 3m 47s\n",
      "380:\tlearn: 0.1267843\ttest: 0.1299902\tbest: 0.1299902 (380)\ttotal: 8.96s\tremaining: 3m 46s\n",
      "400:\tlearn: 0.1263205\ttest: 0.1298869\tbest: 0.1298869 (400)\ttotal: 9.43s\tremaining: 3m 45s\n",
      "420:\tlearn: 0.1258637\ttest: 0.1299710\tbest: 0.1298838 (402)\ttotal: 9.89s\tremaining: 3m 45s\n",
      "440:\tlearn: 0.1254995\ttest: 0.1299464\tbest: 0.1298838 (402)\ttotal: 10.3s\tremaining: 3m 44s\n",
      "460:\tlearn: 0.1250842\ttest: 0.1299953\tbest: 0.1298838 (402)\ttotal: 10.8s\tremaining: 3m 43s\n",
      "480:\tlearn: 0.1246617\ttest: 0.1300163\tbest: 0.1298838 (402)\ttotal: 11.3s\tremaining: 3m 42s\n",
      "500:\tlearn: 0.1243047\ttest: 0.1300179\tbest: 0.1298838 (402)\ttotal: 11.7s\tremaining: 3m 42s\n",
      "520:\tlearn: 0.1240072\ttest: 0.1299968\tbest: 0.1298838 (402)\ttotal: 12.2s\tremaining: 3m 41s\n",
      "540:\tlearn: 0.1237047\ttest: 0.1300181\tbest: 0.1298838 (402)\ttotal: 12.6s\tremaining: 3m 41s\n",
      "\n",
      "bestTest = 0.1298837615\n",
      "bestIteration = 402\n",
      "\n",
      "Shrink model to first 403 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6214540\ttest: 0.6213775\tbest: 0.6213775 (0)\ttotal: 22.7ms\tremaining: 3m 46s\n",
      "20:\tlearn: 0.1766678\ttest: 0.1759785\tbest: 0.1759785 (20)\ttotal: 493ms\tremaining: 3m 54s\n",
      "40:\tlearn: 0.1427521\ttest: 0.1418743\tbest: 0.1418743 (40)\ttotal: 1s\tremaining: 4m 3s\n",
      "60:\tlearn: 0.1363921\ttest: 0.1361608\tbest: 0.1361608 (60)\ttotal: 1.51s\tremaining: 4m 6s\n",
      "80:\tlearn: 0.1344155\ttest: 0.1347089\tbest: 0.1347089 (80)\ttotal: 2.01s\tremaining: 4m 6s\n",
      "100:\tlearn: 0.1337146\ttest: 0.1343444\tbest: 0.1343444 (100)\ttotal: 2.46s\tremaining: 4m 1s\n",
      "120:\tlearn: 0.1330131\ttest: 0.1340280\tbest: 0.1340280 (120)\ttotal: 2.9s\tremaining: 3m 57s\n",
      "140:\tlearn: 0.1323428\ttest: 0.1337183\tbest: 0.1337183 (140)\ttotal: 3.36s\tremaining: 3m 54s\n",
      "160:\tlearn: 0.1315959\ttest: 0.1335433\tbest: 0.1335433 (160)\ttotal: 3.83s\tremaining: 3m 53s\n",
      "180:\tlearn: 0.1309238\ttest: 0.1333094\tbest: 0.1333090 (179)\ttotal: 4.3s\tremaining: 3m 53s\n",
      "200:\tlearn: 0.1305044\ttest: 0.1331324\tbest: 0.1331324 (200)\ttotal: 4.76s\tremaining: 3m 51s\n",
      "220:\tlearn: 0.1300287\ttest: 0.1329712\tbest: 0.1329712 (220)\ttotal: 5.2s\tremaining: 3m 50s\n",
      "240:\tlearn: 0.1294837\ttest: 0.1327931\tbest: 0.1327931 (240)\ttotal: 5.67s\tremaining: 3m 49s\n",
      "260:\tlearn: 0.1287477\ttest: 0.1325035\tbest: 0.1325035 (260)\ttotal: 6.19s\tremaining: 3m 50s\n",
      "280:\tlearn: 0.1282021\ttest: 0.1323704\tbest: 0.1323704 (280)\ttotal: 6.71s\tremaining: 3m 52s\n",
      "300:\tlearn: 0.1276852\ttest: 0.1322099\tbest: 0.1322099 (300)\ttotal: 7.18s\tremaining: 3m 51s\n",
      "320:\tlearn: 0.1272443\ttest: 0.1321995\tbest: 0.1321694 (307)\ttotal: 7.66s\tremaining: 3m 50s\n",
      "340:\tlearn: 0.1268968\ttest: 0.1321932\tbest: 0.1321694 (307)\ttotal: 8.12s\tremaining: 3m 50s\n",
      "360:\tlearn: 0.1263411\ttest: 0.1321762\tbest: 0.1321391 (353)\ttotal: 8.61s\tremaining: 3m 49s\n",
      "380:\tlearn: 0.1259201\ttest: 0.1322344\tbest: 0.1321391 (353)\ttotal: 9.06s\tremaining: 3m 48s\n",
      "400:\tlearn: 0.1255290\ttest: 0.1322384\tbest: 0.1321391 (353)\ttotal: 9.53s\tremaining: 3m 48s\n",
      "420:\tlearn: 0.1251786\ttest: 0.1323345\tbest: 0.1321391 (353)\ttotal: 9.99s\tremaining: 3m 47s\n",
      "440:\tlearn: 0.1248570\ttest: 0.1323177\tbest: 0.1321391 (353)\ttotal: 10.4s\tremaining: 3m 46s\n",
      "460:\tlearn: 0.1244382\ttest: 0.1323273\tbest: 0.1321391 (353)\ttotal: 10.9s\tremaining: 3m 45s\n",
      "\n",
      "bestTest = 0.1321391177\n",
      "bestIteration = 353\n",
      "\n",
      "Shrink model to first 354 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6225329\ttest: 0.6227188\tbest: 0.6227188 (0)\ttotal: 24.8ms\tremaining: 4m 8s\n",
      "20:\tlearn: 0.1793452\ttest: 0.1789789\tbest: 0.1789789 (20)\ttotal: 505ms\tremaining: 4m\n",
      "40:\tlearn: 0.1427985\ttest: 0.1427644\tbest: 0.1427644 (40)\ttotal: 1.02s\tremaining: 4m 7s\n",
      "60:\tlearn: 0.1364328\ttest: 0.1367143\tbest: 0.1367143 (60)\ttotal: 1.56s\tremaining: 4m 14s\n",
      "80:\tlearn: 0.1347745\ttest: 0.1354699\tbest: 0.1354699 (80)\ttotal: 2.07s\tremaining: 4m 13s\n",
      "100:\tlearn: 0.1338773\ttest: 0.1348299\tbest: 0.1348299 (100)\ttotal: 2.53s\tremaining: 4m 8s\n",
      "120:\tlearn: 0.1330543\ttest: 0.1342837\tbest: 0.1342837 (120)\ttotal: 2.99s\tremaining: 4m 4s\n",
      "140:\tlearn: 0.1324468\ttest: 0.1338905\tbest: 0.1338905 (140)\ttotal: 3.45s\tremaining: 4m\n",
      "160:\tlearn: 0.1319095\ttest: 0.1335297\tbest: 0.1335297 (160)\ttotal: 3.91s\tremaining: 3m 59s\n",
      "180:\tlearn: 0.1312457\ttest: 0.1331135\tbest: 0.1331135 (180)\ttotal: 4.4s\tremaining: 3m 58s\n",
      "200:\tlearn: 0.1307112\ttest: 0.1328782\tbest: 0.1328782 (200)\ttotal: 4.86s\tremaining: 3m 57s\n",
      "220:\tlearn: 0.1302975\ttest: 0.1327533\tbest: 0.1327514 (218)\ttotal: 5.31s\tremaining: 3m 55s\n",
      "240:\tlearn: 0.1298544\ttest: 0.1326048\tbest: 0.1326048 (240)\ttotal: 5.8s\tremaining: 3m 54s\n",
      "260:\tlearn: 0.1293514\ttest: 0.1324380\tbest: 0.1324380 (260)\ttotal: 6.26s\tremaining: 3m 53s\n",
      "280:\tlearn: 0.1288515\ttest: 0.1322948\tbest: 0.1322948 (280)\ttotal: 6.73s\tremaining: 3m 52s\n",
      "300:\tlearn: 0.1283155\ttest: 0.1322028\tbest: 0.1321830 (297)\ttotal: 7.2s\tremaining: 3m 51s\n",
      "320:\tlearn: 0.1278595\ttest: 0.1321414\tbest: 0.1321337 (315)\ttotal: 7.66s\tremaining: 3m 50s\n",
      "340:\tlearn: 0.1274681\ttest: 0.1320165\tbest: 0.1320165 (340)\ttotal: 8.13s\tremaining: 3m 50s\n",
      "360:\tlearn: 0.1269690\ttest: 0.1319025\tbest: 0.1319025 (360)\ttotal: 8.59s\tremaining: 3m 49s\n",
      "380:\tlearn: 0.1265357\ttest: 0.1318585\tbest: 0.1318524 (379)\ttotal: 9.06s\tremaining: 3m 48s\n",
      "400:\tlearn: 0.1262011\ttest: 0.1318350\tbest: 0.1318350 (400)\ttotal: 9.53s\tremaining: 3m 48s\n",
      "420:\tlearn: 0.1258101\ttest: 0.1317686\tbest: 0.1317653 (414)\ttotal: 10s\tremaining: 3m 47s\n",
      "440:\tlearn: 0.1254010\ttest: 0.1316973\tbest: 0.1316973 (440)\ttotal: 10.5s\tremaining: 3m 47s\n",
      "460:\tlearn: 0.1250249\ttest: 0.1316675\tbest: 0.1316675 (460)\ttotal: 10.9s\tremaining: 3m 46s\n",
      "480:\tlearn: 0.1247939\ttest: 0.1316492\tbest: 0.1316458 (476)\ttotal: 11.4s\tremaining: 3m 45s\n",
      "500:\tlearn: 0.1244177\ttest: 0.1316201\tbest: 0.1316006 (499)\ttotal: 11.9s\tremaining: 3m 45s\n",
      "520:\tlearn: 0.1240233\ttest: 0.1315656\tbest: 0.1315656 (520)\ttotal: 12.3s\tremaining: 3m 44s\n",
      "540:\tlearn: 0.1235853\ttest: 0.1315081\tbest: 0.1314878 (533)\ttotal: 12.8s\tremaining: 3m 44s\n",
      "560:\tlearn: 0.1231533\ttest: 0.1315449\tbest: 0.1314878 (533)\ttotal: 13.3s\tremaining: 3m 43s\n",
      "580:\tlearn: 0.1227351\ttest: 0.1315880\tbest: 0.1314878 (533)\ttotal: 13.8s\tremaining: 3m 43s\n",
      "600:\tlearn: 0.1223934\ttest: 0.1315854\tbest: 0.1314878 (533)\ttotal: 14.2s\tremaining: 3m 42s\n",
      "620:\tlearn: 0.1220058\ttest: 0.1315306\tbest: 0.1314878 (533)\ttotal: 14.7s\tremaining: 3m 42s\n",
      "640:\tlearn: 0.1216309\ttest: 0.1315013\tbest: 0.1314878 (533)\ttotal: 15.2s\tremaining: 3m 41s\n",
      "660:\tlearn: 0.1213207\ttest: 0.1314171\tbest: 0.1314171 (660)\ttotal: 15.7s\tremaining: 3m 41s\n",
      "680:\tlearn: 0.1208037\ttest: 0.1314092\tbest: 0.1313725 (668)\ttotal: 16.2s\tremaining: 3m 41s\n",
      "700:\tlearn: 0.1205551\ttest: 0.1314067\tbest: 0.1313725 (668)\ttotal: 16.6s\tremaining: 3m 40s\n",
      "720:\tlearn: 0.1202253\ttest: 0.1313589\tbest: 0.1313552 (717)\ttotal: 17.1s\tremaining: 3m 40s\n",
      "740:\tlearn: 0.1199925\ttest: 0.1313984\tbest: 0.1313518 (724)\ttotal: 17.6s\tremaining: 3m 39s\n",
      "760:\tlearn: 0.1195142\ttest: 0.1313409\tbest: 0.1313233 (748)\ttotal: 18.1s\tremaining: 3m 39s\n",
      "780:\tlearn: 0.1191822\ttest: 0.1313201\tbest: 0.1312864 (774)\ttotal: 18.6s\tremaining: 3m 39s\n",
      "800:\tlearn: 0.1189069\ttest: 0.1313072\tbest: 0.1312864 (774)\ttotal: 19.1s\tremaining: 3m 38s\n",
      "820:\tlearn: 0.1185021\ttest: 0.1312873\tbest: 0.1312635 (811)\ttotal: 19.6s\tremaining: 3m 38s\n",
      "840:\tlearn: 0.1181564\ttest: 0.1312959\tbest: 0.1312635 (811)\ttotal: 20.1s\tremaining: 3m 38s\n",
      "860:\tlearn: 0.1177982\ttest: 0.1312715\tbest: 0.1312343 (856)\ttotal: 20.5s\tremaining: 3m 38s\n",
      "880:\tlearn: 0.1174090\ttest: 0.1312397\tbest: 0.1312343 (856)\ttotal: 21s\tremaining: 3m 37s\n",
      "900:\tlearn: 0.1169550\ttest: 0.1313256\tbest: 0.1312042 (887)\ttotal: 21.5s\tremaining: 3m 37s\n",
      "920:\tlearn: 0.1167311\ttest: 0.1313621\tbest: 0.1312042 (887)\ttotal: 22s\tremaining: 3m 36s\n",
      "940:\tlearn: 0.1164045\ttest: 0.1313405\tbest: 0.1312042 (887)\ttotal: 22.5s\tremaining: 3m 36s\n",
      "960:\tlearn: 0.1160380\ttest: 0.1313478\tbest: 0.1312042 (887)\ttotal: 23s\tremaining: 3m 36s\n",
      "980:\tlearn: 0.1157410\ttest: 0.1312867\tbest: 0.1312042 (887)\ttotal: 23.5s\tremaining: 3m 35s\n",
      "1000:\tlearn: 0.1154315\ttest: 0.1313684\tbest: 0.1312042 (887)\ttotal: 24s\tremaining: 3m 35s\n",
      "1020:\tlearn: 0.1151482\ttest: 0.1313896\tbest: 0.1312042 (887)\ttotal: 24.5s\tremaining: 3m 35s\n",
      "1040:\tlearn: 0.1147635\ttest: 0.1313962\tbest: 0.1312042 (887)\ttotal: 25s\tremaining: 3m 34s\n",
      "1060:\tlearn: 0.1144121\ttest: 0.1314251\tbest: 0.1312042 (887)\ttotal: 25.5s\tremaining: 3m 34s\n",
      "1080:\tlearn: 0.1141200\ttest: 0.1313721\tbest: 0.1312042 (887)\ttotal: 25.9s\tremaining: 3m 34s\n",
      "1100:\tlearn: 0.1137505\ttest: 0.1314470\tbest: 0.1312042 (887)\ttotal: 26.4s\tremaining: 3m 33s\n",
      "1120:\tlearn: 0.1134408\ttest: 0.1314651\tbest: 0.1312042 (887)\ttotal: 26.9s\tremaining: 3m 33s\n",
      "1140:\tlearn: 0.1132017\ttest: 0.1314549\tbest: 0.1312042 (887)\ttotal: 27.4s\tremaining: 3m 32s\n",
      "1160:\tlearn: 0.1128832\ttest: 0.1315068\tbest: 0.1312042 (887)\ttotal: 27.9s\tremaining: 3m 32s\n",
      "\n",
      "bestTest = 0.1312042307\n",
      "bestIteration = 887\n",
      "\n",
      "Shrink model to first 888 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6227894\ttest: 0.6228906\tbest: 0.6228906 (0)\ttotal: 21.4ms\tremaining: 3m 33s\n",
      "20:\tlearn: 0.1764965\ttest: 0.1779208\tbest: 0.1779208 (20)\ttotal: 486ms\tremaining: 3m 50s\n",
      "40:\tlearn: 0.1411567\ttest: 0.1452234\tbest: 0.1452234 (40)\ttotal: 1s\tremaining: 4m 3s\n",
      "60:\tlearn: 0.1352847\ttest: 0.1413270\tbest: 0.1413270 (60)\ttotal: 1.54s\tremaining: 4m 10s\n",
      "80:\tlearn: 0.1333612\ttest: 0.1406006\tbest: 0.1406006 (80)\ttotal: 2.04s\tremaining: 4m 9s\n",
      "100:\tlearn: 0.1324622\ttest: 0.1402963\tbest: 0.1402963 (100)\ttotal: 2.5s\tremaining: 4m 5s\n",
      "120:\tlearn: 0.1318047\ttest: 0.1400007\tbest: 0.1400007 (120)\ttotal: 2.95s\tremaining: 4m\n",
      "140:\tlearn: 0.1312510\ttest: 0.1398114\tbest: 0.1398114 (140)\ttotal: 3.41s\tremaining: 3m 58s\n",
      "160:\tlearn: 0.1305717\ttest: 0.1395727\tbest: 0.1395727 (160)\ttotal: 3.9s\tremaining: 3m 58s\n",
      "180:\tlearn: 0.1300299\ttest: 0.1395147\tbest: 0.1395057 (179)\ttotal: 4.38s\tremaining: 3m 57s\n",
      "200:\tlearn: 0.1295579\ttest: 0.1394233\tbest: 0.1394015 (197)\ttotal: 4.87s\tremaining: 3m 57s\n",
      "220:\tlearn: 0.1290839\ttest: 0.1393858\tbest: 0.1393811 (218)\ttotal: 5.35s\tremaining: 3m 56s\n",
      "240:\tlearn: 0.1285295\ttest: 0.1394040\tbest: 0.1393811 (218)\ttotal: 5.84s\tremaining: 3m 56s\n",
      "260:\tlearn: 0.1279274\ttest: 0.1392630\tbest: 0.1392630 (260)\ttotal: 6.32s\tremaining: 3m 55s\n",
      "280:\tlearn: 0.1274580\ttest: 0.1392639\tbest: 0.1392381 (275)\ttotal: 6.78s\tremaining: 3m 54s\n",
      "300:\tlearn: 0.1268957\ttest: 0.1392703\tbest: 0.1392381 (275)\ttotal: 7.27s\tremaining: 3m 54s\n",
      "320:\tlearn: 0.1263887\ttest: 0.1393076\tbest: 0.1392381 (275)\ttotal: 7.75s\tremaining: 3m 53s\n",
      "340:\tlearn: 0.1260110\ttest: 0.1392194\tbest: 0.1392004 (339)\ttotal: 8.21s\tremaining: 3m 52s\n",
      "360:\tlearn: 0.1256788\ttest: 0.1392440\tbest: 0.1392004 (339)\ttotal: 8.66s\tremaining: 3m 51s\n",
      "380:\tlearn: 0.1253442\ttest: 0.1392823\tbest: 0.1392004 (339)\ttotal: 9.12s\tremaining: 3m 50s\n",
      "400:\tlearn: 0.1250125\ttest: 0.1392684\tbest: 0.1392004 (339)\ttotal: 9.6s\tremaining: 3m 49s\n",
      "420:\tlearn: 0.1246517\ttest: 0.1392939\tbest: 0.1392004 (339)\ttotal: 10.1s\tremaining: 3m 48s\n",
      "440:\tlearn: 0.1243379\ttest: 0.1393068\tbest: 0.1392004 (339)\ttotal: 10.5s\tremaining: 3m 47s\n",
      "460:\tlearn: 0.1239552\ttest: 0.1394009\tbest: 0.1392004 (339)\ttotal: 11s\tremaining: 3m 47s\n",
      "\n",
      "bestTest = 0.1392004345\n",
      "bestIteration = 339\n",
      "\n",
      "Shrink model to first 340 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6226296\ttest: 0.6226908\tbest: 0.6226908 (0)\ttotal: 22.5ms\tremaining: 3m 44s\n",
      "20:\tlearn: 0.1785448\ttest: 0.1789719\tbest: 0.1789719 (20)\ttotal: 490ms\tremaining: 3m 52s\n",
      "40:\tlearn: 0.1417985\ttest: 0.1436753\tbest: 0.1436753 (40)\ttotal: 1.02s\tremaining: 4m 7s\n",
      "60:\tlearn: 0.1359621\ttest: 0.1389430\tbest: 0.1389430 (60)\ttotal: 1.55s\tremaining: 4m 13s\n",
      "80:\tlearn: 0.1337979\ttest: 0.1374929\tbest: 0.1374929 (80)\ttotal: 2.08s\tremaining: 4m 15s\n",
      "100:\tlearn: 0.1330692\ttest: 0.1370894\tbest: 0.1370888 (98)\ttotal: 2.55s\tremaining: 4m 9s\n",
      "120:\tlearn: 0.1324289\ttest: 0.1368015\tbest: 0.1368015 (120)\ttotal: 3s\tremaining: 4m 5s\n",
      "140:\tlearn: 0.1317205\ttest: 0.1364959\tbest: 0.1364959 (140)\ttotal: 3.49s\tremaining: 4m 4s\n",
      "160:\tlearn: 0.1310085\ttest: 0.1363129\tbest: 0.1363129 (160)\ttotal: 3.98s\tremaining: 4m 3s\n",
      "180:\tlearn: 0.1305012\ttest: 0.1362043\tbest: 0.1362043 (180)\ttotal: 4.45s\tremaining: 4m 1s\n",
      "200:\tlearn: 0.1299324\ttest: 0.1360739\tbest: 0.1360708 (196)\ttotal: 4.92s\tremaining: 4m\n",
      "220:\tlearn: 0.1293370\ttest: 0.1358681\tbest: 0.1358681 (220)\ttotal: 5.39s\tremaining: 3m 58s\n",
      "240:\tlearn: 0.1288603\ttest: 0.1357205\tbest: 0.1357205 (240)\ttotal: 5.87s\tremaining: 3m 57s\n",
      "260:\tlearn: 0.1284334\ttest: 0.1356412\tbest: 0.1356249 (255)\ttotal: 6.34s\tremaining: 3m 56s\n",
      "280:\tlearn: 0.1277900\ttest: 0.1355484\tbest: 0.1355467 (278)\ttotal: 6.84s\tremaining: 3m 56s\n",
      "300:\tlearn: 0.1273311\ttest: 0.1355519\tbest: 0.1355286 (290)\ttotal: 7.3s\tremaining: 3m 55s\n",
      "320:\tlearn: 0.1268202\ttest: 0.1355415\tbest: 0.1355145 (315)\ttotal: 7.78s\tremaining: 3m 54s\n",
      "340:\tlearn: 0.1262977\ttest: 0.1356260\tbest: 0.1355145 (315)\ttotal: 8.27s\tremaining: 3m 54s\n",
      "360:\tlearn: 0.1259842\ttest: 0.1356405\tbest: 0.1355145 (315)\ttotal: 8.71s\tremaining: 3m 52s\n",
      "380:\tlearn: 0.1256301\ttest: 0.1356602\tbest: 0.1355145 (315)\ttotal: 9.18s\tremaining: 3m 51s\n",
      "400:\tlearn: 0.1251775\ttest: 0.1356484\tbest: 0.1355145 (315)\ttotal: 9.66s\tremaining: 3m 51s\n",
      "420:\tlearn: 0.1248808\ttest: 0.1355988\tbest: 0.1355145 (315)\ttotal: 10.1s\tremaining: 3m 50s\n",
      "\n",
      "bestTest = 0.1355145168\n",
      "bestIteration = 315\n",
      "\n",
      "Shrink model to first 316 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6206829\ttest: 0.6207938\tbest: 0.6207938 (0)\ttotal: 23.2ms\tremaining: 3m 51s\n",
      "20:\tlearn: 0.1776322\ttest: 0.1773549\tbest: 0.1773549 (20)\ttotal: 507ms\tremaining: 4m\n",
      "40:\tlearn: 0.1425160\ttest: 0.1421329\tbest: 0.1421329 (40)\ttotal: 1.04s\tremaining: 4m 12s\n",
      "60:\tlearn: 0.1362459\ttest: 0.1361471\tbest: 0.1361471 (60)\ttotal: 1.6s\tremaining: 4m 21s\n",
      "80:\tlearn: 0.1343819\ttest: 0.1347965\tbest: 0.1347853 (78)\ttotal: 2.09s\tremaining: 4m 16s\n",
      "100:\tlearn: 0.1334831\ttest: 0.1343443\tbest: 0.1343433 (99)\ttotal: 2.57s\tremaining: 4m 12s\n",
      "120:\tlearn: 0.1327736\ttest: 0.1341029\tbest: 0.1340976 (118)\ttotal: 3.06s\tremaining: 4m 10s\n",
      "140:\tlearn: 0.1320967\ttest: 0.1338147\tbest: 0.1338147 (140)\ttotal: 3.52s\tremaining: 4m 5s\n",
      "160:\tlearn: 0.1315248\ttest: 0.1336411\tbest: 0.1336411 (160)\ttotal: 3.99s\tremaining: 4m 3s\n",
      "180:\tlearn: 0.1310171\ttest: 0.1334578\tbest: 0.1334578 (180)\ttotal: 4.47s\tremaining: 4m 2s\n",
      "200:\tlearn: 0.1303845\ttest: 0.1332357\tbest: 0.1332357 (200)\ttotal: 4.95s\tremaining: 4m 1s\n",
      "220:\tlearn: 0.1298522\ttest: 0.1330943\tbest: 0.1330677 (217)\ttotal: 5.41s\tremaining: 3m 59s\n",
      "240:\tlearn: 0.1292068\ttest: 0.1329367\tbest: 0.1329240 (234)\ttotal: 5.9s\tremaining: 3m 58s\n",
      "260:\tlearn: 0.1287502\ttest: 0.1328453\tbest: 0.1328453 (260)\ttotal: 6.37s\tremaining: 3m 57s\n",
      "280:\tlearn: 0.1282710\ttest: 0.1327293\tbest: 0.1327293 (280)\ttotal: 6.85s\tremaining: 3m 57s\n",
      "300:\tlearn: 0.1277888\ttest: 0.1325913\tbest: 0.1325786 (297)\ttotal: 7.33s\tremaining: 3m 56s\n",
      "320:\tlearn: 0.1273596\ttest: 0.1324651\tbest: 0.1324651 (320)\ttotal: 7.79s\tremaining: 3m 54s\n",
      "340:\tlearn: 0.1269828\ttest: 0.1324308\tbest: 0.1324307 (339)\ttotal: 8.26s\tremaining: 3m 53s\n",
      "360:\tlearn: 0.1264899\ttest: 0.1323746\tbest: 0.1323498 (355)\ttotal: 8.73s\tremaining: 3m 53s\n",
      "380:\tlearn: 0.1259406\ttest: 0.1323807\tbest: 0.1323498 (355)\ttotal: 9.21s\tremaining: 3m 52s\n",
      "400:\tlearn: 0.1254601\ttest: 0.1323477\tbest: 0.1323477 (400)\ttotal: 9.69s\tremaining: 3m 51s\n",
      "420:\tlearn: 0.1251207\ttest: 0.1323426\tbest: 0.1323339 (415)\ttotal: 10.2s\tremaining: 3m 51s\n",
      "440:\tlearn: 0.1247601\ttest: 0.1323214\tbest: 0.1323195 (439)\ttotal: 10.6s\tremaining: 3m 50s\n",
      "460:\tlearn: 0.1243446\ttest: 0.1323199\tbest: 0.1322616 (448)\ttotal: 11.1s\tremaining: 3m 50s\n",
      "480:\tlearn: 0.1240410\ttest: 0.1322679\tbest: 0.1322347 (471)\ttotal: 11.6s\tremaining: 3m 49s\n",
      "500:\tlearn: 0.1236518\ttest: 0.1322271\tbest: 0.1322271 (500)\ttotal: 12.1s\tremaining: 3m 48s\n",
      "520:\tlearn: 0.1231781\ttest: 0.1321626\tbest: 0.1321623 (519)\ttotal: 12.6s\tremaining: 3m 48s\n",
      "540:\tlearn: 0.1228899\ttest: 0.1321675\tbest: 0.1321563 (523)\ttotal: 13.1s\tremaining: 3m 48s\n",
      "560:\tlearn: 0.1224036\ttest: 0.1321248\tbest: 0.1321229 (559)\ttotal: 13.5s\tremaining: 3m 47s\n",
      "580:\tlearn: 0.1219726\ttest: 0.1320810\tbest: 0.1320799 (579)\ttotal: 14s\tremaining: 3m 47s\n",
      "600:\tlearn: 0.1216724\ttest: 0.1321241\tbest: 0.1320799 (579)\ttotal: 14.5s\tremaining: 3m 46s\n",
      "620:\tlearn: 0.1212688\ttest: 0.1320581\tbest: 0.1320447 (618)\ttotal: 15s\tremaining: 3m 46s\n",
      "640:\tlearn: 0.1208300\ttest: 0.1320423\tbest: 0.1320395 (635)\ttotal: 15.5s\tremaining: 3m 46s\n",
      "660:\tlearn: 0.1205383\ttest: 0.1319680\tbest: 0.1319469 (648)\ttotal: 16s\tremaining: 3m 45s\n",
      "680:\tlearn: 0.1202890\ttest: 0.1320013\tbest: 0.1319469 (648)\ttotal: 16.5s\tremaining: 3m 45s\n",
      "700:\tlearn: 0.1199749\ttest: 0.1320481\tbest: 0.1319469 (648)\ttotal: 16.9s\tremaining: 3m 44s\n",
      "720:\tlearn: 0.1195562\ttest: 0.1321122\tbest: 0.1319469 (648)\ttotal: 17.4s\tremaining: 3m 44s\n",
      "740:\tlearn: 0.1192047\ttest: 0.1322267\tbest: 0.1319469 (648)\ttotal: 17.9s\tremaining: 3m 43s\n",
      "760:\tlearn: 0.1187968\ttest: 0.1321697\tbest: 0.1319469 (648)\ttotal: 18.4s\tremaining: 3m 43s\n",
      "780:\tlearn: 0.1184428\ttest: 0.1321667\tbest: 0.1319469 (648)\ttotal: 18.9s\tremaining: 3m 42s\n",
      "800:\tlearn: 0.1181542\ttest: 0.1322138\tbest: 0.1319469 (648)\ttotal: 19.3s\tremaining: 3m 42s\n",
      "820:\tlearn: 0.1177160\ttest: 0.1321992\tbest: 0.1319469 (648)\ttotal: 19.8s\tremaining: 3m 41s\n",
      "840:\tlearn: 0.1174314\ttest: 0.1321691\tbest: 0.1319469 (648)\ttotal: 20.3s\tremaining: 3m 41s\n",
      "860:\tlearn: 0.1170701\ttest: 0.1322276\tbest: 0.1319469 (648)\ttotal: 20.9s\tremaining: 3m 41s\n",
      "\n",
      "bestTest = 0.13194687\n",
      "bestIteration = 648\n",
      "\n",
      "Shrink model to first 649 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6170647\ttest: 0.6170406\tbest: 0.6170406 (0)\ttotal: 24.6ms\tremaining: 4m 5s\n",
      "20:\tlearn: 0.1776038\ttest: 0.1778559\tbest: 0.1778559 (20)\ttotal: 502ms\tremaining: 3m 58s\n",
      "40:\tlearn: 0.1421473\ttest: 0.1430229\tbest: 0.1430229 (40)\ttotal: 1.02s\tremaining: 4m 8s\n",
      "60:\tlearn: 0.1360528\ttest: 0.1378575\tbest: 0.1378575 (60)\ttotal: 1.55s\tremaining: 4m 13s\n",
      "80:\tlearn: 0.1341005\ttest: 0.1366621\tbest: 0.1366621 (80)\ttotal: 2.08s\tremaining: 4m 14s\n",
      "100:\tlearn: 0.1331085\ttest: 0.1362308\tbest: 0.1362308 (100)\ttotal: 2.58s\tremaining: 4m 12s\n",
      "120:\tlearn: 0.1323583\ttest: 0.1359424\tbest: 0.1359375 (119)\ttotal: 3.08s\tremaining: 4m 11s\n",
      "140:\tlearn: 0.1317295\ttest: 0.1356403\tbest: 0.1356403 (140)\ttotal: 3.56s\tremaining: 4m 8s\n",
      "160:\tlearn: 0.1309467\ttest: 0.1354273\tbest: 0.1354273 (160)\ttotal: 4.09s\tremaining: 4m 9s\n",
      "180:\tlearn: 0.1303964\ttest: 0.1353324\tbest: 0.1353324 (180)\ttotal: 4.58s\tremaining: 4m 8s\n",
      "200:\tlearn: 0.1298736\ttest: 0.1352338\tbest: 0.1352338 (200)\ttotal: 5.05s\tremaining: 4m 6s\n",
      "220:\tlearn: 0.1294526\ttest: 0.1351684\tbest: 0.1351684 (220)\ttotal: 5.54s\tremaining: 4m 5s\n",
      "240:\tlearn: 0.1290233\ttest: 0.1351107\tbest: 0.1351030 (238)\ttotal: 6.03s\tremaining: 4m 3s\n",
      "260:\tlearn: 0.1285408\ttest: 0.1350107\tbest: 0.1350107 (260)\ttotal: 6.51s\tremaining: 4m 2s\n",
      "280:\tlearn: 0.1279612\ttest: 0.1349446\tbest: 0.1349381 (275)\ttotal: 7s\tremaining: 4m 2s\n",
      "300:\tlearn: 0.1275440\ttest: 0.1349373\tbest: 0.1348952 (282)\ttotal: 7.49s\tremaining: 4m 1s\n",
      "320:\tlearn: 0.1270373\ttest: 0.1349288\tbest: 0.1348952 (282)\ttotal: 8s\tremaining: 4m 1s\n",
      "340:\tlearn: 0.1265112\ttest: 0.1350172\tbest: 0.1348952 (282)\ttotal: 8.5s\tremaining: 4m\n",
      "360:\tlearn: 0.1260334\ttest: 0.1351402\tbest: 0.1348952 (282)\ttotal: 8.97s\tremaining: 3m 59s\n",
      "380:\tlearn: 0.1255967\ttest: 0.1351247\tbest: 0.1348952 (282)\ttotal: 9.45s\tremaining: 3m 58s\n",
      "\n",
      "bestTest = 0.1348951561\n",
      "bestIteration = 282\n",
      "\n",
      "Shrink model to first 283 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "\t0.841\t = Validation score   (roc_auc)\n",
      "\t369.87s\t = Training   runtime\n",
      "\t0.99s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\model.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2863.74s of the 2863.73s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit NNFastAiTabularModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S3F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 102.30 / 282.44 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.1742016077041626.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14758487045764923.\n",
      "Better model found at epoch 2 with valid_loss value: 0.14383669197559357.\n",
      "Better model found at epoch 6 with valid_loss value: 0.14065921306610107.\n",
      "Better model found at epoch 11 with valid_loss value: 0.14044152200222015.\n",
      "Model validation metrics: 0.14044152200222015\n",
      "\tFitting S3F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 102.21 / 312.38 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.5046841502189636.\n",
      "Better model found at epoch 2 with valid_loss value: 0.151032492518425.\n",
      "Better model found at epoch 3 with valid_loss value: 0.13799633085727692.\n",
      "Better model found at epoch 8 with valid_loss value: 0.13733458518981934.\n",
      "No improvement since epoch 8: early stopping\n",
      "Model validation metrics: 0.13733458518981934\n",
      "\tFitting S3F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 332 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=332, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 111.37 / 352.62 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.6459843516349792.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14619886875152588.\n",
      "Better model found at epoch 2 with valid_loss value: 0.1398245394229889.\n",
      "Better model found at epoch 9 with valid_loss value: 0.13558214902877808.\n",
      "No improvement since epoch 9: early stopping\n",
      "Model validation metrics: 0.13558214902877808\n",
      "\tFitting S3F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 102.34 / 408.55 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.16448788344860077.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14275670051574707.\n",
      "Better model found at epoch 2 with valid_loss value: 0.13735996186733246.\n",
      "Better model found at epoch 8 with valid_loss value: 0.13577836751937866.\n",
      "Better model found at epoch 9 with valid_loss value: 0.1333915740251541.\n",
      "No improvement since epoch 9: early stopping\n",
      "Model validation metrics: 0.1333915740251541\n",
      "\tFitting S3F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 333 cont features\n",
      "Automated batch size selection: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(333, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=333, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 102.21 / 492.24 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.18075549602508545.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14584779739379883.\n",
      "Better model found at epoch 6 with valid_loss value: 0.14421457052230835.\n",
      "Better model found at epoch 13 with valid_loss value: 0.14149226248264313.\n",
      "Model validation metrics: 0.14149226248264313\n",
      "\tFitting S3F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 105.63 / 631.58 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.26001113653182983.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14701449871063232.\n",
      "Better model found at epoch 2 with valid_loss value: 0.14174427092075348.\n",
      "Better model found at epoch 6 with valid_loss value: 0.13961507380008698.\n",
      "Better model found at epoch 7 with valid_loss value: 0.13879622519016266.\n",
      "Better model found at epoch 12 with valid_loss value: 0.13835090398788452.\n",
      "Model validation metrics: 0.13835090398788452\n",
      "\tFitting S3F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 105.39 / 911.65 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.16956427693367004.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14426898956298828.\n",
      "Better model found at epoch 2 with valid_loss value: 0.14303608238697052.\n",
      "Better model found at epoch 3 with valid_loss value: 0.1388164460659027.\n",
      "Better model found at epoch 5 with valid_loss value: 0.13727274537086487.\n",
      "Better model found at epoch 9 with valid_loss value: 0.13690364360809326.\n",
      "No improvement since epoch 9: early stopping\n",
      "Model validation metrics: 0.13690364360809326\n",
      "\tFitting S3F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 332 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=332, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 105.67 / 1750.09 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.18671225011348724.\n",
      "Better model found at epoch 1 with valid_loss value: 0.1590549200773239.\n",
      "Better model found at epoch 2 with valid_loss value: 0.1471160501241684.\n",
      "Better model found at epoch 4 with valid_loss value: 0.14217574894428253.\n",
      "No improvement since epoch 4: early stopping\n",
      "Model validation metrics: 0.14217574894428253\n",
      "Saving AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\model.pkl\n",
      "\t0.8311\t = Validation score   (roc_auc)\n",
      "\t2203.46s\t = Training   runtime\n",
      "\t14.17s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 2110.71s of the 2110.69s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit XGBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S3F1 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61156\n",
      "[50]\tvalidation_0-logloss:0.13609\n",
      "[100]\tvalidation_0-logloss:0.13566\n",
      "[117]\tvalidation_0-logloss:0.13568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F2 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61159\n",
      "[50]\tvalidation_0-logloss:0.13150\n",
      "[100]\tvalidation_0-logloss:0.13056\n",
      "[150]\tvalidation_0-logloss:0.13074\n",
      "[157]\tvalidation_0-logloss:0.13067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F3 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61146\n",
      "[50]\tvalidation_0-logloss:0.13286\n",
      "[100]\tvalidation_0-logloss:0.13243\n",
      "[119]\tvalidation_0-logloss:0.13268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F4 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61125\n",
      "[50]\tvalidation_0-logloss:0.13194\n",
      "[100]\tvalidation_0-logloss:0.13123\n",
      "[150]\tvalidation_0-logloss:0.13155\n",
      "[170]\tvalidation_0-logloss:0.13159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F5 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61153\n",
      "[50]\tvalidation_0-logloss:0.13886\n",
      "[96]\tvalidation_0-logloss:0.13931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F6 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61159\n",
      "[50]\tvalidation_0-logloss:0.13685\n",
      "[100]\tvalidation_0-logloss:0.13701\n",
      "[105]\tvalidation_0-logloss:0.13712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F7 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61124\n",
      "[50]\tvalidation_0-logloss:0.13256\n",
      "[100]\tvalidation_0-logloss:0.13163\n",
      "[120]\tvalidation_0-logloss:0.13154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F8 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61138\n",
      "[50]\tvalidation_0-logloss:0.13543\n",
      "[100]\tvalidation_0-logloss:0.13552\n",
      "[108]\tvalidation_0-logloss:0.13571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\XGBoost_BAG_L1\\model.pkl\n",
      "\t0.8409\t = Validation score   (roc_auc)\n",
      "\t68.86s\t = Training   runtime\n",
      "\t2.34s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2085.99s of the 2085.98s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S3F1 - S3F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S3F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.139463\n",
      "[100]\tvalid_set's binary_logloss: 0.138074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.134059\n",
      "[100]\tvalid_set's binary_logloss: 0.131098\n",
      "[150]\tvalid_set's binary_logloss: 0.131014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135611\n",
      "[100]\tvalid_set's binary_logloss: 0.132931\n",
      "[150]\tvalid_set's binary_logloss: 0.133116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135104\n",
      "[100]\tvalid_set's binary_logloss: 0.132419\n",
      "[150]\tvalid_set's binary_logloss: 0.132538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.140406\n",
      "[100]\tvalid_set's binary_logloss: 0.139631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.139587\n",
      "[100]\tvalid_set's binary_logloss: 0.137794\n",
      "[150]\tvalid_set's binary_logloss: 0.138456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135558\n",
      "[100]\tvalid_set's binary_logloss: 0.133032\n",
      "[150]\tvalid_set's binary_logloss: 0.133068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S3F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.13756\n",
      "[100]\tvalid_set's binary_logloss: 0.136125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t0.8383\t = Validation score   (roc_auc)\n",
      "\t74.27s\t = Training   runtime\n",
      "\t1.64s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Repeating k-fold bagging: 4/20\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2060.07s of the 2060.05s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S4F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.139272\n",
      "[100]\tvalid_set's binary_logloss: 0.137104\n",
      "[150]\tvalid_set's binary_logloss: 0.136701\n",
      "[200]\tvalid_set's binary_logloss: 0.136633\n",
      "[250]\tvalid_set's binary_logloss: 0.136769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138672\n",
      "[100]\tvalid_set's binary_logloss: 0.13553\n",
      "[150]\tvalid_set's binary_logloss: 0.134858\n",
      "[200]\tvalid_set's binary_logloss: 0.13469\n",
      "[250]\tvalid_set's binary_logloss: 0.13465\n",
      "[300]\tvalid_set's binary_logloss: 0.134445\n",
      "[350]\tvalid_set's binary_logloss: 0.134408\n",
      "[400]\tvalid_set's binary_logloss: 0.134657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135024\n",
      "[100]\tvalid_set's binary_logloss: 0.13137\n",
      "[150]\tvalid_set's binary_logloss: 0.130399\n",
      "[200]\tvalid_set's binary_logloss: 0.129876\n",
      "[250]\tvalid_set's binary_logloss: 0.129553\n",
      "[300]\tvalid_set's binary_logloss: 0.129419\n",
      "[350]\tvalid_set's binary_logloss: 0.129439\n",
      "[400]\tvalid_set's binary_logloss: 0.129539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.137578\n",
      "[100]\tvalid_set's binary_logloss: 0.135781\n",
      "[150]\tvalid_set's binary_logloss: 0.13561\n",
      "[200]\tvalid_set's binary_logloss: 0.135378\n",
      "[250]\tvalid_set's binary_logloss: 0.135826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.13514\n",
      "[100]\tvalid_set's binary_logloss: 0.131914\n",
      "[150]\tvalid_set's binary_logloss: 0.130981\n",
      "[200]\tvalid_set's binary_logloss: 0.131214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.139651\n",
      "[100]\tvalid_set's binary_logloss: 0.136965\n",
      "[150]\tvalid_set's binary_logloss: 0.136607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.139442\n",
      "[100]\tvalid_set's binary_logloss: 0.137151\n",
      "[150]\tvalid_set's binary_logloss: 0.136272\n",
      "[200]\tvalid_set's binary_logloss: 0.136498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138431\n",
      "[100]\tvalid_set's binary_logloss: 0.137174\n",
      "[150]\tvalid_set's binary_logloss: 0.136249\n",
      "[200]\tvalid_set's binary_logloss: 0.136297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t0.8404\t = Validation score   (roc_auc)\n",
      "\t76.23s\t = Training   runtime\n",
      "\t2.6s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2039.13s of the 2039.11s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S4F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.136157\n",
      "[100]\tvalid_set's binary_logloss: 0.135491\n",
      "[150]\tvalid_set's binary_logloss: 0.135817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135973\n",
      "[100]\tvalid_set's binary_logloss: 0.135025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.131488\n",
      "[100]\tvalid_set's binary_logloss: 0.129869\n",
      "[150]\tvalid_set's binary_logloss: 0.129782\n",
      "[200]\tvalid_set's binary_logloss: 0.129675\n",
      "[250]\tvalid_set's binary_logloss: 0.129794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.136142\n",
      "[100]\tvalid_set's binary_logloss: 0.135659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.132061\n",
      "[100]\tvalid_set's binary_logloss: 0.131056\n",
      "[150]\tvalid_set's binary_logloss: 0.131136\n",
      "[200]\tvalid_set's binary_logloss: 0.131629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.13761\n",
      "[100]\tvalid_set's binary_logloss: 0.136799\n",
      "[150]\tvalid_set's binary_logloss: 0.13651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135365\n",
      "[100]\tvalid_set's binary_logloss: 0.1348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.135466\n",
      "[100]\tvalid_set's binary_logloss: 0.135234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t0.8412\t = Validation score   (roc_auc)\n",
      "\t60.11s\t = Training   runtime\n",
      "\t2.0s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2022.0s of the 2021.98s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S4F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6223786\ttest: 0.6224969\tbest: 0.6224969 (0)\ttotal: 23.7ms\tremaining: 3m 56s\n",
      "20:\tlearn: 0.1778015\ttest: 0.1782374\tbest: 0.1782374 (20)\ttotal: 515ms\tremaining: 4m 4s\n",
      "40:\tlearn: 0.1419206\ttest: 0.1432268\tbest: 0.1432268 (40)\ttotal: 1.07s\tremaining: 4m 20s\n",
      "60:\tlearn: 0.1360185\ttest: 0.1386122\tbest: 0.1386122 (60)\ttotal: 1.6s\tremaining: 4m 20s\n",
      "80:\tlearn: 0.1339866\ttest: 0.1375095\tbest: 0.1375095 (80)\ttotal: 2.12s\tremaining: 4m 19s\n",
      "100:\tlearn: 0.1331524\ttest: 0.1372440\tbest: 0.1372440 (100)\ttotal: 2.59s\tremaining: 4m 14s\n",
      "120:\tlearn: 0.1323538\ttest: 0.1369177\tbest: 0.1369177 (120)\ttotal: 3.06s\tremaining: 4m 10s\n",
      "140:\tlearn: 0.1317938\ttest: 0.1367596\tbest: 0.1367596 (140)\ttotal: 3.52s\tremaining: 4m 6s\n",
      "160:\tlearn: 0.1311585\ttest: 0.1365566\tbest: 0.1365566 (160)\ttotal: 4.01s\tremaining: 4m 5s\n",
      "180:\tlearn: 0.1306271\ttest: 0.1363378\tbest: 0.1363101 (178)\ttotal: 4.49s\tremaining: 4m 3s\n",
      "200:\tlearn: 0.1301755\ttest: 0.1363291\tbest: 0.1363086 (189)\ttotal: 4.95s\tremaining: 4m 1s\n",
      "220:\tlearn: 0.1297207\ttest: 0.1362092\tbest: 0.1361998 (216)\ttotal: 5.41s\tremaining: 3m 59s\n",
      "240:\tlearn: 0.1292868\ttest: 0.1361440\tbest: 0.1361440 (240)\ttotal: 5.87s\tremaining: 3m 57s\n",
      "260:\tlearn: 0.1286473\ttest: 0.1359434\tbest: 0.1359413 (259)\ttotal: 6.36s\tremaining: 3m 57s\n",
      "280:\tlearn: 0.1281790\ttest: 0.1358770\tbest: 0.1358541 (275)\ttotal: 6.82s\tremaining: 3m 55s\n",
      "300:\tlearn: 0.1277813\ttest: 0.1357932\tbest: 0.1357932 (299)\ttotal: 7.26s\tremaining: 3m 53s\n",
      "320:\tlearn: 0.1274182\ttest: 0.1358049\tbest: 0.1357932 (299)\ttotal: 7.7s\tremaining: 3m 52s\n",
      "340:\tlearn: 0.1268968\ttest: 0.1357756\tbest: 0.1357654 (334)\ttotal: 8.18s\tremaining: 3m 51s\n",
      "360:\tlearn: 0.1264286\ttest: 0.1356108\tbest: 0.1356108 (360)\ttotal: 8.66s\tremaining: 3m 51s\n",
      "380:\tlearn: 0.1259278\ttest: 0.1355410\tbest: 0.1355410 (380)\ttotal: 9.14s\tremaining: 3m 50s\n",
      "400:\tlearn: 0.1255149\ttest: 0.1356153\tbest: 0.1355316 (381)\ttotal: 9.59s\tremaining: 3m 49s\n",
      "420:\tlearn: 0.1251208\ttest: 0.1356226\tbest: 0.1355316 (381)\ttotal: 10.1s\tremaining: 3m 48s\n",
      "440:\tlearn: 0.1248091\ttest: 0.1356222\tbest: 0.1355316 (381)\ttotal: 10.5s\tremaining: 3m 48s\n",
      "460:\tlearn: 0.1244534\ttest: 0.1355340\tbest: 0.1355316 (381)\ttotal: 11s\tremaining: 3m 47s\n",
      "480:\tlearn: 0.1240560\ttest: 0.1354769\tbest: 0.1354769 (480)\ttotal: 11.5s\tremaining: 3m 46s\n",
      "500:\tlearn: 0.1236872\ttest: 0.1355039\tbest: 0.1354504 (494)\ttotal: 11.9s\tremaining: 3m 46s\n",
      "520:\tlearn: 0.1233738\ttest: 0.1355929\tbest: 0.1354504 (494)\ttotal: 12.4s\tremaining: 3m 45s\n",
      "540:\tlearn: 0.1228964\ttest: 0.1356636\tbest: 0.1354504 (494)\ttotal: 12.9s\tremaining: 3m 45s\n",
      "560:\tlearn: 0.1224562\ttest: 0.1357531\tbest: 0.1354504 (494)\ttotal: 13.4s\tremaining: 3m 44s\n",
      "580:\tlearn: 0.1220385\ttest: 0.1357641\tbest: 0.1354504 (494)\ttotal: 13.8s\tremaining: 3m 44s\n",
      "600:\tlearn: 0.1217419\ttest: 0.1357502\tbest: 0.1354504 (494)\ttotal: 14.3s\tremaining: 3m 43s\n",
      "620:\tlearn: 0.1214122\ttest: 0.1357799\tbest: 0.1354504 (494)\ttotal: 14.8s\tremaining: 3m 43s\n",
      "640:\tlearn: 0.1210401\ttest: 0.1358005\tbest: 0.1354504 (494)\ttotal: 15.2s\tremaining: 3m 42s\n",
      "660:\tlearn: 0.1205909\ttest: 0.1357841\tbest: 0.1354504 (494)\ttotal: 15.7s\tremaining: 3m 41s\n",
      "\n",
      "bestTest = 0.1354503803\n",
      "bestIteration = 494\n",
      "\n",
      "Shrink model to first 495 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6167006\ttest: 0.6168461\tbest: 0.6168461 (0)\ttotal: 26.9ms\tremaining: 4m 29s\n",
      "20:\tlearn: 0.1764522\ttest: 0.1769753\tbest: 0.1769753 (20)\ttotal: 494ms\tremaining: 3m 54s\n",
      "40:\tlearn: 0.1416679\ttest: 0.1427360\tbest: 0.1427360 (40)\ttotal: 1s\tremaining: 4m 3s\n",
      "60:\tlearn: 0.1359567\ttest: 0.1379633\tbest: 0.1379633 (60)\ttotal: 1.53s\tremaining: 4m 9s\n",
      "80:\tlearn: 0.1338434\ttest: 0.1366598\tbest: 0.1366598 (80)\ttotal: 2.05s\tremaining: 4m 10s\n",
      "100:\tlearn: 0.1332378\ttest: 0.1362966\tbest: 0.1362966 (100)\ttotal: 2.48s\tremaining: 4m 3s\n",
      "120:\tlearn: 0.1326116\ttest: 0.1359618\tbest: 0.1359618 (120)\ttotal: 2.93s\tremaining: 3m 58s\n",
      "140:\tlearn: 0.1319398\ttest: 0.1357191\tbest: 0.1357191 (140)\ttotal: 3.39s\tremaining: 3m 57s\n",
      "160:\tlearn: 0.1312976\ttest: 0.1355119\tbest: 0.1355119 (160)\ttotal: 3.86s\tremaining: 3m 55s\n",
      "180:\tlearn: 0.1307959\ttest: 0.1353606\tbest: 0.1353606 (180)\ttotal: 4.31s\tremaining: 3m 53s\n",
      "200:\tlearn: 0.1302523\ttest: 0.1352932\tbest: 0.1352732 (197)\ttotal: 4.75s\tremaining: 3m 51s\n",
      "220:\tlearn: 0.1297637\ttest: 0.1351631\tbest: 0.1351556 (219)\ttotal: 5.2s\tremaining: 3m 50s\n",
      "240:\tlearn: 0.1291030\ttest: 0.1349756\tbest: 0.1349756 (240)\ttotal: 5.68s\tremaining: 3m 50s\n",
      "260:\tlearn: 0.1284757\ttest: 0.1348371\tbest: 0.1348371 (260)\ttotal: 6.17s\tremaining: 3m 50s\n",
      "280:\tlearn: 0.1279036\ttest: 0.1347258\tbest: 0.1347258 (280)\ttotal: 6.62s\tremaining: 3m 48s\n",
      "300:\tlearn: 0.1274798\ttest: 0.1346839\tbest: 0.1346821 (299)\ttotal: 7.07s\tremaining: 3m 47s\n",
      "320:\tlearn: 0.1270763\ttest: 0.1346615\tbest: 0.1346615 (320)\ttotal: 7.54s\tremaining: 3m 47s\n",
      "340:\tlearn: 0.1265604\ttest: 0.1346867\tbest: 0.1346568 (322)\ttotal: 8.01s\tremaining: 3m 46s\n",
      "360:\tlearn: 0.1260417\ttest: 0.1346170\tbest: 0.1346106 (350)\ttotal: 8.46s\tremaining: 3m 45s\n",
      "380:\tlearn: 0.1255947\ttest: 0.1346136\tbest: 0.1345997 (376)\ttotal: 8.91s\tremaining: 3m 44s\n",
      "400:\tlearn: 0.1251130\ttest: 0.1345290\tbest: 0.1345162 (399)\ttotal: 9.39s\tremaining: 3m 44s\n",
      "420:\tlearn: 0.1247761\ttest: 0.1345196\tbest: 0.1345010 (413)\ttotal: 9.85s\tremaining: 3m 44s\n",
      "440:\tlearn: 0.1243540\ttest: 0.1344919\tbest: 0.1344882 (432)\ttotal: 10.3s\tremaining: 3m 43s\n",
      "460:\tlearn: 0.1240109\ttest: 0.1345226\tbest: 0.1344882 (432)\ttotal: 10.7s\tremaining: 3m 42s\n",
      "480:\tlearn: 0.1235883\ttest: 0.1344787\tbest: 0.1344704 (478)\ttotal: 11.2s\tremaining: 3m 41s\n",
      "500:\tlearn: 0.1231970\ttest: 0.1345324\tbest: 0.1344704 (478)\ttotal: 11.7s\tremaining: 3m 41s\n",
      "520:\tlearn: 0.1229612\ttest: 0.1345351\tbest: 0.1344704 (478)\ttotal: 12.1s\tremaining: 3m 40s\n",
      "540:\tlearn: 0.1225901\ttest: 0.1345204\tbest: 0.1344704 (478)\ttotal: 12.6s\tremaining: 3m 40s\n",
      "560:\tlearn: 0.1221580\ttest: 0.1345515\tbest: 0.1344704 (478)\ttotal: 13.1s\tremaining: 3m 39s\n",
      "580:\tlearn: 0.1217772\ttest: 0.1345813\tbest: 0.1344704 (478)\ttotal: 13.5s\tremaining: 3m 39s\n",
      "600:\tlearn: 0.1213620\ttest: 0.1346108\tbest: 0.1344704 (478)\ttotal: 14s\tremaining: 3m 39s\n",
      "620:\tlearn: 0.1209958\ttest: 0.1346360\tbest: 0.1344704 (478)\ttotal: 14.5s\tremaining: 3m 38s\n",
      "640:\tlearn: 0.1206316\ttest: 0.1346885\tbest: 0.1344704 (478)\ttotal: 14.9s\tremaining: 3m 38s\n",
      "\n",
      "bestTest = 0.1344704401\n",
      "bestIteration = 478\n",
      "\n",
      "Shrink model to first 479 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6175947\ttest: 0.6175209\tbest: 0.6175209 (0)\ttotal: 25.1ms\tremaining: 4m 11s\n",
      "20:\tlearn: 0.1791652\ttest: 0.1784555\tbest: 0.1784555 (20)\ttotal: 499ms\tremaining: 3m 57s\n",
      "40:\tlearn: 0.1423027\ttest: 0.1405333\tbest: 0.1405333 (40)\ttotal: 1.03s\tremaining: 4m 9s\n",
      "60:\tlearn: 0.1366119\ttest: 0.1349052\tbest: 0.1349052 (60)\ttotal: 1.57s\tremaining: 4m 16s\n",
      "80:\tlearn: 0.1345382\ttest: 0.1328725\tbest: 0.1328725 (80)\ttotal: 2.09s\tremaining: 4m 16s\n",
      "100:\tlearn: 0.1337739\ttest: 0.1323956\tbest: 0.1323956 (100)\ttotal: 2.55s\tremaining: 4m 9s\n",
      "120:\tlearn: 0.1331872\ttest: 0.1320647\tbest: 0.1320647 (120)\ttotal: 2.99s\tremaining: 4m 3s\n",
      "140:\tlearn: 0.1325739\ttest: 0.1316436\tbest: 0.1316351 (139)\ttotal: 3.46s\tremaining: 4m 2s\n",
      "160:\tlearn: 0.1318459\ttest: 0.1310762\tbest: 0.1310762 (160)\ttotal: 3.94s\tremaining: 4m 1s\n",
      "180:\tlearn: 0.1313018\ttest: 0.1306655\tbest: 0.1306655 (180)\ttotal: 4.42s\tremaining: 3m 59s\n",
      "200:\tlearn: 0.1307815\ttest: 0.1305100\tbest: 0.1305084 (199)\ttotal: 4.88s\tremaining: 3m 57s\n",
      "220:\tlearn: 0.1303350\ttest: 0.1303285\tbest: 0.1303230 (219)\ttotal: 5.36s\tremaining: 3m 57s\n",
      "240:\tlearn: 0.1298605\ttest: 0.1302369\tbest: 0.1302354 (232)\ttotal: 5.83s\tremaining: 3m 55s\n",
      "260:\tlearn: 0.1294433\ttest: 0.1301140\tbest: 0.1301033 (257)\ttotal: 6.28s\tremaining: 3m 54s\n",
      "280:\tlearn: 0.1288978\ttest: 0.1299672\tbest: 0.1299672 (280)\ttotal: 6.75s\tremaining: 3m 53s\n",
      "300:\tlearn: 0.1284785\ttest: 0.1298645\tbest: 0.1298600 (292)\ttotal: 7.22s\tremaining: 3m 52s\n",
      "320:\tlearn: 0.1280952\ttest: 0.1297659\tbest: 0.1297659 (320)\ttotal: 7.69s\tremaining: 3m 51s\n",
      "340:\tlearn: 0.1276322\ttest: 0.1296600\tbest: 0.1296483 (339)\ttotal: 8.16s\tremaining: 3m 51s\n",
      "360:\tlearn: 0.1272416\ttest: 0.1296166\tbest: 0.1295920 (356)\ttotal: 8.61s\tremaining: 3m 50s\n",
      "380:\tlearn: 0.1266531\ttest: 0.1295595\tbest: 0.1295336 (374)\ttotal: 9.11s\tremaining: 3m 49s\n",
      "400:\tlearn: 0.1262620\ttest: 0.1295854\tbest: 0.1295336 (374)\ttotal: 9.58s\tremaining: 3m 49s\n",
      "420:\tlearn: 0.1258481\ttest: 0.1295313\tbest: 0.1295313 (420)\ttotal: 10.1s\tremaining: 3m 48s\n",
      "440:\tlearn: 0.1255414\ttest: 0.1294718\tbest: 0.1294697 (435)\ttotal: 10.5s\tremaining: 3m 47s\n",
      "460:\tlearn: 0.1251429\ttest: 0.1293922\tbest: 0.1293898 (459)\ttotal: 11s\tremaining: 3m 47s\n",
      "480:\tlearn: 0.1247305\ttest: 0.1293850\tbest: 0.1293650 (475)\ttotal: 11.5s\tremaining: 3m 47s\n",
      "500:\tlearn: 0.1242263\ttest: 0.1292806\tbest: 0.1292783 (499)\ttotal: 12s\tremaining: 3m 47s\n",
      "520:\tlearn: 0.1238761\ttest: 0.1292898\tbest: 0.1292476 (510)\ttotal: 12.5s\tremaining: 3m 46s\n",
      "540:\tlearn: 0.1235033\ttest: 0.1293192\tbest: 0.1292476 (510)\ttotal: 12.9s\tremaining: 3m 46s\n",
      "560:\tlearn: 0.1231178\ttest: 0.1292907\tbest: 0.1292476 (510)\ttotal: 13.4s\tremaining: 3m 45s\n",
      "580:\tlearn: 0.1228519\ttest: 0.1292589\tbest: 0.1292421 (576)\ttotal: 13.9s\tremaining: 3m 44s\n",
      "600:\tlearn: 0.1224606\ttest: 0.1292544\tbest: 0.1292421 (576)\ttotal: 14.3s\tremaining: 3m 44s\n",
      "620:\tlearn: 0.1220831\ttest: 0.1292009\tbest: 0.1292005 (618)\ttotal: 14.8s\tremaining: 3m 43s\n",
      "640:\tlearn: 0.1215676\ttest: 0.1292207\tbest: 0.1291614 (622)\ttotal: 15.3s\tremaining: 3m 43s\n",
      "660:\tlearn: 0.1211297\ttest: 0.1292060\tbest: 0.1291614 (622)\ttotal: 15.8s\tremaining: 3m 43s\n",
      "680:\tlearn: 0.1207541\ttest: 0.1292009\tbest: 0.1291614 (622)\ttotal: 16.3s\tremaining: 3m 42s\n",
      "700:\tlearn: 0.1204394\ttest: 0.1292804\tbest: 0.1291614 (622)\ttotal: 16.8s\tremaining: 3m 42s\n",
      "720:\tlearn: 0.1200280\ttest: 0.1291882\tbest: 0.1291614 (622)\ttotal: 17.3s\tremaining: 3m 42s\n",
      "740:\tlearn: 0.1196538\ttest: 0.1291492\tbest: 0.1291492 (740)\ttotal: 17.8s\tremaining: 3m 41s\n",
      "760:\tlearn: 0.1192215\ttest: 0.1290183\tbest: 0.1290153 (758)\ttotal: 18.3s\tremaining: 3m 41s\n",
      "780:\tlearn: 0.1189338\ttest: 0.1290401\tbest: 0.1290153 (758)\ttotal: 18.7s\tremaining: 3m 41s\n",
      "800:\tlearn: 0.1185141\ttest: 0.1289986\tbest: 0.1289982 (797)\ttotal: 19.2s\tremaining: 3m 40s\n",
      "820:\tlearn: 0.1181571\ttest: 0.1290262\tbest: 0.1289825 (809)\ttotal: 19.7s\tremaining: 3m 40s\n",
      "840:\tlearn: 0.1178561\ttest: 0.1290751\tbest: 0.1289825 (809)\ttotal: 20.2s\tremaining: 3m 39s\n",
      "860:\tlearn: 0.1175532\ttest: 0.1290868\tbest: 0.1289825 (809)\ttotal: 20.7s\tremaining: 3m 39s\n",
      "880:\tlearn: 0.1172206\ttest: 0.1290849\tbest: 0.1289825 (809)\ttotal: 21.2s\tremaining: 3m 39s\n",
      "900:\tlearn: 0.1169730\ttest: 0.1291333\tbest: 0.1289825 (809)\ttotal: 21.6s\tremaining: 3m 38s\n",
      "920:\tlearn: 0.1166753\ttest: 0.1291477\tbest: 0.1289825 (809)\ttotal: 22.1s\tremaining: 3m 38s\n",
      "940:\tlearn: 0.1164528\ttest: 0.1292417\tbest: 0.1289825 (809)\ttotal: 22.6s\tremaining: 3m 37s\n",
      "960:\tlearn: 0.1161698\ttest: 0.1293007\tbest: 0.1289825 (809)\ttotal: 23.1s\tremaining: 3m 37s\n",
      "980:\tlearn: 0.1158734\ttest: 0.1292971\tbest: 0.1289825 (809)\ttotal: 23.6s\tremaining: 3m 36s\n",
      "1000:\tlearn: 0.1156159\ttest: 0.1292910\tbest: 0.1289825 (809)\ttotal: 24.1s\tremaining: 3m 36s\n",
      "1020:\tlearn: 0.1153157\ttest: 0.1293100\tbest: 0.1289825 (809)\ttotal: 24.6s\tremaining: 3m 35s\n",
      "1040:\tlearn: 0.1150254\ttest: 0.1293186\tbest: 0.1289825 (809)\ttotal: 25.1s\tremaining: 3m 35s\n",
      "1060:\tlearn: 0.1146989\ttest: 0.1292629\tbest: 0.1289825 (809)\ttotal: 25.5s\tremaining: 3m 35s\n",
      "\n",
      "bestTest = 0.1289825013\n",
      "bestIteration = 809\n",
      "\n",
      "Shrink model to first 810 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6227660\ttest: 0.6228591\tbest: 0.6228591 (0)\ttotal: 23.1ms\tremaining: 3m 50s\n",
      "20:\tlearn: 0.1778635\ttest: 0.1777921\tbest: 0.1777921 (20)\ttotal: 507ms\tremaining: 4m\n",
      "40:\tlearn: 0.1426234\ttest: 0.1432305\tbest: 0.1432305 (40)\ttotal: 1.02s\tremaining: 4m 7s\n",
      "60:\tlearn: 0.1364154\ttest: 0.1375821\tbest: 0.1375821 (60)\ttotal: 1.54s\tremaining: 4m 10s\n",
      "80:\tlearn: 0.1343529\ttest: 0.1362374\tbest: 0.1362374 (80)\ttotal: 2.04s\tremaining: 4m 10s\n",
      "100:\tlearn: 0.1335582\ttest: 0.1357828\tbest: 0.1357828 (100)\ttotal: 2.51s\tremaining: 4m 6s\n",
      "120:\tlearn: 0.1327348\ttest: 0.1354261\tbest: 0.1354261 (120)\ttotal: 2.97s\tremaining: 4m 2s\n",
      "140:\tlearn: 0.1321518\ttest: 0.1352280\tbest: 0.1352280 (140)\ttotal: 3.43s\tremaining: 3m 59s\n",
      "160:\tlearn: 0.1314787\ttest: 0.1350425\tbest: 0.1350425 (160)\ttotal: 3.92s\tremaining: 3m 59s\n",
      "180:\tlearn: 0.1309516\ttest: 0.1350703\tbest: 0.1350203 (171)\ttotal: 4.39s\tremaining: 3m 58s\n",
      "200:\tlearn: 0.1302966\ttest: 0.1349801\tbest: 0.1349801 (200)\ttotal: 4.87s\tremaining: 3m 57s\n",
      "220:\tlearn: 0.1296906\ttest: 0.1348371\tbest: 0.1348371 (220)\ttotal: 5.33s\tremaining: 3m 55s\n",
      "240:\tlearn: 0.1290188\ttest: 0.1349329\tbest: 0.1348371 (220)\ttotal: 5.82s\tremaining: 3m 55s\n",
      "260:\tlearn: 0.1283760\ttest: 0.1349322\tbest: 0.1348371 (220)\ttotal: 6.32s\tremaining: 3m 55s\n",
      "280:\tlearn: 0.1279143\ttest: 0.1349605\tbest: 0.1348371 (220)\ttotal: 6.79s\tremaining: 3m 54s\n",
      "300:\tlearn: 0.1274854\ttest: 0.1350119\tbest: 0.1348371 (220)\ttotal: 7.26s\tremaining: 3m 53s\n",
      "\n",
      "bestTest = 0.1348371401\n",
      "bestIteration = 220\n",
      "\n",
      "Shrink model to first 221 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6226515\ttest: 0.6227674\tbest: 0.6227674 (0)\ttotal: 22.4ms\tremaining: 3m 43s\n",
      "20:\tlearn: 0.1810630\ttest: 0.1805308\tbest: 0.1805308 (20)\ttotal: 494ms\tremaining: 3m 54s\n",
      "40:\tlearn: 0.1441992\ttest: 0.1432404\tbest: 0.1432404 (40)\ttotal: 1.02s\tremaining: 4m 8s\n",
      "60:\tlearn: 0.1368225\ttest: 0.1357589\tbest: 0.1357589 (60)\ttotal: 1.57s\tremaining: 4m 15s\n",
      "80:\tlearn: 0.1347902\ttest: 0.1339879\tbest: 0.1339879 (80)\ttotal: 2.07s\tremaining: 4m 14s\n",
      "100:\tlearn: 0.1336625\ttest: 0.1331712\tbest: 0.1331712 (100)\ttotal: 2.56s\tremaining: 4m 11s\n",
      "120:\tlearn: 0.1329172\ttest: 0.1326997\tbest: 0.1326997 (120)\ttotal: 3.02s\tremaining: 4m 6s\n",
      "140:\tlearn: 0.1320908\ttest: 0.1321624\tbest: 0.1321576 (139)\ttotal: 3.51s\tremaining: 4m 5s\n",
      "160:\tlearn: 0.1312401\ttest: 0.1319062\tbest: 0.1319062 (160)\ttotal: 4s\tremaining: 4m 4s\n",
      "180:\tlearn: 0.1306071\ttest: 0.1317086\tbest: 0.1317086 (180)\ttotal: 4.49s\tremaining: 4m 3s\n",
      "200:\tlearn: 0.1301490\ttest: 0.1315927\tbest: 0.1315923 (199)\ttotal: 4.95s\tremaining: 4m 1s\n",
      "220:\tlearn: 0.1297603\ttest: 0.1314852\tbest: 0.1314852 (220)\ttotal: 5.41s\tremaining: 3m 59s\n",
      "240:\tlearn: 0.1292804\ttest: 0.1314066\tbest: 0.1314051 (239)\ttotal: 5.88s\tremaining: 3m 58s\n",
      "260:\tlearn: 0.1286236\ttest: 0.1312129\tbest: 0.1311989 (259)\ttotal: 6.38s\tremaining: 3m 58s\n",
      "280:\tlearn: 0.1280970\ttest: 0.1310405\tbest: 0.1310392 (279)\ttotal: 6.85s\tremaining: 3m 57s\n",
      "300:\tlearn: 0.1276026\ttest: 0.1309763\tbest: 0.1309670 (297)\ttotal: 7.33s\tremaining: 3m 56s\n",
      "320:\tlearn: 0.1271558\ttest: 0.1309592\tbest: 0.1309482 (316)\ttotal: 7.81s\tremaining: 3m 55s\n",
      "340:\tlearn: 0.1266958\ttest: 0.1309512\tbest: 0.1309086 (333)\ttotal: 8.3s\tremaining: 3m 55s\n",
      "360:\tlearn: 0.1263916\ttest: 0.1309500\tbest: 0.1309086 (333)\ttotal: 8.75s\tremaining: 3m 53s\n",
      "380:\tlearn: 0.1259490\ttest: 0.1308932\tbest: 0.1308932 (380)\ttotal: 9.22s\tremaining: 3m 52s\n",
      "400:\tlearn: 0.1254705\ttest: 0.1308930\tbest: 0.1308883 (399)\ttotal: 9.71s\tremaining: 3m 52s\n",
      "420:\tlearn: 0.1250995\ttest: 0.1308017\tbest: 0.1307997 (418)\ttotal: 10.2s\tremaining: 3m 52s\n",
      "440:\tlearn: 0.1247378\ttest: 0.1308290\tbest: 0.1307997 (418)\ttotal: 10.7s\tremaining: 3m 50s\n",
      "460:\tlearn: 0.1243068\ttest: 0.1307775\tbest: 0.1307476 (451)\ttotal: 11.1s\tremaining: 3m 50s\n",
      "480:\tlearn: 0.1238984\ttest: 0.1307980\tbest: 0.1307476 (451)\ttotal: 11.6s\tremaining: 3m 50s\n",
      "500:\tlearn: 0.1236178\ttest: 0.1307410\tbest: 0.1307340 (496)\ttotal: 12.1s\tremaining: 3m 49s\n",
      "520:\tlearn: 0.1232310\ttest: 0.1307496\tbest: 0.1307340 (501)\ttotal: 12.6s\tremaining: 3m 48s\n",
      "540:\tlearn: 0.1229103\ttest: 0.1308043\tbest: 0.1307340 (501)\ttotal: 13s\tremaining: 3m 48s\n",
      "560:\tlearn: 0.1225765\ttest: 0.1308215\tbest: 0.1307340 (501)\ttotal: 13.5s\tremaining: 3m 47s\n",
      "580:\tlearn: 0.1221562\ttest: 0.1308460\tbest: 0.1307340 (501)\ttotal: 14s\tremaining: 3m 47s\n",
      "600:\tlearn: 0.1217730\ttest: 0.1308209\tbest: 0.1307340 (501)\ttotal: 14.5s\tremaining: 3m 46s\n",
      "620:\tlearn: 0.1214029\ttest: 0.1307915\tbest: 0.1307340 (501)\ttotal: 15s\tremaining: 3m 46s\n",
      "640:\tlearn: 0.1208355\ttest: 0.1308205\tbest: 0.1307340 (501)\ttotal: 15.5s\tremaining: 3m 46s\n",
      "660:\tlearn: 0.1203655\ttest: 0.1308568\tbest: 0.1307340 (501)\ttotal: 16s\tremaining: 3m 45s\n",
      "\n",
      "bestTest = 0.1307339956\n",
      "bestIteration = 501\n",
      "\n",
      "Shrink model to first 502 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6218309\ttest: 0.6221615\tbest: 0.6221615 (0)\ttotal: 23.6ms\tremaining: 3m 56s\n",
      "20:\tlearn: 0.1741626\ttest: 0.1754012\tbest: 0.1754012 (20)\ttotal: 497ms\tremaining: 3m 56s\n",
      "40:\tlearn: 0.1419342\ttest: 0.1442722\tbest: 0.1442722 (40)\ttotal: 1.03s\tremaining: 4m 9s\n",
      "60:\tlearn: 0.1360784\ttest: 0.1393000\tbest: 0.1393000 (60)\ttotal: 1.57s\tremaining: 4m 15s\n",
      "80:\tlearn: 0.1342378\ttest: 0.1382285\tbest: 0.1382278 (79)\ttotal: 2.08s\tremaining: 4m 14s\n",
      "100:\tlearn: 0.1333275\ttest: 0.1376766\tbest: 0.1376766 (100)\ttotal: 2.56s\tremaining: 4m 10s\n",
      "120:\tlearn: 0.1326886\ttest: 0.1374339\tbest: 0.1374325 (119)\ttotal: 3.03s\tremaining: 4m 7s\n",
      "140:\tlearn: 0.1321038\ttest: 0.1370907\tbest: 0.1370907 (140)\ttotal: 3.5s\tremaining: 4m 4s\n",
      "160:\tlearn: 0.1314046\ttest: 0.1368596\tbest: 0.1368503 (156)\ttotal: 3.98s\tremaining: 4m 3s\n",
      "180:\tlearn: 0.1307441\ttest: 0.1367000\tbest: 0.1366770 (178)\ttotal: 4.49s\tremaining: 4m 3s\n",
      "200:\tlearn: 0.1301026\ttest: 0.1364950\tbest: 0.1364950 (200)\ttotal: 5s\tremaining: 4m 3s\n",
      "220:\tlearn: 0.1294967\ttest: 0.1363980\tbest: 0.1363801 (219)\ttotal: 5.47s\tremaining: 4m 1s\n",
      "240:\tlearn: 0.1287192\ttest: 0.1361391\tbest: 0.1361391 (240)\ttotal: 5.97s\tremaining: 4m 1s\n",
      "260:\tlearn: 0.1281506\ttest: 0.1359877\tbest: 0.1359877 (260)\ttotal: 6.46s\tremaining: 4m 1s\n",
      "280:\tlearn: 0.1277674\ttest: 0.1360524\tbest: 0.1359796 (266)\ttotal: 6.94s\tremaining: 4m\n",
      "300:\tlearn: 0.1272621\ttest: 0.1359919\tbest: 0.1359752 (295)\ttotal: 7.41s\tremaining: 3m 58s\n",
      "320:\tlearn: 0.1268579\ttest: 0.1359613\tbest: 0.1359613 (320)\ttotal: 7.88s\tremaining: 3m 57s\n",
      "340:\tlearn: 0.1262760\ttest: 0.1358983\tbest: 0.1358626 (335)\ttotal: 8.38s\tremaining: 3m 57s\n",
      "360:\tlearn: 0.1259583\ttest: 0.1358653\tbest: 0.1358467 (348)\ttotal: 8.85s\tremaining: 3m 56s\n",
      "380:\tlearn: 0.1254187\ttest: 0.1358305\tbest: 0.1358165 (375)\ttotal: 9.35s\tremaining: 3m 55s\n",
      "400:\tlearn: 0.1249642\ttest: 0.1358641\tbest: 0.1358109 (385)\ttotal: 9.81s\tremaining: 3m 54s\n",
      "420:\tlearn: 0.1245093\ttest: 0.1358314\tbest: 0.1358109 (385)\ttotal: 10.3s\tremaining: 3m 54s\n",
      "440:\tlearn: 0.1241306\ttest: 0.1359235\tbest: 0.1358109 (385)\ttotal: 10.8s\tremaining: 3m 53s\n",
      "460:\tlearn: 0.1238127\ttest: 0.1359933\tbest: 0.1358109 (385)\ttotal: 11.3s\tremaining: 3m 52s\n",
      "480:\tlearn: 0.1235305\ttest: 0.1359601\tbest: 0.1358109 (385)\ttotal: 11.7s\tremaining: 3m 51s\n",
      "500:\tlearn: 0.1232318\ttest: 0.1359776\tbest: 0.1358109 (385)\ttotal: 12.2s\tremaining: 3m 51s\n",
      "520:\tlearn: 0.1228299\ttest: 0.1359595\tbest: 0.1358109 (385)\ttotal: 12.7s\tremaining: 3m 51s\n",
      "\n",
      "bestTest = 0.1358108871\n",
      "bestIteration = 385\n",
      "\n",
      "Shrink model to first 386 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6226136\ttest: 0.6226941\tbest: 0.6226941 (0)\ttotal: 21.5ms\tremaining: 3m 35s\n",
      "20:\tlearn: 0.1764130\ttest: 0.1767579\tbest: 0.1767579 (20)\ttotal: 485ms\tremaining: 3m 50s\n",
      "40:\tlearn: 0.1426275\ttest: 0.1438840\tbest: 0.1438840 (40)\ttotal: 1.02s\tremaining: 4m 7s\n",
      "60:\tlearn: 0.1362610\ttest: 0.1387991\tbest: 0.1387991 (60)\ttotal: 1.55s\tremaining: 4m 13s\n",
      "80:\tlearn: 0.1342862\ttest: 0.1374305\tbest: 0.1374305 (80)\ttotal: 2.05s\tremaining: 4m 11s\n",
      "100:\tlearn: 0.1333707\ttest: 0.1369410\tbest: 0.1369410 (100)\ttotal: 2.52s\tremaining: 4m 6s\n",
      "120:\tlearn: 0.1326109\ttest: 0.1366354\tbest: 0.1366354 (120)\ttotal: 3.01s\tremaining: 4m 5s\n",
      "140:\tlearn: 0.1319798\ttest: 0.1364462\tbest: 0.1364371 (139)\ttotal: 3.48s\tremaining: 4m 3s\n",
      "160:\tlearn: 0.1311869\ttest: 0.1361078\tbest: 0.1361078 (160)\ttotal: 3.96s\tremaining: 4m 2s\n",
      "180:\tlearn: 0.1305863\ttest: 0.1359060\tbest: 0.1359060 (180)\ttotal: 4.44s\tremaining: 4m\n",
      "200:\tlearn: 0.1300013\ttest: 0.1355763\tbest: 0.1355763 (200)\ttotal: 4.93s\tremaining: 4m\n",
      "220:\tlearn: 0.1294691\ttest: 0.1353400\tbest: 0.1353400 (220)\ttotal: 5.42s\tremaining: 3m 59s\n",
      "240:\tlearn: 0.1288775\ttest: 0.1351580\tbest: 0.1351580 (240)\ttotal: 5.88s\tremaining: 3m 58s\n",
      "260:\tlearn: 0.1285664\ttest: 0.1350889\tbest: 0.1350746 (253)\ttotal: 6.33s\tremaining: 3m 56s\n",
      "280:\tlearn: 0.1281565\ttest: 0.1349893\tbest: 0.1349783 (279)\ttotal: 6.8s\tremaining: 3m 55s\n",
      "300:\tlearn: 0.1277385\ttest: 0.1349773\tbest: 0.1349299 (287)\ttotal: 7.27s\tremaining: 3m 54s\n",
      "320:\tlearn: 0.1273244\ttest: 0.1349086\tbest: 0.1349086 (320)\ttotal: 7.72s\tremaining: 3m 52s\n",
      "340:\tlearn: 0.1268461\ttest: 0.1348744\tbest: 0.1348580 (335)\ttotal: 8.19s\tremaining: 3m 51s\n",
      "360:\tlearn: 0.1263256\ttest: 0.1347308\tbest: 0.1347308 (360)\ttotal: 8.68s\tremaining: 3m 51s\n",
      "380:\tlearn: 0.1259141\ttest: 0.1346757\tbest: 0.1346657 (374)\ttotal: 9.14s\tremaining: 3m 50s\n",
      "400:\tlearn: 0.1256384\ttest: 0.1347209\tbest: 0.1346657 (374)\ttotal: 9.59s\tremaining: 3m 49s\n",
      "420:\tlearn: 0.1253309\ttest: 0.1346948\tbest: 0.1346657 (374)\ttotal: 10s\tremaining: 3m 48s\n",
      "440:\tlearn: 0.1249069\ttest: 0.1347296\tbest: 0.1346641 (428)\ttotal: 10.5s\tremaining: 3m 48s\n",
      "460:\tlearn: 0.1244849\ttest: 0.1347926\tbest: 0.1346641 (428)\ttotal: 11s\tremaining: 3m 47s\n",
      "480:\tlearn: 0.1241239\ttest: 0.1348349\tbest: 0.1346641 (428)\ttotal: 11.5s\tremaining: 3m 46s\n",
      "500:\tlearn: 0.1236896\ttest: 0.1348717\tbest: 0.1346641 (428)\ttotal: 11.9s\tremaining: 3m 45s\n",
      "520:\tlearn: 0.1233103\ttest: 0.1349322\tbest: 0.1346641 (428)\ttotal: 12.4s\tremaining: 3m 45s\n",
      "540:\tlearn: 0.1229093\ttest: 0.1348228\tbest: 0.1346641 (428)\ttotal: 12.9s\tremaining: 3m 44s\n",
      "560:\tlearn: 0.1224921\ttest: 0.1347544\tbest: 0.1346641 (428)\ttotal: 13.3s\tremaining: 3m 44s\n",
      "\n",
      "bestTest = 0.1346641325\n",
      "bestIteration = 428\n",
      "\n",
      "Shrink model to first 429 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'Logloss', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6201957\ttest: 0.6201399\tbest: 0.6201399 (0)\ttotal: 23.2ms\tremaining: 3m 52s\n",
      "20:\tlearn: 0.1775492\ttest: 0.1778024\tbest: 0.1778024 (20)\ttotal: 498ms\tremaining: 3m 56s\n",
      "40:\tlearn: 0.1417748\ttest: 0.1430780\tbest: 0.1430780 (40)\ttotal: 1.03s\tremaining: 4m 9s\n",
      "60:\tlearn: 0.1363770\ttest: 0.1386894\tbest: 0.1386894 (60)\ttotal: 1.55s\tremaining: 4m 11s\n",
      "80:\tlearn: 0.1342325\ttest: 0.1371646\tbest: 0.1371646 (80)\ttotal: 2.06s\tremaining: 4m 12s\n",
      "100:\tlearn: 0.1335580\ttest: 0.1369673\tbest: 0.1369673 (100)\ttotal: 2.52s\tremaining: 4m 7s\n",
      "120:\tlearn: 0.1324540\ttest: 0.1364819\tbest: 0.1364717 (119)\ttotal: 3s\tremaining: 4m 5s\n",
      "140:\tlearn: 0.1317826\ttest: 0.1361904\tbest: 0.1361813 (136)\ttotal: 3.47s\tremaining: 4m 2s\n",
      "160:\tlearn: 0.1312004\ttest: 0.1360084\tbest: 0.1360077 (159)\ttotal: 3.95s\tremaining: 4m 1s\n",
      "180:\tlearn: 0.1307732\ttest: 0.1359277\tbest: 0.1359218 (179)\ttotal: 4.42s\tremaining: 3m 59s\n",
      "200:\tlearn: 0.1300247\ttest: 0.1357423\tbest: 0.1357423 (200)\ttotal: 4.91s\tremaining: 3m 59s\n",
      "220:\tlearn: 0.1296054\ttest: 0.1356137\tbest: 0.1356137 (220)\ttotal: 5.37s\tremaining: 3m 57s\n",
      "240:\tlearn: 0.1291535\ttest: 0.1355802\tbest: 0.1355687 (238)\ttotal: 5.85s\tremaining: 3m 56s\n",
      "260:\tlearn: 0.1285261\ttest: 0.1354954\tbest: 0.1354749 (258)\ttotal: 6.34s\tremaining: 3m 56s\n",
      "280:\tlearn: 0.1280752\ttest: 0.1353595\tbest: 0.1353595 (280)\ttotal: 6.81s\tremaining: 3m 55s\n",
      "300:\tlearn: 0.1275842\ttest: 0.1353269\tbest: 0.1353071 (297)\ttotal: 7.28s\tremaining: 3m 54s\n",
      "320:\tlearn: 0.1271072\ttest: 0.1353291\tbest: 0.1352863 (313)\ttotal: 7.76s\tremaining: 3m 54s\n",
      "340:\tlearn: 0.1266199\ttest: 0.1352976\tbest: 0.1352863 (313)\ttotal: 8.26s\tremaining: 3m 53s\n",
      "360:\tlearn: 0.1263563\ttest: 0.1353096\tbest: 0.1352792 (343)\ttotal: 8.71s\tremaining: 3m 52s\n",
      "380:\tlearn: 0.1260492\ttest: 0.1353361\tbest: 0.1352792 (343)\ttotal: 9.17s\tremaining: 3m 51s\n",
      "400:\tlearn: 0.1256982\ttest: 0.1353410\tbest: 0.1352792 (343)\ttotal: 9.65s\tremaining: 3m 50s\n",
      "420:\tlearn: 0.1253569\ttest: 0.1353756\tbest: 0.1352792 (343)\ttotal: 10.2s\tremaining: 3m 51s\n",
      "440:\tlearn: 0.1250604\ttest: 0.1354128\tbest: 0.1352792 (343)\ttotal: 10.6s\tremaining: 3m 50s\n",
      "460:\tlearn: 0.1245894\ttest: 0.1354433\tbest: 0.1352792 (343)\ttotal: 11.1s\tremaining: 3m 49s\n",
      "\n",
      "bestTest = 0.1352792171\n",
      "bestIteration = 343\n",
      "\n",
      "Shrink model to first 344 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "\t0.8412\t = Validation score   (roc_auc)\n",
      "\t496.17s\t = Training   runtime\n",
      "\t1.33s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\model.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1894.74s of the 1894.72s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit NNFastAiTabularModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S4F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 99.56 / 185.49 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.17369335889816284.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14938300848007202.\n",
      "Better model found at epoch 2 with valid_loss value: 0.14297999441623688.\n",
      "Better model found at epoch 4 with valid_loss value: 0.14281754195690155.\n",
      "Better model found at epoch 6 with valid_loss value: 0.1411411464214325.\n",
      "No improvement since epoch 6: early stopping\n",
      "Model validation metrics: 0.1411411464214325\n",
      "\tFitting S4F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 331 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=331, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 102.08 / 202.84 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.236833855509758.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14565858244895935.\n",
      "Better model found at epoch 4 with valid_loss value: 0.14309200644493103.\n",
      "Better model found at epoch 9 with valid_loss value: 0.13831092417240143.\n",
      "No improvement since epoch 9: early stopping\n",
      "Model validation metrics: 0.13831092417240143\n",
      "\tFitting S4F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 102.04 / 224.29 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.667818546295166.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14412634074687958.\n",
      "Better model found at epoch 2 with valid_loss value: 0.14056208729743958.\n",
      "Better model found at epoch 3 with valid_loss value: 0.13810262084007263.\n",
      "Better model found at epoch 5 with valid_loss value: 0.13620811700820923.\n",
      "Better model found at epoch 9 with valid_loss value: 0.13515529036521912.\n",
      "Better model found at epoch 10 with valid_loss value: 0.13482628762722015.\n",
      "Model validation metrics: 0.13482628762722015\n",
      "\tFitting S4F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 332 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=332, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 108.74 / 254.74 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.1948118358850479.\n",
      "Better model found at epoch 1 with valid_loss value: 0.1449822336435318.\n",
      "Better model found at epoch 2 with valid_loss value: 0.14140886068344116.\n",
      "Better model found at epoch 11 with valid_loss value: 0.14053742587566376.\n",
      "Model validation metrics: 0.14053742587566376\n",
      "\tFitting S4F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 105.91 / 300.29 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.17208676040172577.\n",
      "Better model found at epoch 1 with valid_loss value: 0.1631418764591217.\n",
      "Better model found at epoch 2 with valid_loss value: 0.14573955535888672.\n",
      "Better model found at epoch 3 with valid_loss value: 0.14194299280643463.\n",
      "Better model found at epoch 4 with valid_loss value: 0.1403096318244934.\n",
      "Better model found at epoch 5 with valid_loss value: 0.13816183805465698.\n",
      "No improvement since epoch 5: early stopping\n",
      "Model validation metrics: 0.13816183805465698\n",
      "\tFitting S4F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 111.36 / 379.37 sec\n",
      "Better model found at epoch 0 with valid_loss value: 1875970.75.\n",
      "Better model found at epoch 1 with valid_loss value: 0.14619126915931702.\n",
      "Better model found at epoch 2 with valid_loss value: 0.14539934694766998.\n",
      "Better model found at epoch 3 with valid_loss value: 0.14338915050029755.\n",
      "Better model found at epoch 4 with valid_loss value: 0.14314055442810059.\n",
      "No improvement since epoch 4: early stopping\n",
      "Model validation metrics: 0.14314055442810059\n",
      "\tFitting S4F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 334 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(334, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=334, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 171.21 / 531.88 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.17996014654636383.\n",
      "Better model found at epoch 1 with valid_loss value: 0.144452303647995.\n",
      "Better model found at epoch 4 with valid_loss value: 0.1436724215745926.\n",
      "Better model found at epoch 5 with valid_loss value: 0.14012885093688965.\n",
      "No improvement since epoch 5: early stopping\n",
      "Model validation metrics: 0.14012885093688965\n",
      "\tFitting S4F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 0/0 categorical features\n",
      "Using 336 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList()\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=336, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Automated epochs selection: training for 30 epoch(s). Estimated time budget use 151.88 / 971.97 sec\n",
      "Better model found at epoch 0 with valid_loss value: 0.17176033556461334.\n",
      "Better model found at epoch 1 with valid_loss value: 0.1494867205619812.\n",
      "Better model found at epoch 3 with valid_loss value: 0.14555427432060242.\n",
      "Better model found at epoch 6 with valid_loss value: 0.14183132350444794.\n",
      "No improvement since epoch 6: early stopping\n",
      "Model validation metrics: 0.14183132350444794\n",
      "Saving AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\model.pkl\n",
      "\t0.8317\t = Validation score   (roc_auc)\n",
      "\t2993.71s\t = Training   runtime\n",
      "\t19.26s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\model.pkl\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1094.54s of the 1094.53s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit XGBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S4F1 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61127\n",
      "[50]\tvalidation_0-logloss:0.13651\n",
      "[100]\tvalidation_0-logloss:0.13662\n",
      "[102]\tvalidation_0-logloss:0.13660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F2 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61160\n",
      "[50]\tvalidation_0-logloss:0.13501\n",
      "[100]\tvalidation_0-logloss:0.13422\n",
      "[138]\tvalidation_0-logloss:0.13442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F3 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61156\n",
      "[50]\tvalidation_0-logloss:0.13117\n",
      "[100]\tvalidation_0-logloss:0.13003\n",
      "[150]\tvalidation_0-logloss:0.13010\n",
      "[151]\tvalidation_0-logloss:0.13010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F4 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61153\n",
      "[50]\tvalidation_0-logloss:0.13588\n",
      "[100]\tvalidation_0-logloss:0.13588\n",
      "[109]\tvalidation_0-logloss:0.13603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F5 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61133\n",
      "[50]\tvalidation_0-logloss:0.13122\n",
      "[100]\tvalidation_0-logloss:0.13065\n",
      "[131]\tvalidation_0-logloss:0.13083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F6 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61160\n",
      "[50]\tvalidation_0-logloss:0.13719\n",
      "[100]\tvalidation_0-logloss:0.13654\n",
      "[115]\tvalidation_0-logloss:0.13674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F7 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61141\n",
      "[50]\tvalidation_0-logloss:0.13519\n",
      "[100]\tvalidation_0-logloss:0.13494\n",
      "[115]\tvalidation_0-logloss:0.13499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F8 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61127\n",
      "[50]\tvalidation_0-logloss:0.13516\n",
      "[100]\tvalidation_0-logloss:0.13527\n",
      "[108]\tvalidation_0-logloss:0.13538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\XGBoost_BAG_L1\\model.pkl\n",
      "\t0.8412\t = Validation score   (roc_auc)\n",
      "\t96.44s\t = Training   runtime\n",
      "\t3.23s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1065.48s of the 1065.46s of remaining time.\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S4F1 - S4F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S4F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138047\n",
      "[100]\tvalid_set's binary_logloss: 0.136628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.137057\n",
      "[100]\tvalid_set's binary_logloss: 0.134895\n",
      "[150]\tvalid_set's binary_logloss: 0.135542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.134782\n",
      "[100]\tvalid_set's binary_logloss: 0.131658\n",
      "[150]\tvalid_set's binary_logloss: 0.131535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138172\n",
      "[100]\tvalid_set's binary_logloss: 0.136346\n",
      "[150]\tvalid_set's binary_logloss: 0.136752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.134469\n",
      "[100]\tvalid_set's binary_logloss: 0.131253\n",
      "[150]\tvalid_set's binary_logloss: 0.131494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.140054\n",
      "[100]\tvalid_set's binary_logloss: 0.138565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.138585\n",
      "[100]\tvalid_set's binary_logloss: 0.136924\n",
      "[150]\tvalid_set's binary_logloss: 0.137639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S4F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's binary_logloss: 0.137718\n",
      "[100]\tvalid_set's binary_logloss: 0.136603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "\t0.8387\t = Validation score   (roc_auc)\n",
      "\t106.44s\t = Training   runtime\n",
      "\t2.33s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Not enough time left to finish repeated k-fold bagging, stopping early ...\n",
      "Completed 4/20 k-fold bagging repeats ...\n",
      "Loading: AutoGluonSantander/\\models\\KNeighborsUnif_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\KNeighborsDist_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\RandomForestGini_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\RandomForestEntr_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\ExtraTreesGini_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\ExtraTreesEntr_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\utils\\oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 539.49s of the 1031.48s of remaining time.\n",
      "Saving AutoGluonSantander/\\models\\WeightedEnsemble_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonSantander/\\models\\WeightedEnsemble_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "Ensemble size: 80\n",
      "Ensemble weights: \n",
      "[0.     0.     0.25   0.125  0.05   0.     0.1375 0.     0.     0.1625\n",
      " 0.275  0.    ]\n",
      "\t0.0s\t= Estimated out-of-fold prediction time...\n",
      "Saving AutoGluonSantander/\\models\\WeightedEnsemble_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonSantander/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "\t0.8444\t = Validation score   (roc_auc)\n",
      "\t20.15s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "AutoGluon training complete, total runtime = 4389.64s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Loading: AutoGluonSantander/\\models\\trainer.pkl\n",
      "Saving AutoGluonSantander/\\models\\trainer.pkl\n",
      "Saving AutoGluonSantander/\\learner.pkl\n",
      "Saving AutoGluonSantander/\\predictor.pkl\n",
      "Saving AutoGluonSantander/\\__version__ with contents \"0.5.2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutoGluonSantander/\\\")\n",
      "Loading: AutoGluonSantander/\\models\\KNeighborsUnif_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\KNeighborsDist_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\RandomForestGini_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\RandomForestEntr_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\ExtraTreesGini_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\ExtraTreesEntr_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMLarge_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:138: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                      model  score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       WeightedEnsemble_L2   0.844438      37.965096  3767.433768                0.015679          20.154757            2       True         13\n",
      "1            XGBoost_BAG_L1   0.841238       3.230496    96.437334                3.230496          96.437334            1       True         11\n",
      "2           CatBoost_BAG_L1   0.841219       1.326243   496.171743                1.326243         496.171743            1       True          7\n",
      "3           LightGBM_BAG_L1   0.841173       2.003024    60.107104                2.003024          60.107104            1       True          4\n",
      "4         LightGBMXT_BAG_L1   0.840388       2.597199    76.233277                2.597199          76.233277            1       True          3\n",
      "5      LightGBMLarge_BAG_L1   0.838658       2.332517   106.443576                2.332517         106.443576            1       True         12\n",
      "6    NeuralNetFastAI_BAG_L1   0.831707      19.262374  2993.714828               19.262374        2993.714828            1       True         10\n",
      "7   RandomForestEntr_BAG_L1   0.783323       9.592163    23.409308                9.592163          23.409308            1       True          6\n",
      "8   RandomForestGini_BAG_L1   0.783117       9.530079    24.614726                9.530079          24.614726            1       True          5\n",
      "9     ExtraTreesEntr_BAG_L1   0.775350      10.856616    33.758905               10.856616          33.758905            1       True          9\n",
      "10    ExtraTreesGini_BAG_L1   0.774427      10.488206    33.225120               10.488206          33.225120            1       True          8\n",
      "11    KNeighborsDist_BAG_L1   0.529747     140.404132     0.537408              140.404132           0.537408            1       True          2\n",
      "12    KNeighborsUnif_BAG_L1   0.528808     141.038042     0.554969              141.038042           0.554969            1       True          1\n",
      "Number of models trained: 13\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_KNN', 'StackerEnsembleModel_RF', 'StackerEnsembleModel_CatBoost', 'WeightedEnsembleModel', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_NNFastAiTabular', 'StackerEnsembleModel_XT', 'StackerEnsembleModel_XGBoost'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     : 109 | ['imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', ...]\n",
      "('int', [])       : 122 | ['ID', 'var3', 'var15', 'num_var1_0', 'num_var1', ...]\n",
      "('int', ['bool']) : 105 | ['ind_var1_0', 'ind_var1', 'ind_var5_0', 'ind_var5', 'ind_var6_0', ...]\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutoGluonSantander/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\NeuralNetFastAI_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\RandomForestGini_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonSantander/\\models\\XGBoost_BAG_L1\\model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "label = 'TARGET'  # name of target variable to predict in this competition\n",
    "eval_metric = 'roc_auc'  # Optional: specify that competition evaluation metric is AUC\n",
    "save_path = 'AutoGluonSantander/'  # where to store trained models\n",
    "\n",
    "train_data = pd.read_csv('train_santander.csv', low_memory=False)\n",
    "predictor = TabularPredictor(label=label, eval_metric=eval_metric, path=save_path, verbosity=3).fit(\n",
    "    train_data, presets='best_quality', time_limit=5400\n",
    ")\n",
    "\n",
    "results = predictor.fit_summary()\n",
    "test_data = pd.read_csv('test_santander.csv', low_memory=False)\n",
    "y_pred = predictor.predict(test_data)\n",
    "\n",
    "test_data['TARGET'] = y_pred\n",
    "test_data[['ID', 'TARGET']].to_csv('santander_autogluon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007506f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data\n",
    "del test_data\n",
    "del predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf796e88",
   "metadata": {},
   "source": [
    "# Liberty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5461fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutoGluonLiberty/\"\n",
      "Presets specified: ['best_quality']\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'auto_stack': True}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': None,\n",
      " 'auto_stack': True,\n",
      " 'calibrate': 'auto',\n",
      " 'excluded_model_types': None,\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'keep_only_best': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': None,\n",
      " 'num_bag_sets': None,\n",
      " 'num_stack_levels': None,\n",
      " 'pseudo_data': None,\n",
      " 'quantile_levels': None,\n",
      " 'refit_full': False,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 3}\n",
      "========================================\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Saving AutoGluonLiberty/\\learner.pkl\n",
      "Saving AutoGluonLiberty/\\predictor.pkl\n",
      "Beginning AutoGluon training ... Time limit = 5400s\n",
      "AutoGluon will save models to \"AutoGluonLiberty/\\\"\n",
      "AutoGluon Version:  0.5.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Train Data Rows:    452061\n",
      "Train Data Columns: 301\n",
      "Label Column: target\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (25.920137918, 0.0, 0.00723, 0.21969)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    6918.83 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1315.03 MB (19.0% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 19.0% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float64', 'float') : 290 | ['var10', 'var11', 'var12', 'var13', 'var14', ...]\n",
      "\t\t\t\t('int64', 'int')     :   1 | ['id']\n",
      "\t\t\t\t('object', 'object') :  10 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])  : 290 | ['var10', 'var11', 'var12', 'var13', 'var14', ...]\n",
      "\t\t\t\t('int', [])    :   1 | ['id']\n",
      "\t\t\t\t('object', []) :  10 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 289 | ['var10', 'var11', 'var12', 'var13', 'var14', ...]\n",
      "\t\t\t\t('int', [])       :   1 | ['id']\n",
      "\t\t\t\t('int', ['bool']) :   2 | ['dummy', 'weatherVar115']\n",
      "\t\t\t\t('object', [])    :   9 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\t\t\t3.5s = Fit runtime\n",
      "\t\t\t301 features in original data used to generate 301 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 289 | ['var10', 'var11', 'var12', 'var13', 'var14', ...]\n",
      "\t\t\t\t('int', [])       :   1 | ['id']\n",
      "\t\t\t\t('int', ['bool']) :   2 | ['dummy', 'weatherVar115']\n",
      "\t\t\t\t('object', [])    :   9 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 289 | ['var10', 'var11', 'var12', 'var13', 'var14', ...]\n",
      "\t\t\t\t('int', [])       :   1 | ['id']\n",
      "\t\t\t\t('int', ['bool']) :   2 | ['dummy', 'weatherVar115']\n",
      "\t\t\t\t('object', [])    :   9 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\t\t\t0.4s = Fit runtime\n",
      "\t\t\t301 features in original data used to generate 301 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 289 | ['var10', 'var11', 'var12', 'var13', 'var14', ...]\n",
      "\t\t\t\t('int', [])       :   1 | ['id']\n",
      "\t\t\t\t('int', ['bool']) :   2 | ['dummy', 'weatherVar115']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 289 | ['var10', 'var11', 'var12', 'var13', 'var14', ...]\n",
      "\t\t\t\t('int', [])       :   1 | ['id']\n",
      "\t\t\t\t('int', ['bool']) :   2 | ['dummy', 'weatherVar115']\n",
      "\t\t\t0.3s = Fit runtime\n",
      "\t\t\t292 features in original data used to generate 292 features in processed data.\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 9 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 9 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\t\t\t\t0.0s = Fit runtime\n",
      "\t\t\t\t9 features in original data used to generate 9 features in processed data.\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('object', []) : 9 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) : 9 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\t\t\t0.3s = Fit runtime\n",
      "\t\t\t9 features in original data used to generate 9 features in processed data.\n",
      "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  :   9 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\t\t\t\t('float', [])     : 289 | ['var10', 'var11', 'var12', 'var13', 'var14', ...]\n",
      "\t\t\t\t('int', [])       :   1 | ['id']\n",
      "\t\t\t\t('int', ['bool']) :   2 | ['dummy', 'weatherVar115']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  :   9 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\t\t\t\t('float', [])     : 289 | ['var10', 'var11', 'var12', 'var13', 'var14', ...]\n",
      "\t\t\t\t('int', [])       :   1 | ['id']\n",
      "\t\t\t\t('int', ['bool']) :   2 | ['dummy', 'weatherVar115']\n",
      "\t\t\t2.9s = Fit runtime\n",
      "\t\t\t301 features in original data used to generate 301 features in processed data.\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('float64', 'float') : 290 | ['var10', 'var11', 'var12', 'var13', 'var14', ...]\n",
      "\t\t('int64', 'int')     :   1 | ['id']\n",
      "\t\t('object', 'object') :  10 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 290 | ['var10', 'var11', 'var12', 'var13', 'var14', ...]\n",
      "\t\t('int', [])    :   1 | ['id']\n",
      "\t\t('object', []) :  10 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('category', 'category') :   9 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\t\t('float64', 'float')     : 289 | ['var10', 'var11', 'var12', 'var13', 'var14', ...]\n",
      "\t\t('int64', 'int')         :   1 | ['id']\n",
      "\t\t('int8', 'int')          :   2 | ['dummy', 'weatherVar115']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :   9 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "\t\t('float', [])     : 289 | ['var10', 'var11', 'var12', 'var13', 'var14', ...]\n",
      "\t\t('int', [])       :   1 | ['id']\n",
      "\t\t('int', ['bool']) :   2 | ['dummy', 'weatherVar115']\n",
      "\t15.9s = Fit runtime\n",
      "\t301 features in original data used to generate 301 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1053.76 MB (12.6% of available memory)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data preprocessing and feature engineering runtime = 17.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving AutoGluonLiberty/\\learner.pkl\n",
      "Saving AutoGluonLiberty/\\utils\\data\\X.pkl\n",
      "Saving AutoGluonLiberty/\\utils\\data\\y.pkl\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Model configs that will be trained (in order):\n",
      "\tKNeighborsUnif_BAG_L1: \t{'weights': 'uniform', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Unif', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tKNeighborsDist_BAG_L1: \t{'weights': 'distance', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Dist', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBMXT_BAG_L1: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tLightGBM_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tRandomForestMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
      "\tExtraTreesMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n",
      "\tXGBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
      "\tNeuralNetTorch_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
      "\tLightGBMLarge_BAG_L1: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 3587.47s of the 5382.54s of remaining time.\n",
      "Saving AutoGluonLiberty/\\models\\KNeighborsUnif_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\KNeighborsUnif_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Model is expected to require 21.94% of available memory...\n",
      "\tNot enough memory to train KNeighborsUnif_BAG_L1... Skipping this model.\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 3582.77s of the 5377.83s of remaining time.\n",
      "Saving AutoGluonLiberty/\\models\\KNeighborsDist_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\KNeighborsDist_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Model is expected to require 21.78% of available memory...\n",
      "\tNot enough memory to train KNeighborsDist_BAG_L1... Skipping this model.\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3578.05s of the 5373.12s of remaining time.\n",
      "Saving AutoGluonLiberty/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 8.247 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n",
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.982 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n",
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.704 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n",
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.737 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n",
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.978 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n",
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.7 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n",
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.697 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n",
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.996 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n",
      "Saving AutoGluonLiberty/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonLiberty/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-0.2197\t = Validation score   (-root_mean_squared_error)\n",
      "\t91.32s\t = Training   runtime\n",
      "\t2.37s\t = Validation runtime\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 3478.37s of the 5273.44s of remaining time.\n",
      "Saving AutoGluonLiberty/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.553 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n",
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.489 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n",
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.485 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n",
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.49 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n",
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.81 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n",
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.48 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n",
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.472 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n",
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.708 GB, but only 7.704 GB is available...\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n",
      "Saving AutoGluonLiberty/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonLiberty/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-0.2197\t = Validation score   (-root_mean_squared_error)\n",
      "\t86.88s\t = Training   runtime\n",
      "\t2.19s\t = Validation runtime\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3383.54s of the 5178.61s of remaining time.\n",
      "Saving AutoGluonLiberty/\\models\\RandomForestMSE_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\RandomForestMSE_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 79 due to low time. Expected time usage reduced from 12692.4s -> 3383.5s...\n",
      "\t141.01s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonLiberty/\\models\\RandomForestMSE_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonLiberty/\\models\\RandomForestMSE_BAG_L1\\model.pkl\n",
      "\t-0.2415\t = Validation score   (-root_mean_squared_error)\n",
      "\t2590.75s\t = Training   runtime\n",
      "\t18.4s\t = Validation runtime\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 771.17s of the 2566.24s of remaining time.\n",
      "Saving AutoGluonLiberty/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train CatBoost model, roughly requires: 6.708 GB, but only 7.492 GB is available...\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'RMSE', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2203297\ttest: 0.2149113\tbest: 0.2149113 (0)\ttotal: 538ms\tremaining: 1h 29m 42s\n",
      "20:\tlearn: 0.2195506\ttest: 0.2149027\tbest: 0.2149027 (20)\ttotal: 8.43s\tremaining: 1h 6m 47s\n",
      "40:\tlearn: 0.2186629\ttest: 0.2149037\tbest: 0.2149025 (22)\ttotal: 16.1s\tremaining: 1h 5m 13s\n",
      "\n",
      "bestTest = 0.2149025424\n",
      "bestIteration = 22\n",
      "\n",
      "Shrink model to first 23 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train CatBoost model, roughly requires: 6.708 GB, but only 7.719 GB is available...\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'RMSE', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2188848\ttest: 0.2251039\tbest: 0.2251039 (0)\ttotal: 489ms\tremaining: 1h 21m 31s\n",
      "20:\tlearn: 0.2184771\ttest: 0.2251056\tbest: 0.2250988 (15)\ttotal: 8.6s\tremaining: 1h 8m 7s\n",
      "40:\tlearn: 0.2179138\ttest: 0.2251178\tbest: 0.2250988 (15)\ttotal: 16.1s\tremaining: 1h 4m 59s\n",
      "\n",
      "bestTest = 0.2250988277\n",
      "bestIteration = 15\n",
      "\n",
      "Shrink model to first 16 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train CatBoost model, roughly requires: 6.708 GB, but only 7.351 GB is available...\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'RMSE', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2197013\ttest: 0.2194647\tbest: 0.2194647 (0)\ttotal: 425ms\tremaining: 1h 10m 53s\n",
      "20:\tlearn: 0.2190367\ttest: 0.2194627\tbest: 0.2194401 (11)\ttotal: 8.27s\tremaining: 1h 5m 30s\n",
      "\n",
      "bestTest = 0.2194401107\n",
      "bestIteration = 11\n",
      "\n",
      "Shrink model to first 12 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train CatBoost model, roughly requires: 6.708 GB, but only 7.624 GB is available...\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'RMSE', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2180575\ttest: 0.2306844\tbest: 0.2306844 (0)\ttotal: 442ms\tremaining: 1h 13m 40s\n",
      "20:\tlearn: 0.2172945\ttest: 0.2306837\tbest: 0.2306784 (5)\ttotal: 8.53s\tremaining: 1h 7m 34s\n",
      "\n",
      "bestTest = 0.2306783598\n",
      "bestIteration = 5\n",
      "\n",
      "Shrink model to first 6 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train CatBoost model, roughly requires: 6.708 GB, but only 7.227 GB is available...\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'RMSE', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2214448\ttest: 0.2068366\tbest: 0.2068366 (0)\ttotal: 421ms\tremaining: 1h 10m 7s\n",
      "20:\tlearn: 0.2209623\ttest: 0.2068500\tbest: 0.2068283 (13)\ttotal: 8.57s\tremaining: 1h 7m 50s\n",
      "\n",
      "bestTest = 0.2068282502\n",
      "bestIteration = 13\n",
      "\n",
      "Shrink model to first 14 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train CatBoost model, roughly requires: 6.708 GB, but only 7.654 GB is available...\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'RMSE', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2219775\ttest: 0.2028411\tbest: 0.2028411 (0)\ttotal: 444ms\tremaining: 1h 13m 55s\n",
      "20:\tlearn: 0.2215206\ttest: 0.2028411\tbest: 0.2028361 (15)\ttotal: 8.16s\tremaining: 1h 4m 37s\n",
      "40:\tlearn: 0.2208368\ttest: 0.2028569\tbest: 0.2028361 (15)\ttotal: 15.8s\tremaining: 1h 3m 50s\n",
      "\n",
      "bestTest = 0.2028361273\n",
      "bestIteration = 15\n",
      "\n",
      "Shrink model to first 16 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train CatBoost model, roughly requires: 6.708 GB, but only 7.3 GB is available...\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'RMSE', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2172855\ttest: 0.2356757\tbest: 0.2356757 (0)\ttotal: 423ms\tremaining: 1h 10m 30s\n",
      "20:\tlearn: 0.2164820\ttest: 0.2356758\tbest: 0.2356739 (5)\ttotal: 8.28s\tremaining: 1h 5m 32s\n",
      "\n",
      "bestTest = 0.2356739259\n",
      "bestIteration = 5\n",
      "\n",
      "Shrink model to first 6 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train CatBoost model, roughly requires: 6.708 GB, but only 7.681 GB is available...\n",
      "\tCatboost model hyperparameters: {'iterations': 10000, 'learning_rate': 0.05, 'random_seed': 0, 'allow_writing_files': False, 'eval_metric': 'RMSE', 'thread_count': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2195465\ttest: 0.2199826\tbest: 0.2199826 (0)\ttotal: 433ms\tremaining: 1h 12m 12s\n",
      "20:\tlearn: 0.2185781\ttest: 0.2200018\tbest: 0.2199782 (5)\ttotal: 8.08s\tremaining: 1h 4m\n",
      "\n",
      "bestTest = 0.2199781744\n",
      "bestIteration = 5\n",
      "\n",
      "Shrink model to first 6 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonLiberty/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonLiberty/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "\t-0.2197\t = Validation score   (-root_mean_squared_error)\n",
      "\t153.25s\t = Training   runtime\n",
      "\t1.63s\t = Validation runtime\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 610.01s of the 2405.07s of remaining time.\n",
      "Saving AutoGluonLiberty/\\models\\ExtraTreesMSE_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\ExtraTreesMSE_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 85 due to low time. Expected time usage reduced from 2139.4s -> 610.0s...\n",
      "\t23.4s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonLiberty/\\models\\ExtraTreesMSE_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonLiberty/\\models\\ExtraTreesMSE_BAG_L1\\model.pkl\n",
      "\t-0.2424\t = Validation score   (-root_mean_squared_error)\n",
      "\t717.71s\t = Training   runtime\n",
      "\t19.22s\t = Validation runtime\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Skipping NeuralNetFastAI_BAG_L1 due to lack of time remaining.\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Skipping XGBoost_BAG_L1 due to lack of time remaining.\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Skipping NeuralNetTorch_BAG_L1 due to lack of time remaining.\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Skipping LightGBMLarge_BAG_L1 due to lack of time remaining.\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Not enough time left to finish repeated k-fold bagging, stopping early ...\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Loading: AutoGluonLiberty/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\RandomForestMSE_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\ExtraTreesMSE_BAG_L1\\utils\\oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1664.67s of remaining time.\n",
      "Saving AutoGluonLiberty/\\models\\WeightedEnsemble_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\WeightedEnsemble_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "Ensemble size: 52\n",
      "Ensemble weights: \n",
      "[0.30769231 0.38461538 0.01923077 0.26923077 0.01923077]\n",
      "\t1.81s\t= Estimated out-of-fold prediction time...\n",
      "Saving AutoGluonLiberty/\\models\\WeightedEnsemble_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonLiberty/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "\t-0.2197\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tLightGBMXT_BAG_L2: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tLightGBM_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tRandomForestMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
      "\tExtraTreesMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n",
      "\tXGBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
      "\tNeuralNetTorch_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
      "\tLightGBMLarge_BAG_L2: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}}\n",
      "Fitting 9 L2 models ...\n",
      "Loading: AutoGluonLiberty/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\RandomForestMSE_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\CatBoost_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\ExtraTreesMSE_BAG_L1\\utils\\oof.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1659.21s of the 1658.57s of remaining time.\n",
      "Saving AutoGluonLiberty/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 6.765 GB, but only 6.588 GB is available...\n",
      "\tNot enough memory to train LightGBMXT_BAG_L2... Skipping this model.\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1650.8s of the 1650.6s of remaining time.\n",
      "Saving AutoGluonLiberty/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 6.765 GB, but only 6.609 GB is available...\n",
      "\tNot enough memory to train LightGBM_BAG_L2... Skipping this model.\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1643.08s of the 1642.88s of remaining time.\n",
      "Saving AutoGluonLiberty/\\models\\RandomForestMSE_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\RandomForestMSE_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Model is expected to require 13412.8s to train, which exceeds the maximum time limit of 1643.1s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestMSE_BAG_L2.\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 1458.08s of the 1457.88s of remaining time.\n",
      "Saving AutoGluonLiberty/\\models\\CatBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\CatBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Not enough memory to safely train CatBoost model, roughly requires: 6.765 GB, but only 6.618 GB is available...\n",
      "\tNot enough memory to train CatBoost_BAG_L2... Skipping this model.\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 1449.02s of the 1448.82s of remaining time.\n",
      "Saving AutoGluonLiberty/\\models\\ExtraTreesMSE_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\ExtraTreesMSE_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 171 due to low time. Expected time usage reduced from 2533.8s -> 1449.0s...\n",
      "\t42.63s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonLiberty/\\models\\ExtraTreesMSE_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonLiberty/\\models\\ExtraTreesMSE_BAG_L2\\model.pkl\n",
      "\t-0.2266\t = Validation score   (-root_mean_squared_error)\n",
      "\t1565.36s\t = Training   runtime\n",
      "\t36.94s\t = Validation runtime\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Skipping NeuralNetFastAI_BAG_L2 due to lack of time remaining.\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Skipping XGBoost_BAG_L2 due to lack of time remaining.\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Skipping NeuralNetTorch_BAG_L2 due to lack of time remaining.\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Skipping LightGBMLarge_BAG_L2 due to lack of time remaining.\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Not enough time left to finish repeated k-fold bagging, stopping early ...\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Loading: AutoGluonLiberty/\\models\\ExtraTreesMSE_BAG_L2\\utils\\oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L3: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -156.46s of remaining time.\n",
      "Saving AutoGluonLiberty/\\models\\WeightedEnsemble_L3\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\WeightedEnsemble_L3\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "Ensemble size: 1\n",
      "Ensemble weights: \n",
      "[1.]\n",
      "\t0.91s\t= Estimated out-of-fold prediction time...\n",
      "Saving AutoGluonLiberty/\\models\\WeightedEnsemble_L3\\utils\\oof.pkl\n",
      "Saving AutoGluonLiberty/\\models\\WeightedEnsemble_L3\\model.pkl\n",
      "\t-0.2266\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "AutoGluon training complete, total runtime = 5569.46s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Loading: AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Saving AutoGluonLiberty/\\models\\trainer.pkl\n",
      "Saving AutoGluonLiberty/\\learner.pkl\n",
      "Saving AutoGluonLiberty/\\predictor.pkl\n",
      "Saving AutoGluonLiberty/\\__version__ with contents \"0.5.2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutoGluonLiberty/\\\")\n",
      "Loading: AutoGluonLiberty/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\RandomForestMSE_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\ExtraTreesMSE_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\ExtraTreesMSE_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\WeightedEnsemble_L3\\model.pkl\n",
      "D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:138: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                    model  score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     WeightedEnsemble_L2  -0.219650      43.810157  3645.324386                0.008000           5.408999            2       True          6\n",
      "1         LightGBM_BAG_L1  -0.219677       2.193070    86.882173                2.193070          86.882173            1       True          2\n",
      "2         CatBoost_BAG_L1  -0.219679       1.625951   153.249865                1.625951         153.249865            1       True          4\n",
      "3       LightGBMXT_BAG_L1  -0.219682       2.370028    91.317809                2.370028          91.317809            1       True          1\n",
      "4    ExtraTreesMSE_BAG_L2  -0.226612      80.739929  5205.270795               36.937773        1565.355408            2       True          7\n",
      "5     WeightedEnsemble_L3  -0.226612      80.747929  5205.285766                0.008000           0.014971            3       True          8\n",
      "6  RandomForestMSE_BAG_L1  -0.241459      18.398049  2590.753300               18.398049        2590.753300            1       True          3\n",
      "7    ExtraTreesMSE_BAG_L1  -0.242442      19.215059   717.712239               19.215059         717.712239            1       True          5\n",
      "Number of models trained: 8\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_XT', 'StackerEnsembleModel_CatBoost', 'WeightedEnsembleModel', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_RF'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])  :   9 | ['var1', 'var2', 'var3', 'var4', 'var5', ...]\n",
      "('float', [])     : 289 | ['var10', 'var11', 'var12', 'var13', 'var14', ...]\n",
      "('int', [])       :   1 | ['id']\n",
      "('int', ['bool']) :   2 | ['dummy', 'weatherVar115']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutoGluonLiberty/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\CatBoost_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\ExtraTreesMSE_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLiberty/\\models\\RandomForestMSE_BAG_L1\\model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "label = 'target'  # name of target variable to predict in this competition\n",
    "eval_metric = 'rmse'  # Optional: specify that competition evaluation metric is AUC\n",
    "save_path = 'AutoGluonLiberty/'  # where to store trained models\n",
    "\n",
    "train_data = pd.read_csv('train_liberty.csv', low_memory=False)\n",
    "predictor = TabularPredictor(label=label, eval_metric=eval_metric, path=save_path, verbosity=3).fit(\n",
    "    train_data, presets='best_quality', time_limit=5400\n",
    ")\n",
    "\n",
    "results = predictor.fit_summary()\n",
    "test_data = pd.read_csv('test_liberty.csv', low_memory=False)\n",
    "y_predproba = predictor.predict(test_data)\n",
    "\n",
    "test_data['target'] = y_predproba\n",
    "test_data[['id', 'target']].to_csv('liberty_autogluon.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20976a6",
   "metadata": {},
   "source": [
    "# Loan default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7a496d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data\n",
    "del test_data\n",
    "del predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dee9ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"AutoGluonLoan/\"\n",
      "Presets specified: ['best_quality']\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'auto_stack': True}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': None,\n",
      " 'auto_stack': True,\n",
      " 'calibrate': 'auto',\n",
      " 'excluded_model_types': None,\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'keep_only_best': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': None,\n",
      " 'num_bag_sets': None,\n",
      " 'num_stack_levels': None,\n",
      " 'pseudo_data': None,\n",
      " 'quantile_levels': None,\n",
      " 'refit_full': False,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 3}\n",
      "========================================\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Saving AutoGluonLoan/\\learner.pkl\n",
      "Saving AutoGluonLoan/\\predictor.pkl\n",
      "Beginning AutoGluon training ... Time limit = 5400s\n",
      "AutoGluon will save models to \"AutoGluonLoan/\\\"\n",
      "AutoGluon Version:  0.5.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Train Data Rows:    105471\n",
      "Train Data Columns: 770\n",
      "Label Column: loss\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 89) unique label values:  [0, 1, 16, 19, 4, 11, 21, 2, 9, 5]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 43 out of 89 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.998464032767301\n",
      "Train Data Class Count: 43\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9868.06 MB\n",
      "\tTrain Data (Original)  Memory Usage: 789.56 MB (8.0% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 8.0% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float64', 'float') : 652 | ['f3', 'f7', 'f8', 'f9', 'f10', ...]\n",
      "\t\t\t\t('int64', 'int')     :  88 | ['id', 'f1', 'f2', 'f4', 'f5', ...]\n",
      "\t\t\t\t('object', 'object') :  19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\t\t\t\t('uint64', 'int')    :   1 | ['f466']\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])  : 652 | ['f3', 'f7', 'f8', 'f9', 'f10', ...]\n",
      "\t\t\t\t('int', [])    :  89 | ['id', 'f1', 'f2', 'f4', 'f5', ...]\n",
      "\t\t\t\t('object', []) :  19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 651 | ['f3', 'f7', 'f8', 'f9', 'f10', ...]\n",
      "\t\t\t\t('int', [])       :  87 | ['id', 'f1', 'f2', 'f4', 'f5', ...]\n",
      "\t\t\t\t('int', ['bool']) :   3 | ['f678', 'f776', 'f777']\n",
      "\t\t\t\t('object', [])    :  19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\t\t\t2.6s = Fit runtime\n",
      "\t\t\t760 features in original data used to generate 760 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 651 | ['f3', 'f7', 'f8', 'f9', 'f10', ...]\n",
      "\t\t\t\t('int', [])       :  87 | ['id', 'f1', 'f2', 'f4', 'f5', ...]\n",
      "\t\t\t\t('int', ['bool']) :   3 | ['f678', 'f776', 'f777']\n",
      "\t\t\t\t('object', [])    :  19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 651 | ['f3', 'f7', 'f8', 'f9', 'f10', ...]\n",
      "\t\t\t\t('int', [])       :  87 | ['id', 'f1', 'f2', 'f4', 'f5', ...]\n",
      "\t\t\t\t('int', ['bool']) :   3 | ['f678', 'f776', 'f777']\n",
      "\t\t\t\t('object', [])    :  19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\t\t\t0.3s = Fit runtime\n",
      "\t\t\t760 features in original data used to generate 760 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 651 | ['f3', 'f7', 'f8', 'f9', 'f10', ...]\n",
      "\t\t\t\t('int', [])       :  87 | ['id', 'f1', 'f2', 'f4', 'f5', ...]\n",
      "\t\t\t\t('int', ['bool']) :   3 | ['f678', 'f776', 'f777']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 651 | ['f3', 'f7', 'f8', 'f9', 'f10', ...]\n",
      "\t\t\t\t('int', [])       :  87 | ['id', 'f1', 'f2', 'f4', 'f5', ...]\n",
      "\t\t\t\t('int', ['bool']) :   3 | ['f678', 'f776', 'f777']\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t741 features in original data used to generate 741 features in processed data.\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', []) : 19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\t\t\t\t0.0s = Fit runtime\n",
      "\t\t\t\t19 features in original data used to generate 19 features in processed data.\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('object', []) : 19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', []) : 19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\t\t\t1.7s = Fit runtime\n",
      "\t\t\t19 features in original data used to generate 19 features in processed data.\n",
      "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  :  19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\t\t\t\t('float', [])     : 651 | ['f3', 'f7', 'f8', 'f9', 'f10', ...]\n",
      "\t\t\t\t('int', [])       :  87 | ['id', 'f1', 'f2', 'f4', 'f5', ...]\n",
      "\t\t\t\t('int', ['bool']) :   3 | ['f678', 'f776', 'f777']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])  :  19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\t\t\t\t('float', [])     : 651 | ['f3', 'f7', 'f8', 'f9', 'f10', ...]\n",
      "\t\t\t\t('int', [])       :  87 | ['id', 'f1', 'f2', 'f4', 'f5', ...]\n",
      "\t\t\t\t('int', ['bool']) :   3 | ['f678', 'f776', 'f777']\n",
      "\t\t\t2.0s = Fit runtime\n",
      "\t\t\t760 features in original data used to generate 760 features in processed data.\n",
      "\tUseless Original Features (Count: 10): ['f33', 'f34', 'f35', 'f37', 'f38', 'f700', 'f701', 'f702', 'f736', 'f764']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('float64', 'float') : 652 | ['f3', 'f7', 'f8', 'f9', 'f10', ...]\n",
      "\t\t('int64', 'int')     :  88 | ['id', 'f1', 'f2', 'f4', 'f5', ...]\n",
      "\t\t('object', 'object') :  19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\t\t('uint64', 'int')    :   1 | ['f466']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 652 | ['f3', 'f7', 'f8', 'f9', 'f10', ...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t('int', [])    :  89 | ['id', 'f1', 'f2', 'f4', 'f5', ...]\n",
      "\t\t('object', []) :  19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('category', 'category') :  19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\t\t('float64', 'float')     : 651 | ['f3', 'f7', 'f8', 'f9', 'f10', ...]\n",
      "\t\t('int64', 'int')         :  86 | ['id', 'f1', 'f2', 'f4', 'f5', ...]\n",
      "\t\t('int8', 'int')          :   3 | ['f678', 'f776', 'f777']\n",
      "\t\t('uint64', 'int')        :   1 | ['f466']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "\t\t('float', [])     : 651 | ['f3', 'f7', 'f8', 'f9', 'f10', ...]\n",
      "\t\t('int', [])       :  87 | ['id', 'f1', 'f2', 'f4', 'f5', ...]\n",
      "\t\t('int', ['bool']) :   3 | ['f678', 'f776', 'f777']\n",
      "\t11.9s = Fit runtime\n",
      "\t760 features in original data used to generate 760 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 625.65 MB (6.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 13.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving AutoGluonLoan/\\learner.pkl\n",
      "Saving AutoGluonLoan/\\utils\\data\\X.pkl\n",
      "Saving AutoGluonLoan/\\utils\\data\\y.pkl\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Model configs that will be trained (in order):\n",
      "\tKNeighborsUnif_BAG_L1: \t{'weights': 'uniform', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Unif', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tKNeighborsDist_BAG_L1: \t{'weights': 'distance', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Dist', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 95}}\n",
      "\tLightGBMXT_BAG_L1: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tLightGBM_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tRandomForestGini_BAG_L1: \t{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tRandomForestEntr_BAG_L1: \t{'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
      "\tExtraTreesGini_BAG_L1: \t{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tExtraTreesEntr_BAG_L1: \t{'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tXGBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
      "\tNeuralNetTorch_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
      "\tLightGBMLarge_BAG_L1: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 3590.42s of the 5386.95s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\KNeighborsUnif_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\KNeighborsUnif_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t0.16s \t= Train Time (Using 10000/105309 rows) (3589.8s remaining time)\n",
      "\t0.27s \t= Train Time (Using 20000/105309 rows) (3589.53s remaining time)\n",
      "\t0.52s \t= Train Time (Using 40000/105309 rows) (3589.01s remaining time)\n",
      "\t0.08s \t= Train Time (Using 105309/105309 rows) (3588.93s remaining time)\n",
      "\t351.8s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonLoan/\\models\\KNeighborsUnif_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonLoan/\\models\\KNeighborsUnif_BAG_L1\\model.pkl\n",
      "\t-0.6853\t = Validation score   (-mean_absolute_error)\n",
      "\t3.35s\t = Training   runtime\n",
      "\t353.32s\t = Validation runtime\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 3227.64s of the 5024.17s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\KNeighborsDist_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\KNeighborsDist_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t0.15s \t= Train Time (Using 10000/105309 rows) (3227.01s remaining time)\n",
      "\t0.27s \t= Train Time (Using 20000/105309 rows) (3226.74s remaining time)\n",
      "\t0.52s \t= Train Time (Using 40000/105309 rows) (3226.22s remaining time)\n",
      "\t0.08s \t= Train Time (Using 105309/105309 rows) (3226.14s remaining time)\n",
      "\t316.6s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonLoan/\\models\\KNeighborsDist_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonLoan/\\models\\KNeighborsDist_BAG_L1\\model.pkl\n",
      "\t-0.873\t = Validation score   (-mean_absolute_error)\n",
      "\t3.41s\t = Training   runtime\n",
      "\t357.28s\t = Validation runtime\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2860.94s of the 4657.47s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\NeuralNetFastAI_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit NNFastAiTabularModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 5.482 GB, but only 9.075 GB is available...\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 17/19 categorical features\n",
      "Using 740 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList(\n",
      "    (0): Embedding(3738, 160)\n",
      "    (1): Embedding(9099, 264)\n",
      "    (2): Embedding(8604, 256)\n",
      "    (3): Embedding(3451, 153)\n",
      "    (4): Embedding(6458, 218)\n",
      "    (5): Embedding(104, 22)\n",
      "    (6): Embedding(107, 22)\n",
      "    (7): Embedding(6764, 223)\n",
      "    (8): Embedding(7267, 233)\n",
      "    (9): Embedding(4544, 179)\n",
      "    (10): Embedding(1192, 84)\n",
      "    (11): Embedding(4098, 169)\n",
      "    (12): Embedding(730, 64)\n",
      "    (13): Embedding(21, 9)\n",
      "    (14): Embedding(24, 9)\n",
      "    (15): Embedding(966, 75)\n",
      "    (16): Embedding(1036, 78)\n",
      "  )\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(740, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=2958, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=43, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automated epochs selection: training for 4 epoch(s). Estimated time budget use 200.09 / 230.46 sec\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tException occured in `Recorder` when calling event `after_batch`:\n",
      "\t==:\n",
      "11008\n",
      "256\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1126, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1083, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 154, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 232, in _fit\n",
      "    self._fit_folds(X=X, y=y, model_base=model_base, X_pseudo=X_pseudo, y_pseudo=y_pseudo,\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 503, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 217, in after_all_folds_scheduled\n",
      "    self._fit_fold_model(job)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 222, in _fit_fold_model\n",
      "    fold_model = self._fit(self.model_base, time_start_fold, time_limit_fold, fold_ctx, self.model_base_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 252, in _fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **kwargs_fold)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 587, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py\", line 287, in _fit\n",
      "    self.model.fit_one_cycle(epochs, params['lr'], cbs=callbacks)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\callback\\schedule.py\", line 119, in fit_one_cycle\n",
      "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\learner.py\", line 256, in fit\n",
      "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\learner.py\", line 193, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\learner.py\", line 245, in _do_fit\n",
      "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\learner.py\", line 193, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\learner.py\", line 240, in _do_epoch\n",
      "    self._do_epoch_validate()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\learner.py\", line 236, in _do_epoch_validate\n",
      "    with torch.no_grad(): self._with_events(self.all_batches, 'validate', CancelValidException)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\learner.py\", line 193, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\learner.py\", line 199, in all_batches\n",
      "    for o in enumerate(self.dl): self.one_batch(*o)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\learner.py\", line 227, in one_batch\n",
      "    self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\learner.py\", line 195, in _with_events\n",
      "    self(f'after_{event_type}');  final()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\learner.py\", line 171, in __call__\n",
      "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastcore\\foundation.py\", line 156, in map\n",
      "    def map(self, f, *args, gen=False, **kwargs): return self._new(map_ex(self, f, *args, gen=gen, **kwargs))\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastcore\\basics.py\", line 840, in map_ex\n",
      "    return list(res)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastcore\\basics.py\", line 825, in __call__\n",
      "    return self.func(*fargs, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\learner.py\", line 175, in _call_one\n",
      "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\callback\\core.py\", line 62, in __call__\n",
      "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\callback\\core.py\", line 60, in __call__\n",
      "    try: res = getcallable(self, event_name)()\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\learner.py\", line 541, in after_batch\n",
      "    for met in mets: met.accumulate(self.learn)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\learner.py\", line 463, in accumulate\n",
      "    self.total += learn.to_detach(self.func(learn.pred, *learn.yb))*bs\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\metrics.py\", line 287, in mae\n",
      "    inp,targ = flatten_check(inp,targ)\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastai\\torch_core.py\", line 760, in flatten_check\n",
      "    test_eq(len(inp), len(targ))\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastcore\\test.py\", line 37, in test_eq\n",
      "    test(a,b,equals, '==')\n",
      "  File \"D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\fastcore\\test.py\", line 27, in test\n",
      "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
      "AssertionError: Exception occured in `Recorder` when calling event `after_batch`:\n",
      "\t==:\n",
      "11008\n",
      "256\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2756.27s of the 4552.81s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 9.73 GB, but only 8.107 GB is available...\n",
      "\tNot enough memory to train LightGBMXT_BAG_L1... Skipping this model.\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2750.04s of the 4546.58s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 9.73 GB, but only 8.459 GB is available...\n",
      "\tNot enough memory to train LightGBM_BAG_L1... Skipping this model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 2743.95s of the 4540.48s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\RandomForestGini_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestGini_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 198 due to low memory. Expected memory usage reduced from 22.7% -> 15.0% of available memory...\n",
      "\t14.31s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonLoan/\\models\\RandomForestGini_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonLoan/\\models\\RandomForestGini_BAG_L1\\model.pkl\n",
      "\t-0.6842\t = Validation score   (-mean_absolute_error)\n",
      "\t222.71s\t = Training   runtime\n",
      "\t26.85s\t = Validation runtime\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 2486.28s of the 4282.82s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\RandomForestEntr_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestEntr_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 212 due to low memory. Expected memory usage reduced from 21.19% -> 15.0% of available memory...\n",
      "\t14.53s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonLoan/\\models\\RandomForestEntr_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonLoan/\\models\\RandomForestEntr_BAG_L1\\model.pkl\n",
      "\t-0.6842\t = Validation score   (-mean_absolute_error)\n",
      "\t376.2s\t = Training   runtime\n",
      "\t28.01s\t = Validation runtime\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2075.09s of the 3871.62s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\CatBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Not enough memory to safely train CatBoost model, roughly requires: 9.73 GB, but only 8.769 GB is available...\n",
      "\tNot enough memory to train CatBoost_BAG_L1... Skipping this model.\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 2068.98s of the 3865.51s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\ExtraTreesGini_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\ExtraTreesGini_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 154 due to low memory. Expected memory usage reduced from 29.11% -> 15.0% of available memory...\n",
      "\t11.76s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonLoan/\\models\\ExtraTreesGini_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonLoan/\\models\\ExtraTreesGini_BAG_L1\\model.pkl\n",
      "\t-0.6844\t = Validation score   (-mean_absolute_error)\n",
      "\t25.67s\t = Training   runtime\n",
      "\t19.96s\t = Validation runtime\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 2017.22s of the 3813.75s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\ExtraTreesEntr_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\ExtraTreesEntr_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 151 due to low memory. Expected memory usage reduced from 29.66% -> 15.0% of available memory...\n",
      "\t12.01s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonLoan/\\models\\ExtraTreesEntr_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonLoan/\\models\\ExtraTreesEntr_BAG_L1\\model.pkl\n",
      "\t-0.6842\t = Validation score   (-mean_absolute_error)\n",
      "\t19.36s\t = Training   runtime\n",
      "\t19.78s\t = Validation runtime\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1972.19s of the 3768.73s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\XGBoost_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit XGBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Not enough memory to safely train XGBoost model, roughly requires: 9.73 GB, but only 8.951 GB is available...\n",
      "\tNot enough memory to train XGBoost_BAG_L1... Skipping this model.\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1966.31s of the 3762.85s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\NeuralNetTorch_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\NeuralNetTorch_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit TabularNeuralNetTorchModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f91\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f211\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f441\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f212\",\n",
      "        \"f213\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f349\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f468\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f609\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f638\",\n",
      "        \"f639\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f647\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92145 examples, 759 features (740 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(102, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(22, 9)\n",
      "    (16): Embedding(25, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1115, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.6367, Val mean_absolute_error: -0.6822, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5257, Val mean_absolute_error: -0.6822, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5163, Val mean_absolute_error: -0.6822, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.5099, Val mean_absolute_error: -0.6822, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.5044, Val mean_absolute_error: -0.6822, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.5003, Val mean_absolute_error: -0.6822, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.4962, Val mean_absolute_error: -0.6822, Best Epoch: 7\n",
      "Epoch 8 (Update 5752).\tTrain loss: 0.491, Val mean_absolute_error: -0.6822, Best Epoch: 8\n",
      "Epoch 9 (Update 6471).\tTrain loss: 0.4867, Val mean_absolute_error: -0.6822, Best Epoch: 9\n",
      "Epoch 10 (Update 7190).\tTrain loss: 0.484, Val mean_absolute_error: -0.6822, Best Epoch: 10\n",
      "Epoch 11 (Update 7909).\tTrain loss: 0.4794, Val mean_absolute_error: -0.6822, Best Epoch: 11\n",
      "Epoch 12 (Update 8628).\tTrain loss: 0.4763, Val mean_absolute_error: -0.6822, Best Epoch: 12\n",
      "Epoch 13 (Update 9347).\tTrain loss: 0.4725, Val mean_absolute_error: -0.6822, Best Epoch: 13\n",
      "Epoch 14 (Update 10066).\tTrain loss: 0.4685, Val mean_absolute_error: -0.6822, Best Epoch: 14\n",
      "Epoch 15 (Update 10785).\tTrain loss: 0.4651, Val mean_absolute_error: -0.6822, Best Epoch: 15\n",
      "Epoch 16 (Update 11504).\tTrain loss: 0.4616, Val mean_absolute_error: -0.6822, Best Epoch: 16\n",
      "Epoch 17 (Update 12223).\tTrain loss: 0.4567, Val mean_absolute_error: -0.6822, Best Epoch: 17\n",
      "Epoch 18 (Update 12942).\tTrain loss: 0.4549, Val mean_absolute_error: -0.6822, Best Epoch: 18\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 18)\n",
      "Best model found on Epoch 18 (Update 12942). Val mean_absolute_error: -0.6821634761470677\n",
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f91\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f211\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f321\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f647\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f212\",\n",
      "        \"f213\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f349\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f441\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f468\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f609\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f638\",\n",
      "        \"f639\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f678\",\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92145 examples, 760 features (741 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(101, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(21, 8)\n",
      "    (16): Embedding(24, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1115, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.6296, Val mean_absolute_error: -0.6867, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5255, Val mean_absolute_error: -0.6867, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.517, Val mean_absolute_error: -0.6867, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.5095, Val mean_absolute_error: -0.6867, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.5041, Val mean_absolute_error: -0.6867, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.5007, Val mean_absolute_error: -0.6867, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.4948, Val mean_absolute_error: -0.6867, Best Epoch: 7\n",
      "Epoch 8 (Update 5752).\tTrain loss: 0.4911, Val mean_absolute_error: -0.6867, Best Epoch: 8\n",
      "Epoch 9 (Update 6471).\tTrain loss: 0.4876, Val mean_absolute_error: -0.6867, Best Epoch: 9\n",
      "Epoch 10 (Update 7190).\tTrain loss: 0.484, Val mean_absolute_error: -0.6867, Best Epoch: 10\n",
      "Epoch 11 (Update 7909).\tTrain loss: 0.4793, Val mean_absolute_error: -0.6867, Best Epoch: 11\n",
      "Epoch 12 (Update 8628).\tTrain loss: 0.4758, Val mean_absolute_error: -0.6867, Best Epoch: 12\n",
      "Epoch 13 (Update 9347).\tTrain loss: 0.4729, Val mean_absolute_error: -0.6867, Best Epoch: 13\n",
      "Epoch 14 (Update 10066).\tTrain loss: 0.4689, Val mean_absolute_error: -0.6867, Best Epoch: 14\n",
      "Epoch 15 (Update 10785).\tTrain loss: 0.4648, Val mean_absolute_error: -0.6867, Best Epoch: 15\n",
      "Epoch 16 (Update 11504).\tTrain loss: 0.4599, Val mean_absolute_error: -0.6867, Best Epoch: 16\n",
      "Epoch 17 (Update 12223).\tTrain loss: 0.4554, Val mean_absolute_error: -0.6867, Best Epoch: 17\n",
      "Epoch 18 (Update 12942).\tTrain loss: 0.4518, Val mean_absolute_error: -0.6867, Best Epoch: 18\n",
      "Epoch 19 (Update 13661).\tTrain loss: 0.4488, Val mean_absolute_error: -0.6867, Best Epoch: 19\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 19)\n",
      "Best model found on Epoch 19 (Update 13661). Val mean_absolute_error: -0.6867213612883623\n",
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f91\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f211\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f609\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f638\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f647\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f212\",\n",
      "        \"f213\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f349\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f441\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f468\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f639\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f678\",\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92145 examples, 760 features (741 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(102, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(22, 9)\n",
      "    (16): Embedding(25, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1116, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.6375, Val mean_absolute_error: -0.6869, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5253, Val mean_absolute_error: -0.6869, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5159, Val mean_absolute_error: -0.6869, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.5089, Val mean_absolute_error: -0.6869, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.5035, Val mean_absolute_error: -0.6869, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.5001, Val mean_absolute_error: -0.6869, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.4949, Val mean_absolute_error: -0.6869, Best Epoch: 7\n",
      "Epoch 8 (Update 5752).\tTrain loss: 0.493, Val mean_absolute_error: -0.6869, Best Epoch: 8\n",
      "Epoch 9 (Update 6471).\tTrain loss: 0.4871, Val mean_absolute_error: -0.6869, Best Epoch: 9\n",
      "Epoch 10 (Update 7190).\tTrain loss: 0.4841, Val mean_absolute_error: -0.6869, Best Epoch: 10\n",
      "Epoch 11 (Update 7909).\tTrain loss: 0.48, Val mean_absolute_error: -0.6869, Best Epoch: 11\n",
      "Epoch 12 (Update 8628).\tTrain loss: 0.4769, Val mean_absolute_error: -0.6869, Best Epoch: 12\n",
      "Epoch 13 (Update 9347).\tTrain loss: 0.4729, Val mean_absolute_error: -0.6869, Best Epoch: 13\n",
      "Epoch 14 (Update 10066).\tTrain loss: 0.4679, Val mean_absolute_error: -0.6869, Best Epoch: 14\n",
      "Epoch 15 (Update 10785).\tTrain loss: 0.4653, Val mean_absolute_error: -0.6869, Best Epoch: 15\n",
      "Epoch 16 (Update 11504).\tTrain loss: 0.4619, Val mean_absolute_error: -0.6869, Best Epoch: 16\n",
      "Epoch 17 (Update 12223).\tTrain loss: 0.458, Val mean_absolute_error: -0.6869, Best Epoch: 17\n",
      "Epoch 18 (Update 12942).\tTrain loss: 0.4536, Val mean_absolute_error: -0.6869, Best Epoch: 18\n",
      "Epoch 19 (Update 13661).\tTrain loss: 0.4504, Val mean_absolute_error: -0.6869, Best Epoch: 19\n",
      "Epoch 20 (Update 14380).\tTrain loss: 0.4464, Val mean_absolute_error: -0.6869, Best Epoch: 20\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 20)\n",
      "Best model found on Epoch 20 (Update 14380). Val mean_absolute_error: -0.686949255545427\n",
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f91\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f609\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f211\",\n",
      "        \"f212\",\n",
      "        \"f213\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f349\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f441\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f468\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f638\",\n",
      "        \"f639\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f647\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f678\",\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92145 examples, 760 features (741 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(102, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(21, 8)\n",
      "    (16): Embedding(24, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1115, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.6283, Val mean_absolute_error: -0.6867, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5246, Val mean_absolute_error: -0.6867, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5147, Val mean_absolute_error: -0.6867, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.5081, Val mean_absolute_error: -0.6867, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.5048, Val mean_absolute_error: -0.6867, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.4992, Val mean_absolute_error: -0.6867, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.4933, Val mean_absolute_error: -0.6867, Best Epoch: 7\n",
      "Epoch 8 (Update 5752).\tTrain loss: 0.49, Val mean_absolute_error: -0.6867, Best Epoch: 8\n",
      "Epoch 9 (Update 6471).\tTrain loss: 0.4849, Val mean_absolute_error: -0.6867, Best Epoch: 9\n",
      "Epoch 10 (Update 7190).\tTrain loss: 0.4813, Val mean_absolute_error: -0.6867, Best Epoch: 10\n",
      "Epoch 11 (Update 7909).\tTrain loss: 0.4775, Val mean_absolute_error: -0.6867, Best Epoch: 11\n",
      "Epoch 12 (Update 8628).\tTrain loss: 0.4742, Val mean_absolute_error: -0.6867, Best Epoch: 12\n",
      "Epoch 13 (Update 9347).\tTrain loss: 0.4692, Val mean_absolute_error: -0.6867, Best Epoch: 13\n",
      "Epoch 14 (Update 10066).\tTrain loss: 0.4657, Val mean_absolute_error: -0.6867, Best Epoch: 14\n",
      "Epoch 15 (Update 10785).\tTrain loss: 0.4615, Val mean_absolute_error: -0.6867, Best Epoch: 15\n",
      "Epoch 16 (Update 11504).\tTrain loss: 0.4581, Val mean_absolute_error: -0.6867, Best Epoch: 16\n",
      "Epoch 17 (Update 12223).\tTrain loss: 0.4546, Val mean_absolute_error: -0.6867, Best Epoch: 17\n",
      "Epoch 18 (Update 12942).\tTrain loss: 0.4501, Val mean_absolute_error: -0.6867, Best Epoch: 18\n",
      "Epoch 19 (Update 13661).\tTrain loss: 0.4451, Val mean_absolute_error: -0.6867, Best Epoch: 19\n",
      "Epoch 20 (Update 14380).\tTrain loss: 0.4405, Val mean_absolute_error: -0.6867, Best Epoch: 20\n",
      "Epoch 21 (Update 15099).\tTrain loss: 0.4361, Val mean_absolute_error: -0.6867, Best Epoch: 21\n",
      "Best model found on Epoch 21 (Update 15099). Val mean_absolute_error: -0.6867213612883623\n",
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f349\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f638\",\n",
      "        \"f639\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f647\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f91\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f211\",\n",
      "        \"f212\",\n",
      "        \"f213\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f441\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f468\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f609\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f678\",\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92145 examples, 760 features (741 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(102, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(22, 9)\n",
      "    (16): Embedding(25, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1116, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.6371, Val mean_absolute_error: -0.6836, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5245, Val mean_absolute_error: -0.6836, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5147, Val mean_absolute_error: -0.6836, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.5082, Val mean_absolute_error: -0.6836, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.5043, Val mean_absolute_error: -0.6836, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.4998, Val mean_absolute_error: -0.6836, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.4954, Val mean_absolute_error: -0.6836, Best Epoch: 7\n",
      "Epoch 8 (Update 5752).\tTrain loss: 0.4923, Val mean_absolute_error: -0.6836, Best Epoch: 8\n",
      "Epoch 9 (Update 6471).\tTrain loss: 0.4886, Val mean_absolute_error: -0.6836, Best Epoch: 9\n",
      "Epoch 10 (Update 7190).\tTrain loss: 0.4858, Val mean_absolute_error: -0.6836, Best Epoch: 10\n",
      "Epoch 11 (Update 7909).\tTrain loss: 0.4823, Val mean_absolute_error: -0.6836, Best Epoch: 11\n",
      "Epoch 12 (Update 8628).\tTrain loss: 0.4783, Val mean_absolute_error: -0.6836, Best Epoch: 12\n",
      "Epoch 13 (Update 9347).\tTrain loss: 0.476, Val mean_absolute_error: -0.6836, Best Epoch: 13\n",
      "Epoch 14 (Update 10066).\tTrain loss: 0.4722, Val mean_absolute_error: -0.6836, Best Epoch: 14\n",
      "Epoch 15 (Update 10785).\tTrain loss: 0.4679, Val mean_absolute_error: -0.6836, Best Epoch: 15\n",
      "Epoch 16 (Update 11504).\tTrain loss: 0.4652, Val mean_absolute_error: -0.6836, Best Epoch: 16\n",
      "Epoch 17 (Update 12223).\tTrain loss: 0.4611, Val mean_absolute_error: -0.6836, Best Epoch: 17\n",
      "Epoch 18 (Update 12942).\tTrain loss: 0.4572, Val mean_absolute_error: -0.6836, Best Epoch: 18\n",
      "Epoch 19 (Update 13661).\tTrain loss: 0.4533, Val mean_absolute_error: -0.6836, Best Epoch: 19\n",
      "Epoch 20 (Update 14380).\tTrain loss: 0.4496, Val mean_absolute_error: -0.6836, Best Epoch: 20\n",
      "Epoch 21 (Update 15099).\tTrain loss: 0.4459, Val mean_absolute_error: -0.6836, Best Epoch: 21\n",
      "Best model found on Epoch 21 (Update 15099). Val mean_absolute_error: -0.683606806441811\n",
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f211\",\n",
      "        \"f212\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f349\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f468\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f609\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f638\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f647\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f91\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f213\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f441\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f639\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f678\",\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92146 examples, 760 features (741 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(102, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(22, 9)\n",
      "    (16): Embedding(25, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1116, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.6384, Val mean_absolute_error: -0.6814, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5257, Val mean_absolute_error: -0.6814, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5164, Val mean_absolute_error: -0.6814, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.509, Val mean_absolute_error: -0.6814, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.5045, Val mean_absolute_error: -0.6814, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.4999, Val mean_absolute_error: -0.6814, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.496, Val mean_absolute_error: -0.6814, Best Epoch: 7\n",
      "Epoch 8 (Update 5752).\tTrain loss: 0.4939, Val mean_absolute_error: -0.6814, Best Epoch: 8\n",
      "Epoch 9 (Update 6471).\tTrain loss: 0.4892, Val mean_absolute_error: -0.6814, Best Epoch: 9\n",
      "Epoch 10 (Update 7190).\tTrain loss: 0.4859, Val mean_absolute_error: -0.6814, Best Epoch: 10\n",
      "Epoch 11 (Update 7909).\tTrain loss: 0.4825, Val mean_absolute_error: -0.6814, Best Epoch: 11\n",
      "Epoch 12 (Update 8628).\tTrain loss: 0.4779, Val mean_absolute_error: -0.6814, Best Epoch: 12\n",
      "Epoch 13 (Update 9347).\tTrain loss: 0.4743, Val mean_absolute_error: -0.6814, Best Epoch: 13\n",
      "Epoch 14 (Update 10066).\tTrain loss: 0.4697, Val mean_absolute_error: -0.6814, Best Epoch: 14\n",
      "Epoch 15 (Update 10785).\tTrain loss: 0.467, Val mean_absolute_error: -0.6814, Best Epoch: 15\n",
      "Epoch 16 (Update 11504).\tTrain loss: 0.4628, Val mean_absolute_error: -0.6814, Best Epoch: 16\n",
      "Epoch 17 (Update 12223).\tTrain loss: 0.458, Val mean_absolute_error: -0.6814, Best Epoch: 17\n",
      "Epoch 18 (Update 12942).\tTrain loss: 0.4549, Val mean_absolute_error: -0.6814, Best Epoch: 18\n",
      "Epoch 19 (Update 13661).\tTrain loss: 0.451, Val mean_absolute_error: -0.6814, Best Epoch: 19\n",
      "Epoch 20 (Update 14380).\tTrain loss: 0.4461, Val mean_absolute_error: -0.6814, Best Epoch: 20\n",
      "Epoch 21 (Update 15099).\tTrain loss: 0.4434, Val mean_absolute_error: -0.6814, Best Epoch: 21\n",
      "Best model found on Epoch 21 (Update 15099). Val mean_absolute_error: -0.6813796247056142\n",
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f91\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f638\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f647\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f211\",\n",
      "        \"f212\",\n",
      "        \"f213\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f349\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f441\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f468\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f609\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f639\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f678\",\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92146 examples, 760 features (741 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(102, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(22, 9)\n",
      "    (16): Embedding(25, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1116, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.6348, Val mean_absolute_error: -0.6835, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5235, Val mean_absolute_error: -0.6835, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5154, Val mean_absolute_error: -0.6835, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.5101, Val mean_absolute_error: -0.6835, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.5041, Val mean_absolute_error: -0.6835, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.4989, Val mean_absolute_error: -0.6835, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.4961, Val mean_absolute_error: -0.6835, Best Epoch: 7\n",
      "Epoch 8 (Update 5752).\tTrain loss: 0.491, Val mean_absolute_error: -0.6835, Best Epoch: 8\n",
      "Epoch 9 (Update 6471).\tTrain loss: 0.4885, Val mean_absolute_error: -0.6835, Best Epoch: 9\n",
      "Epoch 10 (Update 7190).\tTrain loss: 0.4841, Val mean_absolute_error: -0.6835, Best Epoch: 10\n",
      "Epoch 11 (Update 7909).\tTrain loss: 0.4814, Val mean_absolute_error: -0.6835, Best Epoch: 11\n",
      "Epoch 12 (Update 8628).\tTrain loss: 0.4766, Val mean_absolute_error: -0.6835, Best Epoch: 12\n",
      "Epoch 13 (Update 9347).\tTrain loss: 0.473, Val mean_absolute_error: -0.6835, Best Epoch: 13\n",
      "Epoch 14 (Update 10066).\tTrain loss: 0.47, Val mean_absolute_error: -0.6835, Best Epoch: 14\n",
      "Epoch 15 (Update 10785).\tTrain loss: 0.4662, Val mean_absolute_error: -0.6835, Best Epoch: 15\n",
      "Epoch 16 (Update 11504).\tTrain loss: 0.4617, Val mean_absolute_error: -0.6835, Best Epoch: 16\n",
      "Epoch 17 (Update 12223).\tTrain loss: 0.4591, Val mean_absolute_error: -0.6835, Best Epoch: 17\n",
      "Epoch 18 (Update 12942).\tTrain loss: 0.4558, Val mean_absolute_error: -0.6835, Best Epoch: 18\n",
      "Epoch 19 (Update 13661).\tTrain loss: 0.4517, Val mean_absolute_error: -0.6835, Best Epoch: 19\n",
      "Epoch 20 (Update 14380).\tTrain loss: 0.4486, Val mean_absolute_error: -0.6835, Best Epoch: 20\n",
      "Epoch 21 (Update 15099).\tTrain loss: 0.4441, Val mean_absolute_error: -0.6835, Best Epoch: 21\n",
      "Best model found on Epoch 21 (Update 15099). Val mean_absolute_error: -0.6835067993618476\n",
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f91\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f213\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f349\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f638\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f647\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f211\",\n",
      "        \"f212\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f441\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f468\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f609\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f639\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f678\",\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92146 examples, 760 features (741 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(102, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(22, 9)\n",
      "    (16): Embedding(25, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1116, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.6328, Val mean_absolute_error: -0.6828, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5252, Val mean_absolute_error: -0.6828, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5149, Val mean_absolute_error: -0.6828, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.5087, Val mean_absolute_error: -0.6828, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.504, Val mean_absolute_error: -0.6828, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.4987, Val mean_absolute_error: -0.6828, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.4947, Val mean_absolute_error: -0.6828, Best Epoch: 7\n",
      "Epoch 8 (Update 5752).\tTrain loss: 0.4921, Val mean_absolute_error: -0.6828, Best Epoch: 8\n",
      "Epoch 9 (Update 6471).\tTrain loss: 0.4882, Val mean_absolute_error: -0.6828, Best Epoch: 9\n",
      "Epoch 10 (Update 7190).\tTrain loss: 0.4839, Val mean_absolute_error: -0.6828, Best Epoch: 10\n",
      "Epoch 11 (Update 7909).\tTrain loss: 0.4814, Val mean_absolute_error: -0.6828, Best Epoch: 11\n",
      "Epoch 12 (Update 8628).\tTrain loss: 0.4778, Val mean_absolute_error: -0.6828, Best Epoch: 12\n",
      "Epoch 13 (Update 9347).\tTrain loss: 0.4746, Val mean_absolute_error: -0.6828, Best Epoch: 13\n",
      "Epoch 14 (Update 10066).\tTrain loss: 0.4696, Val mean_absolute_error: -0.6828, Best Epoch: 14\n",
      "Epoch 15 (Update 10785).\tTrain loss: 0.4672, Val mean_absolute_error: -0.6828, Best Epoch: 15\n",
      "Epoch 16 (Update 11504).\tTrain loss: 0.4648, Val mean_absolute_error: -0.6828, Best Epoch: 16\n",
      "Epoch 17 (Update 12223).\tTrain loss: 0.4599, Val mean_absolute_error: -0.6828, Best Epoch: 17\n",
      "Epoch 18 (Update 12942).\tTrain loss: 0.456, Val mean_absolute_error: -0.6828, Best Epoch: 18\n",
      "Epoch 19 (Update 13661).\tTrain loss: 0.4534, Val mean_absolute_error: -0.6828, Best Epoch: 19\n",
      "Epoch 20 (Update 14380).\tTrain loss: 0.4483, Val mean_absolute_error: -0.6828, Best Epoch: 20\n",
      "Epoch 21 (Update 15099).\tTrain loss: 0.4441, Val mean_absolute_error: -0.6828, Best Epoch: 21\n",
      "Best model found on Epoch 21 (Update 15099). Val mean_absolute_error: -0.6828230646509155\n",
      "Saving AutoGluonLoan/\\models\\NeuralNetTorch_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonLoan/\\models\\NeuralNetTorch_BAG_L1\\model.pkl\n",
      "\t-0.6842\t = Validation score   (-mean_absolute_error)\n",
      "\t1672.11s\t = Training   runtime\n",
      "\t18.26s\t = Validation runtime\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 271.32s of the 2067.86s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\LightGBMLarge_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 9.73 GB, but only 9.182 GB is available...\n",
      "\tNot enough memory to train LightGBMLarge_BAG_L1... Skipping this model.\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Not enough time left to finish repeated k-fold bagging, stopping early ...\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Loading: AutoGluonLoan/\\models\\KNeighborsUnif_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\KNeighborsDist_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestGini_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestEntr_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\ExtraTreesGini_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\ExtraTreesEntr_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\NeuralNetTorch_BAG_L1\\utils\\oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 2060.99s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\WeightedEnsemble_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\WeightedEnsemble_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "Ensemble size: 1\n",
      "Ensemble weights: \n",
      "[0. 0. 1. 0. 0. 0. 0.]\n",
      "\t0.21s\t= Estimated out-of-fold prediction time...\n",
      "Saving AutoGluonLoan/\\models\\WeightedEnsemble_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonLoan/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "\t-0.6842\t = Validation score   (-mean_absolute_error)\n",
      "\t55.57s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tNeuralNetFastAI_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 95}}\n",
      "\tLightGBMXT_BAG_L2: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tLightGBM_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tRandomForestGini_BAG_L2: \t{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tRandomForestEntr_BAG_L2: \t{'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
      "\tExtraTreesGini_BAG_L2: \t{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tExtraTreesEntr_BAG_L2: \t{'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tXGBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
      "\tNeuralNetTorch_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
      "\tLightGBMLarge_BAG_L2: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting 11 L2 models ...\n",
      "Loading: AutoGluonLoan/\\models\\KNeighborsUnif_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\KNeighborsDist_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestGini_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestEntr_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\ExtraTreesGini_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\ExtraTreesEntr_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\NeuralNetTorch_BAG_L1\\utils\\oof.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 2005.28s of the 2004.44s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\NeuralNetFastAI_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\NeuralNetFastAI_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit NNFastAiTabularModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 6.591 GB, but only 8.25 GB is available...\n",
      "Fitting Neural Network with parameters {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0}...\n",
      "Using 17/19 categorical features\n",
      "Using 1042 cont features\n",
      "Automated batch size selection: 256\n",
      "TabularModel(\n",
      "  (embeds): ModuleList(\n",
      "    (0): Embedding(3738, 160)\n",
      "    (1): Embedding(9099, 264)\n",
      "    (2): Embedding(8604, 256)\n",
      "    (3): Embedding(3451, 153)\n",
      "    (4): Embedding(6458, 218)\n",
      "    (5): Embedding(104, 22)\n",
      "    (6): Embedding(107, 22)\n",
      "    (7): Embedding(6764, 223)\n",
      "    (8): Embedding(7267, 233)\n",
      "    (9): Embedding(4544, 179)\n",
      "    (10): Embedding(1192, 84)\n",
      "    (11): Embedding(4098, 169)\n",
      "    (12): Embedding(730, 64)\n",
      "    (13): Embedding(21, 9)\n",
      "    (14): Embedding(24, 9)\n",
      "    (15): Embedding(966, 75)\n",
      "    (16): Embedding(1036, 78)\n",
      "  )\n",
      "  (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(1042, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): LinBnDrop(\n",
      "      (0): Linear(in_features=3260, out_features=200, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): LinBnDrop(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): LinBnDrop(\n",
      "      (0): Linear(in_features=100, out_features=43, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L2.\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1936.17s of the 1935.53s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 11.699 GB, but only 8.124 GB is available...\n",
      "\tNot enough memory to train LightGBMXT_BAG_L2... Skipping this model.\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1927.83s of the 1927.23s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 11.699 GB, but only 7.969 GB is available...\n",
      "\tNot enough memory to train LightGBM_BAG_L2... Skipping this model.\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 1919.22s of the 1918.61s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\RandomForestGini_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestGini_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 196 due to low memory. Expected memory usage reduced from 22.86% -> 15.0% of available memory...\n",
      "\t15.59s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonLoan/\\models\\RandomForestGini_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonLoan/\\models\\RandomForestGini_BAG_L2\\model.pkl\n",
      "\t-0.6842\t = Validation score   (-mean_absolute_error)\n",
      "\t258.37s\t = Training   runtime\n",
      "\t38.31s\t = Validation runtime\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 1613.65s of the 1613.07s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\RandomForestEntr_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestEntr_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 218 due to low memory. Expected memory usage reduced from 20.62% -> 15.0% of available memory...\n",
      "\t16.63s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonLoan/\\models\\RandomForestEntr_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonLoan/\\models\\RandomForestEntr_BAG_L2\\model.pkl\n",
      "\t-0.6842\t = Validation score   (-mean_absolute_error)\n",
      "\t341.42s\t = Training   runtime\n",
      "\t36.89s\t = Validation runtime\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 1226.88s of the 1226.31s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\CatBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\CatBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Not enough memory to safely train CatBoost model, roughly requires: 11.699 GB, but only 8.319 GB is available...\n",
      "\tNot enough memory to train CatBoost_BAG_L2... Skipping this model.\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 1219.46s of the 1218.89s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\ExtraTreesGini_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\ExtraTreesGini_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 159 due to low memory. Expected memory usage reduced from 28.17% -> 15.0% of available memory...\n",
      "\t12.5s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonLoan/\\models\\ExtraTreesGini_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonLoan/\\models\\ExtraTreesGini_BAG_L2\\model.pkl\n",
      "\t-0.6842\t = Validation score   (-mean_absolute_error)\n",
      "\t36.18s\t = Training   runtime\n",
      "\t27.68s\t = Validation runtime\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 1148.82s of the 1148.25s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\ExtraTreesEntr_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\ExtraTreesEntr_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 156 due to low memory. Expected memory usage reduced from 28.71% -> 15.0% of available memory...\n",
      "\t12.41s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonLoan/\\models\\ExtraTreesEntr_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonLoan/\\models\\ExtraTreesEntr_BAG_L2\\model.pkl\n",
      "\t-0.6842\t = Validation score   (-mean_absolute_error)\n",
      "\t23.21s\t = Training   runtime\n",
      "\t26.95s\t = Validation runtime\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 1092.44s of the 1091.87s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\XGBoost_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit XGBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Not enough memory to safely train XGBoost model, roughly requires: 11.699 GB, but only 8.532 GB is available...\n",
      "\tNot enough memory to train XGBoost_BAG_L2... Skipping this model.\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1085.15s of the 1084.58s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\NeuralNetTorch_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\NeuralNetTorch_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit TabularNeuralNetTorchModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"RandomForestEntr_BAG_L1_0\",\n",
      "        \"ExtraTreesGini_BAG_L1_0\",\n",
      "        \"ExtraTreesEntr_BAG_L1_0\",\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f91\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f211\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f349\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f435\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f441\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f468\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f647\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"KNeighborsUnif_BAG_L1_0\",\n",
      "        \"KNeighborsUnif_BAG_L1_1\",\n",
      "        \"KNeighborsUnif_BAG_L1_2\",\n",
      "        \"KNeighborsUnif_BAG_L1_3\",\n",
      "        \"KNeighborsUnif_BAG_L1_4\",\n",
      "        \"KNeighborsUnif_BAG_L1_5\",\n",
      "        \"KNeighborsUnif_BAG_L1_6\",\n",
      "        \"KNeighborsUnif_BAG_L1_7\",\n",
      "        \"KNeighborsUnif_BAG_L1_8\",\n",
      "        \"KNeighborsUnif_BAG_L1_9\",\n",
      "        \"KNeighborsUnif_BAG_L1_10\",\n",
      "        \"KNeighborsUnif_BAG_L1_11\",\n",
      "        \"KNeighborsUnif_BAG_L1_12\",\n",
      "        \"KNeighborsUnif_BAG_L1_13\",\n",
      "        \"KNeighborsUnif_BAG_L1_14\",\n",
      "        \"KNeighborsUnif_BAG_L1_15\",\n",
      "        \"KNeighborsUnif_BAG_L1_16\",\n",
      "        \"KNeighborsDist_BAG_L1_0\",\n",
      "        \"KNeighborsDist_BAG_L1_1\",\n",
      "        \"KNeighborsDist_BAG_L1_2\",\n",
      "        \"KNeighborsDist_BAG_L1_3\",\n",
      "        \"KNeighborsDist_BAG_L1_4\",\n",
      "        \"KNeighborsDist_BAG_L1_5\",\n",
      "        \"KNeighborsDist_BAG_L1_6\",\n",
      "        \"KNeighborsDist_BAG_L1_7\",\n",
      "        \"KNeighborsDist_BAG_L1_8\",\n",
      "        \"KNeighborsDist_BAG_L1_9\",\n",
      "        \"KNeighborsDist_BAG_L1_10\",\n",
      "        \"KNeighborsDist_BAG_L1_11\",\n",
      "        \"KNeighborsDist_BAG_L1_12\",\n",
      "        \"KNeighborsDist_BAG_L1_13\",\n",
      "        \"KNeighborsDist_BAG_L1_14\",\n",
      "        \"KNeighborsDist_BAG_L1_15\",\n",
      "        \"KNeighborsDist_BAG_L1_16\",\n",
      "        \"KNeighborsDist_BAG_L1_17\",\n",
      "        \"KNeighborsDist_BAG_L1_18\",\n",
      "        \"KNeighborsDist_BAG_L1_19\",\n",
      "        \"KNeighborsDist_BAG_L1_20\",\n",
      "        \"KNeighborsDist_BAG_L1_21\",\n",
      "        \"KNeighborsDist_BAG_L1_22\",\n",
      "        \"KNeighborsDist_BAG_L1_23\",\n",
      "        \"KNeighborsDist_BAG_L1_24\",\n",
      "        \"KNeighborsDist_BAG_L1_25\",\n",
      "        \"KNeighborsDist_BAG_L1_26\",\n",
      "        \"KNeighborsDist_BAG_L1_27\",\n",
      "        \"KNeighborsDist_BAG_L1_28\",\n",
      "        \"KNeighborsDist_BAG_L1_29\",\n",
      "        \"KNeighborsDist_BAG_L1_30\",\n",
      "        \"KNeighborsDist_BAG_L1_31\",\n",
      "        \"KNeighborsDist_BAG_L1_32\",\n",
      "        \"KNeighborsDist_BAG_L1_33\",\n",
      "        \"KNeighborsDist_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_35\",\n",
      "        \"KNeighborsDist_BAG_L1_36\",\n",
      "        \"KNeighborsDist_BAG_L1_37\",\n",
      "        \"KNeighborsDist_BAG_L1_38\",\n",
      "        \"KNeighborsDist_BAG_L1_39\",\n",
      "        \"KNeighborsDist_BAG_L1_40\",\n",
      "        \"KNeighborsDist_BAG_L1_41\",\n",
      "        \"KNeighborsDist_BAG_L1_42\",\n",
      "        \"RandomForestGini_BAG_L1_0\",\n",
      "        \"RandomForestGini_BAG_L1_1\",\n",
      "        \"RandomForestGini_BAG_L1_2\",\n",
      "        \"RandomForestGini_BAG_L1_3\",\n",
      "        \"RandomForestGini_BAG_L1_4\",\n",
      "        \"RandomForestGini_BAG_L1_5\",\n",
      "        \"RandomForestGini_BAG_L1_6\",\n",
      "        \"RandomForestGini_BAG_L1_7\",\n",
      "        \"RandomForestGini_BAG_L1_8\",\n",
      "        \"RandomForestGini_BAG_L1_9\",\n",
      "        \"RandomForestGini_BAG_L1_10\",\n",
      "        \"RandomForestGini_BAG_L1_11\",\n",
      "        \"RandomForestGini_BAG_L1_12\",\n",
      "        \"RandomForestGini_BAG_L1_13\",\n",
      "        \"RandomForestGini_BAG_L1_14\",\n",
      "        \"RandomForestGini_BAG_L1_15\",\n",
      "        \"RandomForestGini_BAG_L1_16\",\n",
      "        \"RandomForestGini_BAG_L1_17\",\n",
      "        \"RandomForestGini_BAG_L1_18\",\n",
      "        \"RandomForestGini_BAG_L1_19\",\n",
      "        \"RandomForestGini_BAG_L1_20\",\n",
      "        \"RandomForestGini_BAG_L1_21\",\n",
      "        \"RandomForestGini_BAG_L1_22\",\n",
      "        \"RandomForestGini_BAG_L1_23\",\n",
      "        \"RandomForestGini_BAG_L1_24\",\n",
      "        \"RandomForestGini_BAG_L1_25\",\n",
      "        \"RandomForestGini_BAG_L1_26\",\n",
      "        \"RandomForestGini_BAG_L1_27\",\n",
      "        \"RandomForestGini_BAG_L1_28\",\n",
      "        \"RandomForestGini_BAG_L1_29\",\n",
      "        \"RandomForestGini_BAG_L1_30\",\n",
      "        \"RandomForestGini_BAG_L1_31\",\n",
      "        \"RandomForestGini_BAG_L1_32\",\n",
      "        \"RandomForestGini_BAG_L1_33\",\n",
      "        \"RandomForestGini_BAG_L1_34\",\n",
      "        \"RandomForestGini_BAG_L1_35\",\n",
      "        \"RandomForestGini_BAG_L1_36\",\n",
      "        \"RandomForestGini_BAG_L1_37\",\n",
      "        \"RandomForestGini_BAG_L1_38\",\n",
      "        \"RandomForestGini_BAG_L1_39\",\n",
      "        \"RandomForestGini_BAG_L1_40\",\n",
      "        \"RandomForestGini_BAG_L1_41\",\n",
      "        \"RandomForestGini_BAG_L1_42\",\n",
      "        \"RandomForestEntr_BAG_L1_1\",\n",
      "        \"RandomForestEntr_BAG_L1_2\",\n",
      "        \"RandomForestEntr_BAG_L1_3\",\n",
      "        \"RandomForestEntr_BAG_L1_4\",\n",
      "        \"RandomForestEntr_BAG_L1_5\",\n",
      "        \"RandomForestEntr_BAG_L1_6\",\n",
      "        \"RandomForestEntr_BAG_L1_7\",\n",
      "        \"RandomForestEntr_BAG_L1_8\",\n",
      "        \"RandomForestEntr_BAG_L1_9\",\n",
      "        \"RandomForestEntr_BAG_L1_10\",\n",
      "        \"RandomForestEntr_BAG_L1_11\",\n",
      "        \"RandomForestEntr_BAG_L1_12\",\n",
      "        \"RandomForestEntr_BAG_L1_13\",\n",
      "        \"RandomForestEntr_BAG_L1_14\",\n",
      "        \"RandomForestEntr_BAG_L1_15\",\n",
      "        \"RandomForestEntr_BAG_L1_16\",\n",
      "        \"RandomForestEntr_BAG_L1_17\",\n",
      "        \"RandomForestEntr_BAG_L1_18\",\n",
      "        \"RandomForestEntr_BAG_L1_19\",\n",
      "        \"RandomForestEntr_BAG_L1_20\",\n",
      "        \"RandomForestEntr_BAG_L1_21\",\n",
      "        \"RandomForestEntr_BAG_L1_22\",\n",
      "        \"RandomForestEntr_BAG_L1_23\",\n",
      "        \"RandomForestEntr_BAG_L1_24\",\n",
      "        \"RandomForestEntr_BAG_L1_25\",\n",
      "        \"RandomForestEntr_BAG_L1_26\",\n",
      "        \"RandomForestEntr_BAG_L1_27\",\n",
      "        \"RandomForestEntr_BAG_L1_28\",\n",
      "        \"RandomForestEntr_BAG_L1_29\",\n",
      "        \"RandomForestEntr_BAG_L1_30\",\n",
      "        \"RandomForestEntr_BAG_L1_31\",\n",
      "        \"RandomForestEntr_BAG_L1_32\",\n",
      "        \"RandomForestEntr_BAG_L1_33\",\n",
      "        \"RandomForestEntr_BAG_L1_34\",\n",
      "        \"RandomForestEntr_BAG_L1_35\",\n",
      "        \"RandomForestEntr_BAG_L1_36\",\n",
      "        \"RandomForestEntr_BAG_L1_37\",\n",
      "        \"RandomForestEntr_BAG_L1_38\",\n",
      "        \"RandomForestEntr_BAG_L1_39\",\n",
      "        \"RandomForestEntr_BAG_L1_40\",\n",
      "        \"RandomForestEntr_BAG_L1_41\",\n",
      "        \"RandomForestEntr_BAG_L1_42\",\n",
      "        \"ExtraTreesGini_BAG_L1_1\",\n",
      "        \"ExtraTreesGini_BAG_L1_2\",\n",
      "        \"ExtraTreesGini_BAG_L1_3\",\n",
      "        \"ExtraTreesGini_BAG_L1_4\",\n",
      "        \"ExtraTreesGini_BAG_L1_5\",\n",
      "        \"ExtraTreesGini_BAG_L1_6\",\n",
      "        \"ExtraTreesGini_BAG_L1_7\",\n",
      "        \"ExtraTreesGini_BAG_L1_8\",\n",
      "        \"ExtraTreesGini_BAG_L1_9\",\n",
      "        \"ExtraTreesGini_BAG_L1_10\",\n",
      "        \"ExtraTreesGini_BAG_L1_11\",\n",
      "        \"ExtraTreesGini_BAG_L1_12\",\n",
      "        \"ExtraTreesGini_BAG_L1_13\",\n",
      "        \"ExtraTreesGini_BAG_L1_14\",\n",
      "        \"ExtraTreesGini_BAG_L1_15\",\n",
      "        \"ExtraTreesGini_BAG_L1_16\",\n",
      "        \"ExtraTreesGini_BAG_L1_17\",\n",
      "        \"ExtraTreesGini_BAG_L1_18\",\n",
      "        \"ExtraTreesGini_BAG_L1_19\",\n",
      "        \"ExtraTreesGini_BAG_L1_20\",\n",
      "        \"ExtraTreesGini_BAG_L1_21\",\n",
      "        \"ExtraTreesGini_BAG_L1_22\",\n",
      "        \"ExtraTreesGini_BAG_L1_23\",\n",
      "        \"ExtraTreesGini_BAG_L1_24\",\n",
      "        \"ExtraTreesGini_BAG_L1_25\",\n",
      "        \"ExtraTreesGini_BAG_L1_26\",\n",
      "        \"ExtraTreesGini_BAG_L1_27\",\n",
      "        \"ExtraTreesGini_BAG_L1_28\",\n",
      "        \"ExtraTreesGini_BAG_L1_29\",\n",
      "        \"ExtraTreesGini_BAG_L1_30\",\n",
      "        \"ExtraTreesGini_BAG_L1_31\",\n",
      "        \"ExtraTreesGini_BAG_L1_32\",\n",
      "        \"ExtraTreesGini_BAG_L1_33\",\n",
      "        \"ExtraTreesGini_BAG_L1_34\",\n",
      "        \"ExtraTreesGini_BAG_L1_35\",\n",
      "        \"ExtraTreesGini_BAG_L1_36\",\n",
      "        \"ExtraTreesGini_BAG_L1_37\",\n",
      "        \"ExtraTreesGini_BAG_L1_38\",\n",
      "        \"ExtraTreesGini_BAG_L1_39\",\n",
      "        \"ExtraTreesGini_BAG_L1_40\",\n",
      "        \"ExtraTreesGini_BAG_L1_41\",\n",
      "        \"ExtraTreesGini_BAG_L1_42\",\n",
      "        \"ExtraTreesEntr_BAG_L1_1\",\n",
      "        \"ExtraTreesEntr_BAG_L1_2\",\n",
      "        \"ExtraTreesEntr_BAG_L1_3\",\n",
      "        \"ExtraTreesEntr_BAG_L1_4\",\n",
      "        \"ExtraTreesEntr_BAG_L1_5\",\n",
      "        \"ExtraTreesEntr_BAG_L1_6\",\n",
      "        \"ExtraTreesEntr_BAG_L1_7\",\n",
      "        \"ExtraTreesEntr_BAG_L1_8\",\n",
      "        \"ExtraTreesEntr_BAG_L1_9\",\n",
      "        \"ExtraTreesEntr_BAG_L1_10\",\n",
      "        \"ExtraTreesEntr_BAG_L1_11\",\n",
      "        \"ExtraTreesEntr_BAG_L1_12\",\n",
      "        \"ExtraTreesEntr_BAG_L1_13\",\n",
      "        \"ExtraTreesEntr_BAG_L1_14\",\n",
      "        \"ExtraTreesEntr_BAG_L1_15\",\n",
      "        \"ExtraTreesEntr_BAG_L1_16\",\n",
      "        \"ExtraTreesEntr_BAG_L1_17\",\n",
      "        \"ExtraTreesEntr_BAG_L1_18\",\n",
      "        \"ExtraTreesEntr_BAG_L1_19\",\n",
      "        \"ExtraTreesEntr_BAG_L1_20\",\n",
      "        \"ExtraTreesEntr_BAG_L1_21\",\n",
      "        \"ExtraTreesEntr_BAG_L1_22\",\n",
      "        \"ExtraTreesEntr_BAG_L1_23\",\n",
      "        \"ExtraTreesEntr_BAG_L1_24\",\n",
      "        \"ExtraTreesEntr_BAG_L1_25\",\n",
      "        \"ExtraTreesEntr_BAG_L1_26\",\n",
      "        \"ExtraTreesEntr_BAG_L1_27\",\n",
      "        \"ExtraTreesEntr_BAG_L1_28\",\n",
      "        \"ExtraTreesEntr_BAG_L1_29\",\n",
      "        \"ExtraTreesEntr_BAG_L1_30\",\n",
      "        \"ExtraTreesEntr_BAG_L1_31\",\n",
      "        \"ExtraTreesEntr_BAG_L1_32\",\n",
      "        \"ExtraTreesEntr_BAG_L1_33\",\n",
      "        \"ExtraTreesEntr_BAG_L1_34\",\n",
      "        \"ExtraTreesEntr_BAG_L1_35\",\n",
      "        \"ExtraTreesEntr_BAG_L1_36\",\n",
      "        \"ExtraTreesEntr_BAG_L1_37\",\n",
      "        \"ExtraTreesEntr_BAG_L1_38\",\n",
      "        \"ExtraTreesEntr_BAG_L1_39\",\n",
      "        \"ExtraTreesEntr_BAG_L1_40\",\n",
      "        \"ExtraTreesEntr_BAG_L1_41\",\n",
      "        \"ExtraTreesEntr_BAG_L1_42\",\n",
      "        \"NeuralNetTorch_BAG_L1_0\",\n",
      "        \"NeuralNetTorch_BAG_L1_1\",\n",
      "        \"NeuralNetTorch_BAG_L1_2\",\n",
      "        \"NeuralNetTorch_BAG_L1_3\",\n",
      "        \"NeuralNetTorch_BAG_L1_4\",\n",
      "        \"NeuralNetTorch_BAG_L1_5\",\n",
      "        \"NeuralNetTorch_BAG_L1_6\",\n",
      "        \"NeuralNetTorch_BAG_L1_7\",\n",
      "        \"NeuralNetTorch_BAG_L1_8\",\n",
      "        \"NeuralNetTorch_BAG_L1_9\",\n",
      "        \"NeuralNetTorch_BAG_L1_10\",\n",
      "        \"NeuralNetTorch_BAG_L1_11\",\n",
      "        \"NeuralNetTorch_BAG_L1_12\",\n",
      "        \"NeuralNetTorch_BAG_L1_13\",\n",
      "        \"NeuralNetTorch_BAG_L1_14\",\n",
      "        \"NeuralNetTorch_BAG_L1_15\",\n",
      "        \"NeuralNetTorch_BAG_L1_16\",\n",
      "        \"NeuralNetTorch_BAG_L1_17\",\n",
      "        \"NeuralNetTorch_BAG_L1_18\",\n",
      "        \"NeuralNetTorch_BAG_L1_19\",\n",
      "        \"NeuralNetTorch_BAG_L1_20\",\n",
      "        \"NeuralNetTorch_BAG_L1_21\",\n",
      "        \"NeuralNetTorch_BAG_L1_22\",\n",
      "        \"NeuralNetTorch_BAG_L1_23\",\n",
      "        \"NeuralNetTorch_BAG_L1_24\",\n",
      "        \"NeuralNetTorch_BAG_L1_25\",\n",
      "        \"NeuralNetTorch_BAG_L1_26\",\n",
      "        \"NeuralNetTorch_BAG_L1_27\",\n",
      "        \"NeuralNetTorch_BAG_L1_28\",\n",
      "        \"NeuralNetTorch_BAG_L1_29\",\n",
      "        \"NeuralNetTorch_BAG_L1_30\",\n",
      "        \"NeuralNetTorch_BAG_L1_31\",\n",
      "        \"NeuralNetTorch_BAG_L1_32\",\n",
      "        \"NeuralNetTorch_BAG_L1_33\",\n",
      "        \"NeuralNetTorch_BAG_L1_34\",\n",
      "        \"NeuralNetTorch_BAG_L1_35\",\n",
      "        \"NeuralNetTorch_BAG_L1_36\",\n",
      "        \"NeuralNetTorch_BAG_L1_37\",\n",
      "        \"NeuralNetTorch_BAG_L1_38\",\n",
      "        \"NeuralNetTorch_BAG_L1_39\",\n",
      "        \"NeuralNetTorch_BAG_L1_40\",\n",
      "        \"NeuralNetTorch_BAG_L1_41\",\n",
      "        \"NeuralNetTorch_BAG_L1_42\",\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f212\",\n",
      "        \"f213\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f436\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f609\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f638\",\n",
      "        \"f639\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [\n",
      "        \"KNeighborsUnif_BAG_L1_17\",\n",
      "        \"KNeighborsUnif_BAG_L1_18\",\n",
      "        \"KNeighborsUnif_BAG_L1_19\",\n",
      "        \"KNeighborsUnif_BAG_L1_20\",\n",
      "        \"KNeighborsUnif_BAG_L1_21\",\n",
      "        \"KNeighborsUnif_BAG_L1_22\",\n",
      "        \"KNeighborsUnif_BAG_L1_23\",\n",
      "        \"KNeighborsUnif_BAG_L1_24\",\n",
      "        \"KNeighborsUnif_BAG_L1_25\",\n",
      "        \"KNeighborsUnif_BAG_L1_26\",\n",
      "        \"KNeighborsUnif_BAG_L1_27\",\n",
      "        \"KNeighborsUnif_BAG_L1_28\",\n",
      "        \"KNeighborsUnif_BAG_L1_29\",\n",
      "        \"KNeighborsUnif_BAG_L1_30\",\n",
      "        \"KNeighborsUnif_BAG_L1_31\",\n",
      "        \"KNeighborsUnif_BAG_L1_32\",\n",
      "        \"KNeighborsUnif_BAG_L1_33\",\n",
      "        \"KNeighborsUnif_BAG_L1_34\",\n",
      "        \"KNeighborsUnif_BAG_L1_35\",\n",
      "        \"KNeighborsUnif_BAG_L1_36\",\n",
      "        \"KNeighborsUnif_BAG_L1_37\",\n",
      "        \"KNeighborsUnif_BAG_L1_38\",\n",
      "        \"KNeighborsUnif_BAG_L1_39\",\n",
      "        \"KNeighborsUnif_BAG_L1_40\",\n",
      "        \"KNeighborsUnif_BAG_L1_41\",\n",
      "        \"KNeighborsUnif_BAG_L1_42\"\n",
      "    ],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f678\",\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92145 examples, 1061 features (1042 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(102, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(22, 9)\n",
      "    (16): Embedding(25, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1443, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.6057, Val mean_absolute_error: -0.6822, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5165, Val mean_absolute_error: -0.6822, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5103, Val mean_absolute_error: -0.6822, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.5085, Val mean_absolute_error: -0.6822, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.5049, Val mean_absolute_error: -0.6822, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.5036, Val mean_absolute_error: -0.6822, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.5008, Val mean_absolute_error: -0.6822, Best Epoch: 7\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "Best model found on Epoch 7 (Update 5033). Val mean_absolute_error: -0.6821634761470677\n",
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"RandomForestEntr_BAG_L1_0\",\n",
      "        \"ExtraTreesGini_BAG_L1_0\",\n",
      "        \"ExtraTreesEntr_BAG_L1_0\",\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f91\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f211\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f349\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f639\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f647\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"KNeighborsUnif_BAG_L1_0\",\n",
      "        \"KNeighborsUnif_BAG_L1_1\",\n",
      "        \"KNeighborsUnif_BAG_L1_2\",\n",
      "        \"KNeighborsUnif_BAG_L1_3\",\n",
      "        \"KNeighborsUnif_BAG_L1_4\",\n",
      "        \"KNeighborsUnif_BAG_L1_5\",\n",
      "        \"KNeighborsUnif_BAG_L1_6\",\n",
      "        \"KNeighborsUnif_BAG_L1_7\",\n",
      "        \"KNeighborsUnif_BAG_L1_8\",\n",
      "        \"KNeighborsUnif_BAG_L1_9\",\n",
      "        \"KNeighborsUnif_BAG_L1_10\",\n",
      "        \"KNeighborsUnif_BAG_L1_11\",\n",
      "        \"KNeighborsUnif_BAG_L1_12\",\n",
      "        \"KNeighborsUnif_BAG_L1_13\",\n",
      "        \"KNeighborsUnif_BAG_L1_14\",\n",
      "        \"KNeighborsUnif_BAG_L1_15\",\n",
      "        \"KNeighborsUnif_BAG_L1_16\",\n",
      "        \"KNeighborsUnif_BAG_L1_17\",\n",
      "        \"KNeighborsUnif_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_0\",\n",
      "        \"KNeighborsDist_BAG_L1_1\",\n",
      "        \"KNeighborsDist_BAG_L1_2\",\n",
      "        \"KNeighborsDist_BAG_L1_3\",\n",
      "        \"KNeighborsDist_BAG_L1_4\",\n",
      "        \"KNeighborsDist_BAG_L1_5\",\n",
      "        \"KNeighborsDist_BAG_L1_6\",\n",
      "        \"KNeighborsDist_BAG_L1_7\",\n",
      "        \"KNeighborsDist_BAG_L1_8\",\n",
      "        \"KNeighborsDist_BAG_L1_9\",\n",
      "        \"KNeighborsDist_BAG_L1_10\",\n",
      "        \"KNeighborsDist_BAG_L1_11\",\n",
      "        \"KNeighborsDist_BAG_L1_12\",\n",
      "        \"KNeighborsDist_BAG_L1_13\",\n",
      "        \"KNeighborsDist_BAG_L1_14\",\n",
      "        \"KNeighborsDist_BAG_L1_15\",\n",
      "        \"KNeighborsDist_BAG_L1_16\",\n",
      "        \"KNeighborsDist_BAG_L1_17\",\n",
      "        \"KNeighborsDist_BAG_L1_18\",\n",
      "        \"KNeighborsDist_BAG_L1_19\",\n",
      "        \"KNeighborsDist_BAG_L1_20\",\n",
      "        \"KNeighborsDist_BAG_L1_21\",\n",
      "        \"KNeighborsDist_BAG_L1_22\",\n",
      "        \"KNeighborsDist_BAG_L1_23\",\n",
      "        \"KNeighborsDist_BAG_L1_24\",\n",
      "        \"KNeighborsDist_BAG_L1_25\",\n",
      "        \"KNeighborsDist_BAG_L1_26\",\n",
      "        \"KNeighborsDist_BAG_L1_27\",\n",
      "        \"KNeighborsDist_BAG_L1_28\",\n",
      "        \"KNeighborsDist_BAG_L1_29\",\n",
      "        \"KNeighborsDist_BAG_L1_30\",\n",
      "        \"KNeighborsDist_BAG_L1_31\",\n",
      "        \"KNeighborsDist_BAG_L1_32\",\n",
      "        \"KNeighborsDist_BAG_L1_33\",\n",
      "        \"KNeighborsDist_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_35\",\n",
      "        \"KNeighborsDist_BAG_L1_36\",\n",
      "        \"KNeighborsDist_BAG_L1_37\",\n",
      "        \"KNeighborsDist_BAG_L1_38\",\n",
      "        \"KNeighborsDist_BAG_L1_39\",\n",
      "        \"KNeighborsDist_BAG_L1_40\",\n",
      "        \"KNeighborsDist_BAG_L1_41\",\n",
      "        \"KNeighborsDist_BAG_L1_42\",\n",
      "        \"RandomForestGini_BAG_L1_0\",\n",
      "        \"RandomForestGini_BAG_L1_1\",\n",
      "        \"RandomForestGini_BAG_L1_2\",\n",
      "        \"RandomForestGini_BAG_L1_3\",\n",
      "        \"RandomForestGini_BAG_L1_4\",\n",
      "        \"RandomForestGini_BAG_L1_5\",\n",
      "        \"RandomForestGini_BAG_L1_6\",\n",
      "        \"RandomForestGini_BAG_L1_7\",\n",
      "        \"RandomForestGini_BAG_L1_8\",\n",
      "        \"RandomForestGini_BAG_L1_9\",\n",
      "        \"RandomForestGini_BAG_L1_10\",\n",
      "        \"RandomForestGini_BAG_L1_11\",\n",
      "        \"RandomForestGini_BAG_L1_12\",\n",
      "        \"RandomForestGini_BAG_L1_13\",\n",
      "        \"RandomForestGini_BAG_L1_14\",\n",
      "        \"RandomForestGini_BAG_L1_15\",\n",
      "        \"RandomForestGini_BAG_L1_16\",\n",
      "        \"RandomForestGini_BAG_L1_17\",\n",
      "        \"RandomForestGini_BAG_L1_18\",\n",
      "        \"RandomForestGini_BAG_L1_19\",\n",
      "        \"RandomForestGini_BAG_L1_20\",\n",
      "        \"RandomForestGini_BAG_L1_21\",\n",
      "        \"RandomForestGini_BAG_L1_22\",\n",
      "        \"RandomForestGini_BAG_L1_23\",\n",
      "        \"RandomForestGini_BAG_L1_24\",\n",
      "        \"RandomForestGini_BAG_L1_25\",\n",
      "        \"RandomForestGini_BAG_L1_26\",\n",
      "        \"RandomForestGini_BAG_L1_27\",\n",
      "        \"RandomForestGini_BAG_L1_28\",\n",
      "        \"RandomForestGini_BAG_L1_29\",\n",
      "        \"RandomForestGini_BAG_L1_30\",\n",
      "        \"RandomForestGini_BAG_L1_31\",\n",
      "        \"RandomForestGini_BAG_L1_32\",\n",
      "        \"RandomForestGini_BAG_L1_33\",\n",
      "        \"RandomForestGini_BAG_L1_34\",\n",
      "        \"RandomForestGini_BAG_L1_35\",\n",
      "        \"RandomForestGini_BAG_L1_36\",\n",
      "        \"RandomForestGini_BAG_L1_37\",\n",
      "        \"RandomForestGini_BAG_L1_38\",\n",
      "        \"RandomForestGini_BAG_L1_39\",\n",
      "        \"RandomForestGini_BAG_L1_40\",\n",
      "        \"RandomForestGini_BAG_L1_41\",\n",
      "        \"RandomForestGini_BAG_L1_42\",\n",
      "        \"RandomForestEntr_BAG_L1_1\",\n",
      "        \"RandomForestEntr_BAG_L1_2\",\n",
      "        \"RandomForestEntr_BAG_L1_3\",\n",
      "        \"RandomForestEntr_BAG_L1_4\",\n",
      "        \"RandomForestEntr_BAG_L1_5\",\n",
      "        \"RandomForestEntr_BAG_L1_6\",\n",
      "        \"RandomForestEntr_BAG_L1_7\",\n",
      "        \"RandomForestEntr_BAG_L1_8\",\n",
      "        \"RandomForestEntr_BAG_L1_9\",\n",
      "        \"RandomForestEntr_BAG_L1_10\",\n",
      "        \"RandomForestEntr_BAG_L1_11\",\n",
      "        \"RandomForestEntr_BAG_L1_12\",\n",
      "        \"RandomForestEntr_BAG_L1_13\",\n",
      "        \"RandomForestEntr_BAG_L1_14\",\n",
      "        \"RandomForestEntr_BAG_L1_15\",\n",
      "        \"RandomForestEntr_BAG_L1_16\",\n",
      "        \"RandomForestEntr_BAG_L1_17\",\n",
      "        \"RandomForestEntr_BAG_L1_18\",\n",
      "        \"RandomForestEntr_BAG_L1_19\",\n",
      "        \"RandomForestEntr_BAG_L1_20\",\n",
      "        \"RandomForestEntr_BAG_L1_21\",\n",
      "        \"RandomForestEntr_BAG_L1_22\",\n",
      "        \"RandomForestEntr_BAG_L1_23\",\n",
      "        \"RandomForestEntr_BAG_L1_24\",\n",
      "        \"RandomForestEntr_BAG_L1_25\",\n",
      "        \"RandomForestEntr_BAG_L1_26\",\n",
      "        \"RandomForestEntr_BAG_L1_27\",\n",
      "        \"RandomForestEntr_BAG_L1_28\",\n",
      "        \"RandomForestEntr_BAG_L1_29\",\n",
      "        \"RandomForestEntr_BAG_L1_30\",\n",
      "        \"RandomForestEntr_BAG_L1_31\",\n",
      "        \"RandomForestEntr_BAG_L1_32\",\n",
      "        \"RandomForestEntr_BAG_L1_33\",\n",
      "        \"RandomForestEntr_BAG_L1_34\",\n",
      "        \"RandomForestEntr_BAG_L1_35\",\n",
      "        \"RandomForestEntr_BAG_L1_36\",\n",
      "        \"RandomForestEntr_BAG_L1_37\",\n",
      "        \"RandomForestEntr_BAG_L1_38\",\n",
      "        \"RandomForestEntr_BAG_L1_39\",\n",
      "        \"RandomForestEntr_BAG_L1_40\",\n",
      "        \"RandomForestEntr_BAG_L1_41\",\n",
      "        \"RandomForestEntr_BAG_L1_42\",\n",
      "        \"ExtraTreesGini_BAG_L1_1\",\n",
      "        \"ExtraTreesGini_BAG_L1_2\",\n",
      "        \"ExtraTreesGini_BAG_L1_3\",\n",
      "        \"ExtraTreesGini_BAG_L1_4\",\n",
      "        \"ExtraTreesGini_BAG_L1_5\",\n",
      "        \"ExtraTreesGini_BAG_L1_6\",\n",
      "        \"ExtraTreesGini_BAG_L1_7\",\n",
      "        \"ExtraTreesGini_BAG_L1_8\",\n",
      "        \"ExtraTreesGini_BAG_L1_9\",\n",
      "        \"ExtraTreesGini_BAG_L1_10\",\n",
      "        \"ExtraTreesGini_BAG_L1_11\",\n",
      "        \"ExtraTreesGini_BAG_L1_12\",\n",
      "        \"ExtraTreesGini_BAG_L1_13\",\n",
      "        \"ExtraTreesGini_BAG_L1_14\",\n",
      "        \"ExtraTreesGini_BAG_L1_15\",\n",
      "        \"ExtraTreesGini_BAG_L1_16\",\n",
      "        \"ExtraTreesGini_BAG_L1_17\",\n",
      "        \"ExtraTreesGini_BAG_L1_18\",\n",
      "        \"ExtraTreesGini_BAG_L1_19\",\n",
      "        \"ExtraTreesGini_BAG_L1_20\",\n",
      "        \"ExtraTreesGini_BAG_L1_21\",\n",
      "        \"ExtraTreesGini_BAG_L1_22\",\n",
      "        \"ExtraTreesGini_BAG_L1_23\",\n",
      "        \"ExtraTreesGini_BAG_L1_24\",\n",
      "        \"ExtraTreesGini_BAG_L1_25\",\n",
      "        \"ExtraTreesGini_BAG_L1_26\",\n",
      "        \"ExtraTreesGini_BAG_L1_27\",\n",
      "        \"ExtraTreesGini_BAG_L1_28\",\n",
      "        \"ExtraTreesGini_BAG_L1_29\",\n",
      "        \"ExtraTreesGini_BAG_L1_30\",\n",
      "        \"ExtraTreesGini_BAG_L1_31\",\n",
      "        \"ExtraTreesGini_BAG_L1_32\",\n",
      "        \"ExtraTreesGini_BAG_L1_33\",\n",
      "        \"ExtraTreesGini_BAG_L1_34\",\n",
      "        \"ExtraTreesGini_BAG_L1_35\",\n",
      "        \"ExtraTreesGini_BAG_L1_36\",\n",
      "        \"ExtraTreesGini_BAG_L1_37\",\n",
      "        \"ExtraTreesGini_BAG_L1_38\",\n",
      "        \"ExtraTreesGini_BAG_L1_39\",\n",
      "        \"ExtraTreesGini_BAG_L1_40\",\n",
      "        \"ExtraTreesGini_BAG_L1_41\",\n",
      "        \"ExtraTreesGini_BAG_L1_42\",\n",
      "        \"ExtraTreesEntr_BAG_L1_1\",\n",
      "        \"ExtraTreesEntr_BAG_L1_2\",\n",
      "        \"ExtraTreesEntr_BAG_L1_3\",\n",
      "        \"ExtraTreesEntr_BAG_L1_4\",\n",
      "        \"ExtraTreesEntr_BAG_L1_5\",\n",
      "        \"ExtraTreesEntr_BAG_L1_6\",\n",
      "        \"ExtraTreesEntr_BAG_L1_7\",\n",
      "        \"ExtraTreesEntr_BAG_L1_8\",\n",
      "        \"ExtraTreesEntr_BAG_L1_9\",\n",
      "        \"ExtraTreesEntr_BAG_L1_10\",\n",
      "        \"ExtraTreesEntr_BAG_L1_11\",\n",
      "        \"ExtraTreesEntr_BAG_L1_12\",\n",
      "        \"ExtraTreesEntr_BAG_L1_13\",\n",
      "        \"ExtraTreesEntr_BAG_L1_14\",\n",
      "        \"ExtraTreesEntr_BAG_L1_15\",\n",
      "        \"ExtraTreesEntr_BAG_L1_16\",\n",
      "        \"ExtraTreesEntr_BAG_L1_17\",\n",
      "        \"ExtraTreesEntr_BAG_L1_18\",\n",
      "        \"ExtraTreesEntr_BAG_L1_19\",\n",
      "        \"ExtraTreesEntr_BAG_L1_20\",\n",
      "        \"ExtraTreesEntr_BAG_L1_21\",\n",
      "        \"ExtraTreesEntr_BAG_L1_22\",\n",
      "        \"ExtraTreesEntr_BAG_L1_23\",\n",
      "        \"ExtraTreesEntr_BAG_L1_24\",\n",
      "        \"ExtraTreesEntr_BAG_L1_25\",\n",
      "        \"ExtraTreesEntr_BAG_L1_26\",\n",
      "        \"ExtraTreesEntr_BAG_L1_27\",\n",
      "        \"ExtraTreesEntr_BAG_L1_28\",\n",
      "        \"ExtraTreesEntr_BAG_L1_29\",\n",
      "        \"ExtraTreesEntr_BAG_L1_30\",\n",
      "        \"ExtraTreesEntr_BAG_L1_31\",\n",
      "        \"ExtraTreesEntr_BAG_L1_32\",\n",
      "        \"ExtraTreesEntr_BAG_L1_33\",\n",
      "        \"ExtraTreesEntr_BAG_L1_34\",\n",
      "        \"ExtraTreesEntr_BAG_L1_35\",\n",
      "        \"ExtraTreesEntr_BAG_L1_36\",\n",
      "        \"ExtraTreesEntr_BAG_L1_37\",\n",
      "        \"ExtraTreesEntr_BAG_L1_38\",\n",
      "        \"ExtraTreesEntr_BAG_L1_39\",\n",
      "        \"ExtraTreesEntr_BAG_L1_40\",\n",
      "        \"ExtraTreesEntr_BAG_L1_41\",\n",
      "        \"ExtraTreesEntr_BAG_L1_42\",\n",
      "        \"NeuralNetTorch_BAG_L1_0\",\n",
      "        \"NeuralNetTorch_BAG_L1_1\",\n",
      "        \"NeuralNetTorch_BAG_L1_2\",\n",
      "        \"NeuralNetTorch_BAG_L1_3\",\n",
      "        \"NeuralNetTorch_BAG_L1_4\",\n",
      "        \"NeuralNetTorch_BAG_L1_5\",\n",
      "        \"NeuralNetTorch_BAG_L1_6\",\n",
      "        \"NeuralNetTorch_BAG_L1_7\",\n",
      "        \"NeuralNetTorch_BAG_L1_8\",\n",
      "        \"NeuralNetTorch_BAG_L1_9\",\n",
      "        \"NeuralNetTorch_BAG_L1_10\",\n",
      "        \"NeuralNetTorch_BAG_L1_11\",\n",
      "        \"NeuralNetTorch_BAG_L1_12\",\n",
      "        \"NeuralNetTorch_BAG_L1_13\",\n",
      "        \"NeuralNetTorch_BAG_L1_14\",\n",
      "        \"NeuralNetTorch_BAG_L1_15\",\n",
      "        \"NeuralNetTorch_BAG_L1_16\",\n",
      "        \"NeuralNetTorch_BAG_L1_17\",\n",
      "        \"NeuralNetTorch_BAG_L1_18\",\n",
      "        \"NeuralNetTorch_BAG_L1_19\",\n",
      "        \"NeuralNetTorch_BAG_L1_20\",\n",
      "        \"NeuralNetTorch_BAG_L1_21\",\n",
      "        \"NeuralNetTorch_BAG_L1_22\",\n",
      "        \"NeuralNetTorch_BAG_L1_23\",\n",
      "        \"NeuralNetTorch_BAG_L1_24\",\n",
      "        \"NeuralNetTorch_BAG_L1_25\",\n",
      "        \"NeuralNetTorch_BAG_L1_26\",\n",
      "        \"NeuralNetTorch_BAG_L1_27\",\n",
      "        \"NeuralNetTorch_BAG_L1_28\",\n",
      "        \"NeuralNetTorch_BAG_L1_29\",\n",
      "        \"NeuralNetTorch_BAG_L1_30\",\n",
      "        \"NeuralNetTorch_BAG_L1_31\",\n",
      "        \"NeuralNetTorch_BAG_L1_32\",\n",
      "        \"NeuralNetTorch_BAG_L1_33\",\n",
      "        \"NeuralNetTorch_BAG_L1_34\",\n",
      "        \"NeuralNetTorch_BAG_L1_35\",\n",
      "        \"NeuralNetTorch_BAG_L1_36\",\n",
      "        \"NeuralNetTorch_BAG_L1_37\",\n",
      "        \"NeuralNetTorch_BAG_L1_38\",\n",
      "        \"NeuralNetTorch_BAG_L1_39\",\n",
      "        \"NeuralNetTorch_BAG_L1_40\",\n",
      "        \"NeuralNetTorch_BAG_L1_41\",\n",
      "        \"NeuralNetTorch_BAG_L1_42\",\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f212\",\n",
      "        \"f213\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f441\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f468\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f609\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f638\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [\n",
      "        \"KNeighborsUnif_BAG_L1_18\",\n",
      "        \"KNeighborsUnif_BAG_L1_19\",\n",
      "        \"KNeighborsUnif_BAG_L1_20\",\n",
      "        \"KNeighborsUnif_BAG_L1_21\",\n",
      "        \"KNeighborsUnif_BAG_L1_22\",\n",
      "        \"KNeighborsUnif_BAG_L1_23\",\n",
      "        \"KNeighborsUnif_BAG_L1_24\",\n",
      "        \"KNeighborsUnif_BAG_L1_25\",\n",
      "        \"KNeighborsUnif_BAG_L1_26\",\n",
      "        \"KNeighborsUnif_BAG_L1_27\",\n",
      "        \"KNeighborsUnif_BAG_L1_28\",\n",
      "        \"KNeighborsUnif_BAG_L1_29\",\n",
      "        \"KNeighborsUnif_BAG_L1_30\",\n",
      "        \"KNeighborsUnif_BAG_L1_31\",\n",
      "        \"KNeighborsUnif_BAG_L1_32\",\n",
      "        \"KNeighborsUnif_BAG_L1_33\",\n",
      "        \"KNeighborsUnif_BAG_L1_35\",\n",
      "        \"KNeighborsUnif_BAG_L1_36\",\n",
      "        \"KNeighborsUnif_BAG_L1_37\",\n",
      "        \"KNeighborsUnif_BAG_L1_38\",\n",
      "        \"KNeighborsUnif_BAG_L1_39\",\n",
      "        \"KNeighborsUnif_BAG_L1_40\",\n",
      "        \"KNeighborsUnif_BAG_L1_41\",\n",
      "        \"KNeighborsUnif_BAG_L1_42\"\n",
      "    ],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f678\",\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92145 examples, 1061 features (1042 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(102, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(22, 9)\n",
      "    (16): Embedding(25, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1441, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.5984, Val mean_absolute_error: -0.6867, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5165, Val mean_absolute_error: -0.6867, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5107, Val mean_absolute_error: -0.6867, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.5083, Val mean_absolute_error: -0.6867, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.5045, Val mean_absolute_error: -0.6867, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.502, Val mean_absolute_error: -0.6867, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.5011, Val mean_absolute_error: -0.6867, Best Epoch: 7\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "Best model found on Epoch 7 (Update 5033). Val mean_absolute_error: -0.6867213612883623\n",
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"RandomForestEntr_BAG_L1_0\",\n",
      "        \"ExtraTreesGini_BAG_L1_0\",\n",
      "        \"ExtraTreesEntr_BAG_L1_0\",\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f91\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f349\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f638\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f647\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"KNeighborsUnif_BAG_L1_0\",\n",
      "        \"KNeighborsUnif_BAG_L1_1\",\n",
      "        \"KNeighborsUnif_BAG_L1_2\",\n",
      "        \"KNeighborsUnif_BAG_L1_3\",\n",
      "        \"KNeighborsUnif_BAG_L1_4\",\n",
      "        \"KNeighborsUnif_BAG_L1_5\",\n",
      "        \"KNeighborsUnif_BAG_L1_6\",\n",
      "        \"KNeighborsUnif_BAG_L1_7\",\n",
      "        \"KNeighborsUnif_BAG_L1_8\",\n",
      "        \"KNeighborsUnif_BAG_L1_9\",\n",
      "        \"KNeighborsUnif_BAG_L1_10\",\n",
      "        \"KNeighborsUnif_BAG_L1_11\",\n",
      "        \"KNeighborsUnif_BAG_L1_12\",\n",
      "        \"KNeighborsUnif_BAG_L1_13\",\n",
      "        \"KNeighborsUnif_BAG_L1_14\",\n",
      "        \"KNeighborsUnif_BAG_L1_15\",\n",
      "        \"KNeighborsUnif_BAG_L1_16\",\n",
      "        \"KNeighborsUnif_BAG_L1_17\",\n",
      "        \"KNeighborsUnif_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_0\",\n",
      "        \"KNeighborsDist_BAG_L1_1\",\n",
      "        \"KNeighborsDist_BAG_L1_2\",\n",
      "        \"KNeighborsDist_BAG_L1_3\",\n",
      "        \"KNeighborsDist_BAG_L1_4\",\n",
      "        \"KNeighborsDist_BAG_L1_5\",\n",
      "        \"KNeighborsDist_BAG_L1_6\",\n",
      "        \"KNeighborsDist_BAG_L1_7\",\n",
      "        \"KNeighborsDist_BAG_L1_8\",\n",
      "        \"KNeighborsDist_BAG_L1_9\",\n",
      "        \"KNeighborsDist_BAG_L1_10\",\n",
      "        \"KNeighborsDist_BAG_L1_11\",\n",
      "        \"KNeighborsDist_BAG_L1_12\",\n",
      "        \"KNeighborsDist_BAG_L1_13\",\n",
      "        \"KNeighborsDist_BAG_L1_14\",\n",
      "        \"KNeighborsDist_BAG_L1_15\",\n",
      "        \"KNeighborsDist_BAG_L1_16\",\n",
      "        \"KNeighborsDist_BAG_L1_17\",\n",
      "        \"KNeighborsDist_BAG_L1_18\",\n",
      "        \"KNeighborsDist_BAG_L1_19\",\n",
      "        \"KNeighborsDist_BAG_L1_20\",\n",
      "        \"KNeighborsDist_BAG_L1_21\",\n",
      "        \"KNeighborsDist_BAG_L1_22\",\n",
      "        \"KNeighborsDist_BAG_L1_23\",\n",
      "        \"KNeighborsDist_BAG_L1_24\",\n",
      "        \"KNeighborsDist_BAG_L1_25\",\n",
      "        \"KNeighborsDist_BAG_L1_26\",\n",
      "        \"KNeighborsDist_BAG_L1_27\",\n",
      "        \"KNeighborsDist_BAG_L1_28\",\n",
      "        \"KNeighborsDist_BAG_L1_29\",\n",
      "        \"KNeighborsDist_BAG_L1_30\",\n",
      "        \"KNeighborsDist_BAG_L1_31\",\n",
      "        \"KNeighborsDist_BAG_L1_32\",\n",
      "        \"KNeighborsDist_BAG_L1_33\",\n",
      "        \"KNeighborsDist_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_35\",\n",
      "        \"KNeighborsDist_BAG_L1_36\",\n",
      "        \"KNeighborsDist_BAG_L1_37\",\n",
      "        \"KNeighborsDist_BAG_L1_38\",\n",
      "        \"KNeighborsDist_BAG_L1_39\",\n",
      "        \"KNeighborsDist_BAG_L1_40\",\n",
      "        \"KNeighborsDist_BAG_L1_41\",\n",
      "        \"KNeighborsDist_BAG_L1_42\",\n",
      "        \"RandomForestGini_BAG_L1_0\",\n",
      "        \"RandomForestGini_BAG_L1_1\",\n",
      "        \"RandomForestGini_BAG_L1_2\",\n",
      "        \"RandomForestGini_BAG_L1_3\",\n",
      "        \"RandomForestGini_BAG_L1_4\",\n",
      "        \"RandomForestGini_BAG_L1_5\",\n",
      "        \"RandomForestGini_BAG_L1_6\",\n",
      "        \"RandomForestGini_BAG_L1_7\",\n",
      "        \"RandomForestGini_BAG_L1_8\",\n",
      "        \"RandomForestGini_BAG_L1_9\",\n",
      "        \"RandomForestGini_BAG_L1_10\",\n",
      "        \"RandomForestGini_BAG_L1_11\",\n",
      "        \"RandomForestGini_BAG_L1_12\",\n",
      "        \"RandomForestGini_BAG_L1_13\",\n",
      "        \"RandomForestGini_BAG_L1_14\",\n",
      "        \"RandomForestGini_BAG_L1_15\",\n",
      "        \"RandomForestGini_BAG_L1_16\",\n",
      "        \"RandomForestGini_BAG_L1_17\",\n",
      "        \"RandomForestGini_BAG_L1_18\",\n",
      "        \"RandomForestGini_BAG_L1_19\",\n",
      "        \"RandomForestGini_BAG_L1_20\",\n",
      "        \"RandomForestGini_BAG_L1_21\",\n",
      "        \"RandomForestGini_BAG_L1_22\",\n",
      "        \"RandomForestGini_BAG_L1_23\",\n",
      "        \"RandomForestGini_BAG_L1_24\",\n",
      "        \"RandomForestGini_BAG_L1_25\",\n",
      "        \"RandomForestGini_BAG_L1_26\",\n",
      "        \"RandomForestGini_BAG_L1_27\",\n",
      "        \"RandomForestGini_BAG_L1_28\",\n",
      "        \"RandomForestGini_BAG_L1_29\",\n",
      "        \"RandomForestGini_BAG_L1_30\",\n",
      "        \"RandomForestGini_BAG_L1_31\",\n",
      "        \"RandomForestGini_BAG_L1_32\",\n",
      "        \"RandomForestGini_BAG_L1_33\",\n",
      "        \"RandomForestGini_BAG_L1_34\",\n",
      "        \"RandomForestGini_BAG_L1_35\",\n",
      "        \"RandomForestGini_BAG_L1_36\",\n",
      "        \"RandomForestGini_BAG_L1_37\",\n",
      "        \"RandomForestGini_BAG_L1_38\",\n",
      "        \"RandomForestGini_BAG_L1_39\",\n",
      "        \"RandomForestGini_BAG_L1_40\",\n",
      "        \"RandomForestGini_BAG_L1_41\",\n",
      "        \"RandomForestGini_BAG_L1_42\",\n",
      "        \"RandomForestEntr_BAG_L1_1\",\n",
      "        \"RandomForestEntr_BAG_L1_2\",\n",
      "        \"RandomForestEntr_BAG_L1_3\",\n",
      "        \"RandomForestEntr_BAG_L1_4\",\n",
      "        \"RandomForestEntr_BAG_L1_5\",\n",
      "        \"RandomForestEntr_BAG_L1_6\",\n",
      "        \"RandomForestEntr_BAG_L1_7\",\n",
      "        \"RandomForestEntr_BAG_L1_8\",\n",
      "        \"RandomForestEntr_BAG_L1_9\",\n",
      "        \"RandomForestEntr_BAG_L1_10\",\n",
      "        \"RandomForestEntr_BAG_L1_11\",\n",
      "        \"RandomForestEntr_BAG_L1_12\",\n",
      "        \"RandomForestEntr_BAG_L1_13\",\n",
      "        \"RandomForestEntr_BAG_L1_14\",\n",
      "        \"RandomForestEntr_BAG_L1_15\",\n",
      "        \"RandomForestEntr_BAG_L1_16\",\n",
      "        \"RandomForestEntr_BAG_L1_17\",\n",
      "        \"RandomForestEntr_BAG_L1_18\",\n",
      "        \"RandomForestEntr_BAG_L1_19\",\n",
      "        \"RandomForestEntr_BAG_L1_20\",\n",
      "        \"RandomForestEntr_BAG_L1_21\",\n",
      "        \"RandomForestEntr_BAG_L1_22\",\n",
      "        \"RandomForestEntr_BAG_L1_23\",\n",
      "        \"RandomForestEntr_BAG_L1_24\",\n",
      "        \"RandomForestEntr_BAG_L1_25\",\n",
      "        \"RandomForestEntr_BAG_L1_26\",\n",
      "        \"RandomForestEntr_BAG_L1_27\",\n",
      "        \"RandomForestEntr_BAG_L1_28\",\n",
      "        \"RandomForestEntr_BAG_L1_29\",\n",
      "        \"RandomForestEntr_BAG_L1_30\",\n",
      "        \"RandomForestEntr_BAG_L1_31\",\n",
      "        \"RandomForestEntr_BAG_L1_32\",\n",
      "        \"RandomForestEntr_BAG_L1_33\",\n",
      "        \"RandomForestEntr_BAG_L1_34\",\n",
      "        \"RandomForestEntr_BAG_L1_35\",\n",
      "        \"RandomForestEntr_BAG_L1_36\",\n",
      "        \"RandomForestEntr_BAG_L1_37\",\n",
      "        \"RandomForestEntr_BAG_L1_38\",\n",
      "        \"RandomForestEntr_BAG_L1_39\",\n",
      "        \"RandomForestEntr_BAG_L1_40\",\n",
      "        \"RandomForestEntr_BAG_L1_41\",\n",
      "        \"RandomForestEntr_BAG_L1_42\",\n",
      "        \"ExtraTreesGini_BAG_L1_1\",\n",
      "        \"ExtraTreesGini_BAG_L1_2\",\n",
      "        \"ExtraTreesGini_BAG_L1_3\",\n",
      "        \"ExtraTreesGini_BAG_L1_4\",\n",
      "        \"ExtraTreesGini_BAG_L1_5\",\n",
      "        \"ExtraTreesGini_BAG_L1_6\",\n",
      "        \"ExtraTreesGini_BAG_L1_7\",\n",
      "        \"ExtraTreesGini_BAG_L1_8\",\n",
      "        \"ExtraTreesGini_BAG_L1_9\",\n",
      "        \"ExtraTreesGini_BAG_L1_10\",\n",
      "        \"ExtraTreesGini_BAG_L1_11\",\n",
      "        \"ExtraTreesGini_BAG_L1_12\",\n",
      "        \"ExtraTreesGini_BAG_L1_13\",\n",
      "        \"ExtraTreesGini_BAG_L1_14\",\n",
      "        \"ExtraTreesGini_BAG_L1_15\",\n",
      "        \"ExtraTreesGini_BAG_L1_16\",\n",
      "        \"ExtraTreesGini_BAG_L1_17\",\n",
      "        \"ExtraTreesGini_BAG_L1_18\",\n",
      "        \"ExtraTreesGini_BAG_L1_19\",\n",
      "        \"ExtraTreesGini_BAG_L1_20\",\n",
      "        \"ExtraTreesGini_BAG_L1_21\",\n",
      "        \"ExtraTreesGini_BAG_L1_22\",\n",
      "        \"ExtraTreesGini_BAG_L1_23\",\n",
      "        \"ExtraTreesGini_BAG_L1_24\",\n",
      "        \"ExtraTreesGini_BAG_L1_25\",\n",
      "        \"ExtraTreesGini_BAG_L1_26\",\n",
      "        \"ExtraTreesGini_BAG_L1_27\",\n",
      "        \"ExtraTreesGini_BAG_L1_28\",\n",
      "        \"ExtraTreesGini_BAG_L1_29\",\n",
      "        \"ExtraTreesGini_BAG_L1_30\",\n",
      "        \"ExtraTreesGini_BAG_L1_31\",\n",
      "        \"ExtraTreesGini_BAG_L1_32\",\n",
      "        \"ExtraTreesGini_BAG_L1_33\",\n",
      "        \"ExtraTreesGini_BAG_L1_34\",\n",
      "        \"ExtraTreesGini_BAG_L1_35\",\n",
      "        \"ExtraTreesGini_BAG_L1_36\",\n",
      "        \"ExtraTreesGini_BAG_L1_37\",\n",
      "        \"ExtraTreesGini_BAG_L1_38\",\n",
      "        \"ExtraTreesGini_BAG_L1_39\",\n",
      "        \"ExtraTreesGini_BAG_L1_40\",\n",
      "        \"ExtraTreesGini_BAG_L1_41\",\n",
      "        \"ExtraTreesGini_BAG_L1_42\",\n",
      "        \"ExtraTreesEntr_BAG_L1_1\",\n",
      "        \"ExtraTreesEntr_BAG_L1_2\",\n",
      "        \"ExtraTreesEntr_BAG_L1_3\",\n",
      "        \"ExtraTreesEntr_BAG_L1_4\",\n",
      "        \"ExtraTreesEntr_BAG_L1_5\",\n",
      "        \"ExtraTreesEntr_BAG_L1_6\",\n",
      "        \"ExtraTreesEntr_BAG_L1_7\",\n",
      "        \"ExtraTreesEntr_BAG_L1_8\",\n",
      "        \"ExtraTreesEntr_BAG_L1_9\",\n",
      "        \"ExtraTreesEntr_BAG_L1_10\",\n",
      "        \"ExtraTreesEntr_BAG_L1_11\",\n",
      "        \"ExtraTreesEntr_BAG_L1_12\",\n",
      "        \"ExtraTreesEntr_BAG_L1_13\",\n",
      "        \"ExtraTreesEntr_BAG_L1_14\",\n",
      "        \"ExtraTreesEntr_BAG_L1_15\",\n",
      "        \"ExtraTreesEntr_BAG_L1_16\",\n",
      "        \"ExtraTreesEntr_BAG_L1_17\",\n",
      "        \"ExtraTreesEntr_BAG_L1_18\",\n",
      "        \"ExtraTreesEntr_BAG_L1_19\",\n",
      "        \"ExtraTreesEntr_BAG_L1_20\",\n",
      "        \"ExtraTreesEntr_BAG_L1_21\",\n",
      "        \"ExtraTreesEntr_BAG_L1_22\",\n",
      "        \"ExtraTreesEntr_BAG_L1_23\",\n",
      "        \"ExtraTreesEntr_BAG_L1_24\",\n",
      "        \"ExtraTreesEntr_BAG_L1_25\",\n",
      "        \"ExtraTreesEntr_BAG_L1_26\",\n",
      "        \"ExtraTreesEntr_BAG_L1_27\",\n",
      "        \"ExtraTreesEntr_BAG_L1_28\",\n",
      "        \"ExtraTreesEntr_BAG_L1_29\",\n",
      "        \"ExtraTreesEntr_BAG_L1_30\",\n",
      "        \"ExtraTreesEntr_BAG_L1_31\",\n",
      "        \"ExtraTreesEntr_BAG_L1_32\",\n",
      "        \"ExtraTreesEntr_BAG_L1_33\",\n",
      "        \"ExtraTreesEntr_BAG_L1_34\",\n",
      "        \"ExtraTreesEntr_BAG_L1_35\",\n",
      "        \"ExtraTreesEntr_BAG_L1_36\",\n",
      "        \"ExtraTreesEntr_BAG_L1_37\",\n",
      "        \"ExtraTreesEntr_BAG_L1_38\",\n",
      "        \"ExtraTreesEntr_BAG_L1_39\",\n",
      "        \"ExtraTreesEntr_BAG_L1_40\",\n",
      "        \"ExtraTreesEntr_BAG_L1_41\",\n",
      "        \"ExtraTreesEntr_BAG_L1_42\",\n",
      "        \"NeuralNetTorch_BAG_L1_0\",\n",
      "        \"NeuralNetTorch_BAG_L1_1\",\n",
      "        \"NeuralNetTorch_BAG_L1_2\",\n",
      "        \"NeuralNetTorch_BAG_L1_3\",\n",
      "        \"NeuralNetTorch_BAG_L1_4\",\n",
      "        \"NeuralNetTorch_BAG_L1_5\",\n",
      "        \"NeuralNetTorch_BAG_L1_6\",\n",
      "        \"NeuralNetTorch_BAG_L1_7\",\n",
      "        \"NeuralNetTorch_BAG_L1_8\",\n",
      "        \"NeuralNetTorch_BAG_L1_9\",\n",
      "        \"NeuralNetTorch_BAG_L1_10\",\n",
      "        \"NeuralNetTorch_BAG_L1_11\",\n",
      "        \"NeuralNetTorch_BAG_L1_12\",\n",
      "        \"NeuralNetTorch_BAG_L1_13\",\n",
      "        \"NeuralNetTorch_BAG_L1_14\",\n",
      "        \"NeuralNetTorch_BAG_L1_15\",\n",
      "        \"NeuralNetTorch_BAG_L1_16\",\n",
      "        \"NeuralNetTorch_BAG_L1_17\",\n",
      "        \"NeuralNetTorch_BAG_L1_18\",\n",
      "        \"NeuralNetTorch_BAG_L1_19\",\n",
      "        \"NeuralNetTorch_BAG_L1_20\",\n",
      "        \"NeuralNetTorch_BAG_L1_21\",\n",
      "        \"NeuralNetTorch_BAG_L1_22\",\n",
      "        \"NeuralNetTorch_BAG_L1_23\",\n",
      "        \"NeuralNetTorch_BAG_L1_24\",\n",
      "        \"NeuralNetTorch_BAG_L1_25\",\n",
      "        \"NeuralNetTorch_BAG_L1_26\",\n",
      "        \"NeuralNetTorch_BAG_L1_27\",\n",
      "        \"NeuralNetTorch_BAG_L1_28\",\n",
      "        \"NeuralNetTorch_BAG_L1_29\",\n",
      "        \"NeuralNetTorch_BAG_L1_30\",\n",
      "        \"NeuralNetTorch_BAG_L1_31\",\n",
      "        \"NeuralNetTorch_BAG_L1_32\",\n",
      "        \"NeuralNetTorch_BAG_L1_33\",\n",
      "        \"NeuralNetTorch_BAG_L1_34\",\n",
      "        \"NeuralNetTorch_BAG_L1_35\",\n",
      "        \"NeuralNetTorch_BAG_L1_36\",\n",
      "        \"NeuralNetTorch_BAG_L1_37\",\n",
      "        \"NeuralNetTorch_BAG_L1_38\",\n",
      "        \"NeuralNetTorch_BAG_L1_39\",\n",
      "        \"NeuralNetTorch_BAG_L1_40\",\n",
      "        \"NeuralNetTorch_BAG_L1_41\",\n",
      "        \"NeuralNetTorch_BAG_L1_42\",\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f211\",\n",
      "        \"f212\",\n",
      "        \"f213\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f441\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f468\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f609\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f639\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [\n",
      "        \"KNeighborsUnif_BAG_L1_18\",\n",
      "        \"KNeighborsUnif_BAG_L1_19\",\n",
      "        \"KNeighborsUnif_BAG_L1_20\",\n",
      "        \"KNeighborsUnif_BAG_L1_21\",\n",
      "        \"KNeighborsUnif_BAG_L1_22\",\n",
      "        \"KNeighborsUnif_BAG_L1_23\",\n",
      "        \"KNeighborsUnif_BAG_L1_24\",\n",
      "        \"KNeighborsUnif_BAG_L1_25\",\n",
      "        \"KNeighborsUnif_BAG_L1_26\",\n",
      "        \"KNeighborsUnif_BAG_L1_27\",\n",
      "        \"KNeighborsUnif_BAG_L1_28\",\n",
      "        \"KNeighborsUnif_BAG_L1_29\",\n",
      "        \"KNeighborsUnif_BAG_L1_30\",\n",
      "        \"KNeighborsUnif_BAG_L1_31\",\n",
      "        \"KNeighborsUnif_BAG_L1_32\",\n",
      "        \"KNeighborsUnif_BAG_L1_33\",\n",
      "        \"KNeighborsUnif_BAG_L1_35\",\n",
      "        \"KNeighborsUnif_BAG_L1_36\",\n",
      "        \"KNeighborsUnif_BAG_L1_37\",\n",
      "        \"KNeighborsUnif_BAG_L1_38\",\n",
      "        \"KNeighborsUnif_BAG_L1_39\",\n",
      "        \"KNeighborsUnif_BAG_L1_40\",\n",
      "        \"KNeighborsUnif_BAG_L1_41\",\n",
      "        \"KNeighborsUnif_BAG_L1_42\"\n",
      "    ],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f678\",\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92145 examples, 1061 features (1042 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(101, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(19, 8)\n",
      "    (16): Embedding(22, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1440, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.6047, Val mean_absolute_error: -0.6869, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5162, Val mean_absolute_error: -0.6869, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5108, Val mean_absolute_error: -0.6869, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.5097, Val mean_absolute_error: -0.6869, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.5049, Val mean_absolute_error: -0.6869, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.5031, Val mean_absolute_error: -0.6869, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.5008, Val mean_absolute_error: -0.6869, Best Epoch: 7\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "Best model found on Epoch 7 (Update 5033). Val mean_absolute_error: -0.686949255545427\n",
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"RandomForestEntr_BAG_L1_0\",\n",
      "        \"ExtraTreesGini_BAG_L1_0\",\n",
      "        \"ExtraTreesEntr_BAG_L1_0\",\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f91\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f211\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f349\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f647\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"KNeighborsUnif_BAG_L1_0\",\n",
      "        \"KNeighborsUnif_BAG_L1_1\",\n",
      "        \"KNeighborsUnif_BAG_L1_2\",\n",
      "        \"KNeighborsUnif_BAG_L1_3\",\n",
      "        \"KNeighborsUnif_BAG_L1_4\",\n",
      "        \"KNeighborsUnif_BAG_L1_5\",\n",
      "        \"KNeighborsUnif_BAG_L1_6\",\n",
      "        \"KNeighborsUnif_BAG_L1_7\",\n",
      "        \"KNeighborsUnif_BAG_L1_8\",\n",
      "        \"KNeighborsUnif_BAG_L1_9\",\n",
      "        \"KNeighborsUnif_BAG_L1_10\",\n",
      "        \"KNeighborsUnif_BAG_L1_11\",\n",
      "        \"KNeighborsUnif_BAG_L1_12\",\n",
      "        \"KNeighborsUnif_BAG_L1_13\",\n",
      "        \"KNeighborsUnif_BAG_L1_14\",\n",
      "        \"KNeighborsUnif_BAG_L1_15\",\n",
      "        \"KNeighborsUnif_BAG_L1_16\",\n",
      "        \"KNeighborsUnif_BAG_L1_17\",\n",
      "        \"KNeighborsUnif_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_0\",\n",
      "        \"KNeighborsDist_BAG_L1_1\",\n",
      "        \"KNeighborsDist_BAG_L1_2\",\n",
      "        \"KNeighborsDist_BAG_L1_3\",\n",
      "        \"KNeighborsDist_BAG_L1_4\",\n",
      "        \"KNeighborsDist_BAG_L1_5\",\n",
      "        \"KNeighborsDist_BAG_L1_6\",\n",
      "        \"KNeighborsDist_BAG_L1_7\",\n",
      "        \"KNeighborsDist_BAG_L1_8\",\n",
      "        \"KNeighborsDist_BAG_L1_9\",\n",
      "        \"KNeighborsDist_BAG_L1_10\",\n",
      "        \"KNeighborsDist_BAG_L1_11\",\n",
      "        \"KNeighborsDist_BAG_L1_12\",\n",
      "        \"KNeighborsDist_BAG_L1_13\",\n",
      "        \"KNeighborsDist_BAG_L1_14\",\n",
      "        \"KNeighborsDist_BAG_L1_15\",\n",
      "        \"KNeighborsDist_BAG_L1_16\",\n",
      "        \"KNeighborsDist_BAG_L1_17\",\n",
      "        \"KNeighborsDist_BAG_L1_18\",\n",
      "        \"KNeighborsDist_BAG_L1_19\",\n",
      "        \"KNeighborsDist_BAG_L1_20\",\n",
      "        \"KNeighborsDist_BAG_L1_21\",\n",
      "        \"KNeighborsDist_BAG_L1_22\",\n",
      "        \"KNeighborsDist_BAG_L1_23\",\n",
      "        \"KNeighborsDist_BAG_L1_24\",\n",
      "        \"KNeighborsDist_BAG_L1_25\",\n",
      "        \"KNeighborsDist_BAG_L1_26\",\n",
      "        \"KNeighborsDist_BAG_L1_27\",\n",
      "        \"KNeighborsDist_BAG_L1_28\",\n",
      "        \"KNeighborsDist_BAG_L1_29\",\n",
      "        \"KNeighborsDist_BAG_L1_30\",\n",
      "        \"KNeighborsDist_BAG_L1_31\",\n",
      "        \"KNeighborsDist_BAG_L1_32\",\n",
      "        \"KNeighborsDist_BAG_L1_33\",\n",
      "        \"KNeighborsDist_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_35\",\n",
      "        \"KNeighborsDist_BAG_L1_36\",\n",
      "        \"KNeighborsDist_BAG_L1_37\",\n",
      "        \"KNeighborsDist_BAG_L1_38\",\n",
      "        \"KNeighborsDist_BAG_L1_39\",\n",
      "        \"KNeighborsDist_BAG_L1_40\",\n",
      "        \"KNeighborsDist_BAG_L1_41\",\n",
      "        \"KNeighborsDist_BAG_L1_42\",\n",
      "        \"RandomForestGini_BAG_L1_0\",\n",
      "        \"RandomForestGini_BAG_L1_1\",\n",
      "        \"RandomForestGini_BAG_L1_2\",\n",
      "        \"RandomForestGini_BAG_L1_3\",\n",
      "        \"RandomForestGini_BAG_L1_4\",\n",
      "        \"RandomForestGini_BAG_L1_5\",\n",
      "        \"RandomForestGini_BAG_L1_6\",\n",
      "        \"RandomForestGini_BAG_L1_7\",\n",
      "        \"RandomForestGini_BAG_L1_8\",\n",
      "        \"RandomForestGini_BAG_L1_9\",\n",
      "        \"RandomForestGini_BAG_L1_10\",\n",
      "        \"RandomForestGini_BAG_L1_11\",\n",
      "        \"RandomForestGini_BAG_L1_12\",\n",
      "        \"RandomForestGini_BAG_L1_13\",\n",
      "        \"RandomForestGini_BAG_L1_14\",\n",
      "        \"RandomForestGini_BAG_L1_15\",\n",
      "        \"RandomForestGini_BAG_L1_16\",\n",
      "        \"RandomForestGini_BAG_L1_17\",\n",
      "        \"RandomForestGini_BAG_L1_18\",\n",
      "        \"RandomForestGini_BAG_L1_19\",\n",
      "        \"RandomForestGini_BAG_L1_20\",\n",
      "        \"RandomForestGini_BAG_L1_21\",\n",
      "        \"RandomForestGini_BAG_L1_22\",\n",
      "        \"RandomForestGini_BAG_L1_23\",\n",
      "        \"RandomForestGini_BAG_L1_24\",\n",
      "        \"RandomForestGini_BAG_L1_25\",\n",
      "        \"RandomForestGini_BAG_L1_26\",\n",
      "        \"RandomForestGini_BAG_L1_27\",\n",
      "        \"RandomForestGini_BAG_L1_28\",\n",
      "        \"RandomForestGini_BAG_L1_29\",\n",
      "        \"RandomForestGini_BAG_L1_30\",\n",
      "        \"RandomForestGini_BAG_L1_31\",\n",
      "        \"RandomForestGini_BAG_L1_32\",\n",
      "        \"RandomForestGini_BAG_L1_33\",\n",
      "        \"RandomForestGini_BAG_L1_34\",\n",
      "        \"RandomForestGini_BAG_L1_35\",\n",
      "        \"RandomForestGini_BAG_L1_36\",\n",
      "        \"RandomForestGini_BAG_L1_37\",\n",
      "        \"RandomForestGini_BAG_L1_38\",\n",
      "        \"RandomForestGini_BAG_L1_39\",\n",
      "        \"RandomForestGini_BAG_L1_40\",\n",
      "        \"RandomForestGini_BAG_L1_41\",\n",
      "        \"RandomForestGini_BAG_L1_42\",\n",
      "        \"RandomForestEntr_BAG_L1_1\",\n",
      "        \"RandomForestEntr_BAG_L1_2\",\n",
      "        \"RandomForestEntr_BAG_L1_3\",\n",
      "        \"RandomForestEntr_BAG_L1_4\",\n",
      "        \"RandomForestEntr_BAG_L1_5\",\n",
      "        \"RandomForestEntr_BAG_L1_6\",\n",
      "        \"RandomForestEntr_BAG_L1_7\",\n",
      "        \"RandomForestEntr_BAG_L1_8\",\n",
      "        \"RandomForestEntr_BAG_L1_9\",\n",
      "        \"RandomForestEntr_BAG_L1_10\",\n",
      "        \"RandomForestEntr_BAG_L1_11\",\n",
      "        \"RandomForestEntr_BAG_L1_12\",\n",
      "        \"RandomForestEntr_BAG_L1_13\",\n",
      "        \"RandomForestEntr_BAG_L1_14\",\n",
      "        \"RandomForestEntr_BAG_L1_15\",\n",
      "        \"RandomForestEntr_BAG_L1_16\",\n",
      "        \"RandomForestEntr_BAG_L1_17\",\n",
      "        \"RandomForestEntr_BAG_L1_18\",\n",
      "        \"RandomForestEntr_BAG_L1_19\",\n",
      "        \"RandomForestEntr_BAG_L1_20\",\n",
      "        \"RandomForestEntr_BAG_L1_21\",\n",
      "        \"RandomForestEntr_BAG_L1_22\",\n",
      "        \"RandomForestEntr_BAG_L1_23\",\n",
      "        \"RandomForestEntr_BAG_L1_24\",\n",
      "        \"RandomForestEntr_BAG_L1_25\",\n",
      "        \"RandomForestEntr_BAG_L1_26\",\n",
      "        \"RandomForestEntr_BAG_L1_27\",\n",
      "        \"RandomForestEntr_BAG_L1_28\",\n",
      "        \"RandomForestEntr_BAG_L1_29\",\n",
      "        \"RandomForestEntr_BAG_L1_30\",\n",
      "        \"RandomForestEntr_BAG_L1_31\",\n",
      "        \"RandomForestEntr_BAG_L1_32\",\n",
      "        \"RandomForestEntr_BAG_L1_33\",\n",
      "        \"RandomForestEntr_BAG_L1_34\",\n",
      "        \"RandomForestEntr_BAG_L1_35\",\n",
      "        \"RandomForestEntr_BAG_L1_36\",\n",
      "        \"RandomForestEntr_BAG_L1_37\",\n",
      "        \"RandomForestEntr_BAG_L1_38\",\n",
      "        \"RandomForestEntr_BAG_L1_39\",\n",
      "        \"RandomForestEntr_BAG_L1_40\",\n",
      "        \"RandomForestEntr_BAG_L1_41\",\n",
      "        \"RandomForestEntr_BAG_L1_42\",\n",
      "        \"ExtraTreesGini_BAG_L1_1\",\n",
      "        \"ExtraTreesGini_BAG_L1_2\",\n",
      "        \"ExtraTreesGini_BAG_L1_3\",\n",
      "        \"ExtraTreesGini_BAG_L1_4\",\n",
      "        \"ExtraTreesGini_BAG_L1_5\",\n",
      "        \"ExtraTreesGini_BAG_L1_6\",\n",
      "        \"ExtraTreesGini_BAG_L1_7\",\n",
      "        \"ExtraTreesGini_BAG_L1_8\",\n",
      "        \"ExtraTreesGini_BAG_L1_9\",\n",
      "        \"ExtraTreesGini_BAG_L1_10\",\n",
      "        \"ExtraTreesGini_BAG_L1_11\",\n",
      "        \"ExtraTreesGini_BAG_L1_12\",\n",
      "        \"ExtraTreesGini_BAG_L1_13\",\n",
      "        \"ExtraTreesGini_BAG_L1_14\",\n",
      "        \"ExtraTreesGini_BAG_L1_15\",\n",
      "        \"ExtraTreesGini_BAG_L1_16\",\n",
      "        \"ExtraTreesGini_BAG_L1_17\",\n",
      "        \"ExtraTreesGini_BAG_L1_18\",\n",
      "        \"ExtraTreesGini_BAG_L1_19\",\n",
      "        \"ExtraTreesGini_BAG_L1_20\",\n",
      "        \"ExtraTreesGini_BAG_L1_21\",\n",
      "        \"ExtraTreesGini_BAG_L1_22\",\n",
      "        \"ExtraTreesGini_BAG_L1_23\",\n",
      "        \"ExtraTreesGini_BAG_L1_24\",\n",
      "        \"ExtraTreesGini_BAG_L1_25\",\n",
      "        \"ExtraTreesGini_BAG_L1_26\",\n",
      "        \"ExtraTreesGini_BAG_L1_27\",\n",
      "        \"ExtraTreesGini_BAG_L1_28\",\n",
      "        \"ExtraTreesGini_BAG_L1_29\",\n",
      "        \"ExtraTreesGini_BAG_L1_30\",\n",
      "        \"ExtraTreesGini_BAG_L1_31\",\n",
      "        \"ExtraTreesGini_BAG_L1_32\",\n",
      "        \"ExtraTreesGini_BAG_L1_33\",\n",
      "        \"ExtraTreesGini_BAG_L1_34\",\n",
      "        \"ExtraTreesGini_BAG_L1_35\",\n",
      "        \"ExtraTreesGini_BAG_L1_36\",\n",
      "        \"ExtraTreesGini_BAG_L1_37\",\n",
      "        \"ExtraTreesGini_BAG_L1_38\",\n",
      "        \"ExtraTreesGini_BAG_L1_39\",\n",
      "        \"ExtraTreesGini_BAG_L1_40\",\n",
      "        \"ExtraTreesGini_BAG_L1_41\",\n",
      "        \"ExtraTreesGini_BAG_L1_42\",\n",
      "        \"ExtraTreesEntr_BAG_L1_1\",\n",
      "        \"ExtraTreesEntr_BAG_L1_2\",\n",
      "        \"ExtraTreesEntr_BAG_L1_3\",\n",
      "        \"ExtraTreesEntr_BAG_L1_4\",\n",
      "        \"ExtraTreesEntr_BAG_L1_5\",\n",
      "        \"ExtraTreesEntr_BAG_L1_6\",\n",
      "        \"ExtraTreesEntr_BAG_L1_7\",\n",
      "        \"ExtraTreesEntr_BAG_L1_8\",\n",
      "        \"ExtraTreesEntr_BAG_L1_9\",\n",
      "        \"ExtraTreesEntr_BAG_L1_10\",\n",
      "        \"ExtraTreesEntr_BAG_L1_11\",\n",
      "        \"ExtraTreesEntr_BAG_L1_12\",\n",
      "        \"ExtraTreesEntr_BAG_L1_13\",\n",
      "        \"ExtraTreesEntr_BAG_L1_14\",\n",
      "        \"ExtraTreesEntr_BAG_L1_15\",\n",
      "        \"ExtraTreesEntr_BAG_L1_16\",\n",
      "        \"ExtraTreesEntr_BAG_L1_17\",\n",
      "        \"ExtraTreesEntr_BAG_L1_18\",\n",
      "        \"ExtraTreesEntr_BAG_L1_19\",\n",
      "        \"ExtraTreesEntr_BAG_L1_20\",\n",
      "        \"ExtraTreesEntr_BAG_L1_21\",\n",
      "        \"ExtraTreesEntr_BAG_L1_22\",\n",
      "        \"ExtraTreesEntr_BAG_L1_23\",\n",
      "        \"ExtraTreesEntr_BAG_L1_24\",\n",
      "        \"ExtraTreesEntr_BAG_L1_25\",\n",
      "        \"ExtraTreesEntr_BAG_L1_26\",\n",
      "        \"ExtraTreesEntr_BAG_L1_27\",\n",
      "        \"ExtraTreesEntr_BAG_L1_28\",\n",
      "        \"ExtraTreesEntr_BAG_L1_29\",\n",
      "        \"ExtraTreesEntr_BAG_L1_30\",\n",
      "        \"ExtraTreesEntr_BAG_L1_31\",\n",
      "        \"ExtraTreesEntr_BAG_L1_32\",\n",
      "        \"ExtraTreesEntr_BAG_L1_33\",\n",
      "        \"ExtraTreesEntr_BAG_L1_34\",\n",
      "        \"ExtraTreesEntr_BAG_L1_35\",\n",
      "        \"ExtraTreesEntr_BAG_L1_36\",\n",
      "        \"ExtraTreesEntr_BAG_L1_37\",\n",
      "        \"ExtraTreesEntr_BAG_L1_38\",\n",
      "        \"ExtraTreesEntr_BAG_L1_39\",\n",
      "        \"ExtraTreesEntr_BAG_L1_40\",\n",
      "        \"ExtraTreesEntr_BAG_L1_41\",\n",
      "        \"ExtraTreesEntr_BAG_L1_42\",\n",
      "        \"NeuralNetTorch_BAG_L1_0\",\n",
      "        \"NeuralNetTorch_BAG_L1_1\",\n",
      "        \"NeuralNetTorch_BAG_L1_2\",\n",
      "        \"NeuralNetTorch_BAG_L1_3\",\n",
      "        \"NeuralNetTorch_BAG_L1_4\",\n",
      "        \"NeuralNetTorch_BAG_L1_5\",\n",
      "        \"NeuralNetTorch_BAG_L1_6\",\n",
      "        \"NeuralNetTorch_BAG_L1_7\",\n",
      "        \"NeuralNetTorch_BAG_L1_8\",\n",
      "        \"NeuralNetTorch_BAG_L1_9\",\n",
      "        \"NeuralNetTorch_BAG_L1_10\",\n",
      "        \"NeuralNetTorch_BAG_L1_11\",\n",
      "        \"NeuralNetTorch_BAG_L1_12\",\n",
      "        \"NeuralNetTorch_BAG_L1_13\",\n",
      "        \"NeuralNetTorch_BAG_L1_14\",\n",
      "        \"NeuralNetTorch_BAG_L1_15\",\n",
      "        \"NeuralNetTorch_BAG_L1_16\",\n",
      "        \"NeuralNetTorch_BAG_L1_17\",\n",
      "        \"NeuralNetTorch_BAG_L1_18\",\n",
      "        \"NeuralNetTorch_BAG_L1_19\",\n",
      "        \"NeuralNetTorch_BAG_L1_20\",\n",
      "        \"NeuralNetTorch_BAG_L1_21\",\n",
      "        \"NeuralNetTorch_BAG_L1_22\",\n",
      "        \"NeuralNetTorch_BAG_L1_23\",\n",
      "        \"NeuralNetTorch_BAG_L1_24\",\n",
      "        \"NeuralNetTorch_BAG_L1_25\",\n",
      "        \"NeuralNetTorch_BAG_L1_26\",\n",
      "        \"NeuralNetTorch_BAG_L1_27\",\n",
      "        \"NeuralNetTorch_BAG_L1_28\",\n",
      "        \"NeuralNetTorch_BAG_L1_29\",\n",
      "        \"NeuralNetTorch_BAG_L1_30\",\n",
      "        \"NeuralNetTorch_BAG_L1_31\",\n",
      "        \"NeuralNetTorch_BAG_L1_32\",\n",
      "        \"NeuralNetTorch_BAG_L1_33\",\n",
      "        \"NeuralNetTorch_BAG_L1_34\",\n",
      "        \"NeuralNetTorch_BAG_L1_35\",\n",
      "        \"NeuralNetTorch_BAG_L1_36\",\n",
      "        \"NeuralNetTorch_BAG_L1_37\",\n",
      "        \"NeuralNetTorch_BAG_L1_38\",\n",
      "        \"NeuralNetTorch_BAG_L1_39\",\n",
      "        \"NeuralNetTorch_BAG_L1_40\",\n",
      "        \"NeuralNetTorch_BAG_L1_41\",\n",
      "        \"NeuralNetTorch_BAG_L1_42\",\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f212\",\n",
      "        \"f213\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f441\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f468\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f609\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f638\",\n",
      "        \"f639\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [\n",
      "        \"KNeighborsUnif_BAG_L1_18\",\n",
      "        \"KNeighborsUnif_BAG_L1_19\",\n",
      "        \"KNeighborsUnif_BAG_L1_20\",\n",
      "        \"KNeighborsUnif_BAG_L1_21\",\n",
      "        \"KNeighborsUnif_BAG_L1_22\",\n",
      "        \"KNeighborsUnif_BAG_L1_23\",\n",
      "        \"KNeighborsUnif_BAG_L1_24\",\n",
      "        \"KNeighborsUnif_BAG_L1_25\",\n",
      "        \"KNeighborsUnif_BAG_L1_26\",\n",
      "        \"KNeighborsUnif_BAG_L1_27\",\n",
      "        \"KNeighborsUnif_BAG_L1_28\",\n",
      "        \"KNeighborsUnif_BAG_L1_29\",\n",
      "        \"KNeighborsUnif_BAG_L1_30\",\n",
      "        \"KNeighborsUnif_BAG_L1_31\",\n",
      "        \"KNeighborsUnif_BAG_L1_32\",\n",
      "        \"KNeighborsUnif_BAG_L1_33\",\n",
      "        \"KNeighborsUnif_BAG_L1_35\",\n",
      "        \"KNeighborsUnif_BAG_L1_36\",\n",
      "        \"KNeighborsUnif_BAG_L1_37\",\n",
      "        \"KNeighborsUnif_BAG_L1_38\",\n",
      "        \"KNeighborsUnif_BAG_L1_39\",\n",
      "        \"KNeighborsUnif_BAG_L1_40\",\n",
      "        \"KNeighborsUnif_BAG_L1_41\",\n",
      "        \"KNeighborsUnif_BAG_L1_42\"\n",
      "    ],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92145 examples, 1060 features (1041 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(102, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(22, 9)\n",
      "    (16): Embedding(25, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1440, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.5992, Val mean_absolute_error: -0.6867, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.517, Val mean_absolute_error: -0.6867, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5118, Val mean_absolute_error: -0.6867, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.5078, Val mean_absolute_error: -0.6867, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.5059, Val mean_absolute_error: -0.6867, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.5032, Val mean_absolute_error: -0.6867, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.5021, Val mean_absolute_error: -0.6867, Best Epoch: 7\n",
      "Epoch 8 (Update 5752).\tTrain loss: 0.4992, Val mean_absolute_error: -0.6867, Best Epoch: 8\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "Best model found on Epoch 8 (Update 5752). Val mean_absolute_error: -0.6867213612883623\n",
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"RandomForestEntr_BAG_L1_0\",\n",
      "        \"ExtraTreesGini_BAG_L1_0\",\n",
      "        \"ExtraTreesEntr_BAG_L1_0\",\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f211\",\n",
      "        \"f213\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f638\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"KNeighborsUnif_BAG_L1_0\",\n",
      "        \"KNeighborsUnif_BAG_L1_1\",\n",
      "        \"KNeighborsUnif_BAG_L1_2\",\n",
      "        \"KNeighborsUnif_BAG_L1_3\",\n",
      "        \"KNeighborsUnif_BAG_L1_4\",\n",
      "        \"KNeighborsUnif_BAG_L1_5\",\n",
      "        \"KNeighborsUnif_BAG_L1_6\",\n",
      "        \"KNeighborsUnif_BAG_L1_7\",\n",
      "        \"KNeighborsUnif_BAG_L1_8\",\n",
      "        \"KNeighborsUnif_BAG_L1_9\",\n",
      "        \"KNeighborsUnif_BAG_L1_10\",\n",
      "        \"KNeighborsUnif_BAG_L1_11\",\n",
      "        \"KNeighborsUnif_BAG_L1_12\",\n",
      "        \"KNeighborsUnif_BAG_L1_13\",\n",
      "        \"KNeighborsUnif_BAG_L1_14\",\n",
      "        \"KNeighborsUnif_BAG_L1_15\",\n",
      "        \"KNeighborsUnif_BAG_L1_16\",\n",
      "        \"KNeighborsUnif_BAG_L1_17\",\n",
      "        \"KNeighborsUnif_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_0\",\n",
      "        \"KNeighborsDist_BAG_L1_1\",\n",
      "        \"KNeighborsDist_BAG_L1_2\",\n",
      "        \"KNeighborsDist_BAG_L1_3\",\n",
      "        \"KNeighborsDist_BAG_L1_4\",\n",
      "        \"KNeighborsDist_BAG_L1_5\",\n",
      "        \"KNeighborsDist_BAG_L1_6\",\n",
      "        \"KNeighborsDist_BAG_L1_7\",\n",
      "        \"KNeighborsDist_BAG_L1_8\",\n",
      "        \"KNeighborsDist_BAG_L1_9\",\n",
      "        \"KNeighborsDist_BAG_L1_10\",\n",
      "        \"KNeighborsDist_BAG_L1_11\",\n",
      "        \"KNeighborsDist_BAG_L1_12\",\n",
      "        \"KNeighborsDist_BAG_L1_13\",\n",
      "        \"KNeighborsDist_BAG_L1_14\",\n",
      "        \"KNeighborsDist_BAG_L1_15\",\n",
      "        \"KNeighborsDist_BAG_L1_16\",\n",
      "        \"KNeighborsDist_BAG_L1_17\",\n",
      "        \"KNeighborsDist_BAG_L1_18\",\n",
      "        \"KNeighborsDist_BAG_L1_19\",\n",
      "        \"KNeighborsDist_BAG_L1_20\",\n",
      "        \"KNeighborsDist_BAG_L1_21\",\n",
      "        \"KNeighborsDist_BAG_L1_22\",\n",
      "        \"KNeighborsDist_BAG_L1_23\",\n",
      "        \"KNeighborsDist_BAG_L1_24\",\n",
      "        \"KNeighborsDist_BAG_L1_25\",\n",
      "        \"KNeighborsDist_BAG_L1_26\",\n",
      "        \"KNeighborsDist_BAG_L1_27\",\n",
      "        \"KNeighborsDist_BAG_L1_28\",\n",
      "        \"KNeighborsDist_BAG_L1_29\",\n",
      "        \"KNeighborsDist_BAG_L1_30\",\n",
      "        \"KNeighborsDist_BAG_L1_31\",\n",
      "        \"KNeighborsDist_BAG_L1_32\",\n",
      "        \"KNeighborsDist_BAG_L1_33\",\n",
      "        \"KNeighborsDist_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_35\",\n",
      "        \"KNeighborsDist_BAG_L1_36\",\n",
      "        \"KNeighborsDist_BAG_L1_37\",\n",
      "        \"KNeighborsDist_BAG_L1_38\",\n",
      "        \"KNeighborsDist_BAG_L1_39\",\n",
      "        \"KNeighborsDist_BAG_L1_40\",\n",
      "        \"KNeighborsDist_BAG_L1_41\",\n",
      "        \"KNeighborsDist_BAG_L1_42\",\n",
      "        \"RandomForestGini_BAG_L1_0\",\n",
      "        \"RandomForestGini_BAG_L1_1\",\n",
      "        \"RandomForestGini_BAG_L1_2\",\n",
      "        \"RandomForestGini_BAG_L1_3\",\n",
      "        \"RandomForestGini_BAG_L1_4\",\n",
      "        \"RandomForestGini_BAG_L1_5\",\n",
      "        \"RandomForestGini_BAG_L1_6\",\n",
      "        \"RandomForestGini_BAG_L1_7\",\n",
      "        \"RandomForestGini_BAG_L1_8\",\n",
      "        \"RandomForestGini_BAG_L1_9\",\n",
      "        \"RandomForestGini_BAG_L1_10\",\n",
      "        \"RandomForestGini_BAG_L1_11\",\n",
      "        \"RandomForestGini_BAG_L1_12\",\n",
      "        \"RandomForestGini_BAG_L1_13\",\n",
      "        \"RandomForestGini_BAG_L1_14\",\n",
      "        \"RandomForestGini_BAG_L1_15\",\n",
      "        \"RandomForestGini_BAG_L1_16\",\n",
      "        \"RandomForestGini_BAG_L1_17\",\n",
      "        \"RandomForestGini_BAG_L1_18\",\n",
      "        \"RandomForestGini_BAG_L1_19\",\n",
      "        \"RandomForestGini_BAG_L1_20\",\n",
      "        \"RandomForestGini_BAG_L1_21\",\n",
      "        \"RandomForestGini_BAG_L1_22\",\n",
      "        \"RandomForestGini_BAG_L1_23\",\n",
      "        \"RandomForestGini_BAG_L1_24\",\n",
      "        \"RandomForestGini_BAG_L1_25\",\n",
      "        \"RandomForestGini_BAG_L1_26\",\n",
      "        \"RandomForestGini_BAG_L1_27\",\n",
      "        \"RandomForestGini_BAG_L1_28\",\n",
      "        \"RandomForestGini_BAG_L1_29\",\n",
      "        \"RandomForestGini_BAG_L1_30\",\n",
      "        \"RandomForestGini_BAG_L1_31\",\n",
      "        \"RandomForestGini_BAG_L1_32\",\n",
      "        \"RandomForestGini_BAG_L1_33\",\n",
      "        \"RandomForestGini_BAG_L1_34\",\n",
      "        \"RandomForestGini_BAG_L1_35\",\n",
      "        \"RandomForestGini_BAG_L1_36\",\n",
      "        \"RandomForestGini_BAG_L1_37\",\n",
      "        \"RandomForestGini_BAG_L1_38\",\n",
      "        \"RandomForestGini_BAG_L1_39\",\n",
      "        \"RandomForestGini_BAG_L1_40\",\n",
      "        \"RandomForestGini_BAG_L1_41\",\n",
      "        \"RandomForestGini_BAG_L1_42\",\n",
      "        \"RandomForestEntr_BAG_L1_1\",\n",
      "        \"RandomForestEntr_BAG_L1_2\",\n",
      "        \"RandomForestEntr_BAG_L1_3\",\n",
      "        \"RandomForestEntr_BAG_L1_4\",\n",
      "        \"RandomForestEntr_BAG_L1_5\",\n",
      "        \"RandomForestEntr_BAG_L1_6\",\n",
      "        \"RandomForestEntr_BAG_L1_7\",\n",
      "        \"RandomForestEntr_BAG_L1_8\",\n",
      "        \"RandomForestEntr_BAG_L1_9\",\n",
      "        \"RandomForestEntr_BAG_L1_10\",\n",
      "        \"RandomForestEntr_BAG_L1_11\",\n",
      "        \"RandomForestEntr_BAG_L1_12\",\n",
      "        \"RandomForestEntr_BAG_L1_13\",\n",
      "        \"RandomForestEntr_BAG_L1_14\",\n",
      "        \"RandomForestEntr_BAG_L1_15\",\n",
      "        \"RandomForestEntr_BAG_L1_16\",\n",
      "        \"RandomForestEntr_BAG_L1_17\",\n",
      "        \"RandomForestEntr_BAG_L1_18\",\n",
      "        \"RandomForestEntr_BAG_L1_19\",\n",
      "        \"RandomForestEntr_BAG_L1_20\",\n",
      "        \"RandomForestEntr_BAG_L1_21\",\n",
      "        \"RandomForestEntr_BAG_L1_22\",\n",
      "        \"RandomForestEntr_BAG_L1_23\",\n",
      "        \"RandomForestEntr_BAG_L1_24\",\n",
      "        \"RandomForestEntr_BAG_L1_25\",\n",
      "        \"RandomForestEntr_BAG_L1_26\",\n",
      "        \"RandomForestEntr_BAG_L1_27\",\n",
      "        \"RandomForestEntr_BAG_L1_28\",\n",
      "        \"RandomForestEntr_BAG_L1_29\",\n",
      "        \"RandomForestEntr_BAG_L1_30\",\n",
      "        \"RandomForestEntr_BAG_L1_31\",\n",
      "        \"RandomForestEntr_BAG_L1_32\",\n",
      "        \"RandomForestEntr_BAG_L1_33\",\n",
      "        \"RandomForestEntr_BAG_L1_34\",\n",
      "        \"RandomForestEntr_BAG_L1_35\",\n",
      "        \"RandomForestEntr_BAG_L1_36\",\n",
      "        \"RandomForestEntr_BAG_L1_37\",\n",
      "        \"RandomForestEntr_BAG_L1_38\",\n",
      "        \"RandomForestEntr_BAG_L1_39\",\n",
      "        \"RandomForestEntr_BAG_L1_40\",\n",
      "        \"RandomForestEntr_BAG_L1_41\",\n",
      "        \"RandomForestEntr_BAG_L1_42\",\n",
      "        \"ExtraTreesGini_BAG_L1_1\",\n",
      "        \"ExtraTreesGini_BAG_L1_2\",\n",
      "        \"ExtraTreesGini_BAG_L1_3\",\n",
      "        \"ExtraTreesGini_BAG_L1_4\",\n",
      "        \"ExtraTreesGini_BAG_L1_5\",\n",
      "        \"ExtraTreesGini_BAG_L1_6\",\n",
      "        \"ExtraTreesGini_BAG_L1_7\",\n",
      "        \"ExtraTreesGini_BAG_L1_8\",\n",
      "        \"ExtraTreesGini_BAG_L1_9\",\n",
      "        \"ExtraTreesGini_BAG_L1_10\",\n",
      "        \"ExtraTreesGini_BAG_L1_11\",\n",
      "        \"ExtraTreesGini_BAG_L1_12\",\n",
      "        \"ExtraTreesGini_BAG_L1_13\",\n",
      "        \"ExtraTreesGini_BAG_L1_14\",\n",
      "        \"ExtraTreesGini_BAG_L1_15\",\n",
      "        \"ExtraTreesGini_BAG_L1_16\",\n",
      "        \"ExtraTreesGini_BAG_L1_17\",\n",
      "        \"ExtraTreesGini_BAG_L1_18\",\n",
      "        \"ExtraTreesGini_BAG_L1_19\",\n",
      "        \"ExtraTreesGini_BAG_L1_20\",\n",
      "        \"ExtraTreesGini_BAG_L1_21\",\n",
      "        \"ExtraTreesGini_BAG_L1_22\",\n",
      "        \"ExtraTreesGini_BAG_L1_23\",\n",
      "        \"ExtraTreesGini_BAG_L1_24\",\n",
      "        \"ExtraTreesGini_BAG_L1_25\",\n",
      "        \"ExtraTreesGini_BAG_L1_26\",\n",
      "        \"ExtraTreesGini_BAG_L1_27\",\n",
      "        \"ExtraTreesGini_BAG_L1_28\",\n",
      "        \"ExtraTreesGini_BAG_L1_29\",\n",
      "        \"ExtraTreesGini_BAG_L1_30\",\n",
      "        \"ExtraTreesGini_BAG_L1_31\",\n",
      "        \"ExtraTreesGini_BAG_L1_32\",\n",
      "        \"ExtraTreesGini_BAG_L1_33\",\n",
      "        \"ExtraTreesGini_BAG_L1_34\",\n",
      "        \"ExtraTreesGini_BAG_L1_35\",\n",
      "        \"ExtraTreesGini_BAG_L1_36\",\n",
      "        \"ExtraTreesGini_BAG_L1_37\",\n",
      "        \"ExtraTreesGini_BAG_L1_38\",\n",
      "        \"ExtraTreesGini_BAG_L1_39\",\n",
      "        \"ExtraTreesGini_BAG_L1_40\",\n",
      "        \"ExtraTreesGini_BAG_L1_41\",\n",
      "        \"ExtraTreesGini_BAG_L1_42\",\n",
      "        \"ExtraTreesEntr_BAG_L1_1\",\n",
      "        \"ExtraTreesEntr_BAG_L1_2\",\n",
      "        \"ExtraTreesEntr_BAG_L1_3\",\n",
      "        \"ExtraTreesEntr_BAG_L1_4\",\n",
      "        \"ExtraTreesEntr_BAG_L1_5\",\n",
      "        \"ExtraTreesEntr_BAG_L1_6\",\n",
      "        \"ExtraTreesEntr_BAG_L1_7\",\n",
      "        \"ExtraTreesEntr_BAG_L1_8\",\n",
      "        \"ExtraTreesEntr_BAG_L1_9\",\n",
      "        \"ExtraTreesEntr_BAG_L1_10\",\n",
      "        \"ExtraTreesEntr_BAG_L1_11\",\n",
      "        \"ExtraTreesEntr_BAG_L1_12\",\n",
      "        \"ExtraTreesEntr_BAG_L1_13\",\n",
      "        \"ExtraTreesEntr_BAG_L1_14\",\n",
      "        \"ExtraTreesEntr_BAG_L1_15\",\n",
      "        \"ExtraTreesEntr_BAG_L1_16\",\n",
      "        \"ExtraTreesEntr_BAG_L1_17\",\n",
      "        \"ExtraTreesEntr_BAG_L1_18\",\n",
      "        \"ExtraTreesEntr_BAG_L1_19\",\n",
      "        \"ExtraTreesEntr_BAG_L1_20\",\n",
      "        \"ExtraTreesEntr_BAG_L1_21\",\n",
      "        \"ExtraTreesEntr_BAG_L1_22\",\n",
      "        \"ExtraTreesEntr_BAG_L1_23\",\n",
      "        \"ExtraTreesEntr_BAG_L1_24\",\n",
      "        \"ExtraTreesEntr_BAG_L1_25\",\n",
      "        \"ExtraTreesEntr_BAG_L1_26\",\n",
      "        \"ExtraTreesEntr_BAG_L1_27\",\n",
      "        \"ExtraTreesEntr_BAG_L1_28\",\n",
      "        \"ExtraTreesEntr_BAG_L1_29\",\n",
      "        \"ExtraTreesEntr_BAG_L1_30\",\n",
      "        \"ExtraTreesEntr_BAG_L1_31\",\n",
      "        \"ExtraTreesEntr_BAG_L1_32\",\n",
      "        \"ExtraTreesEntr_BAG_L1_33\",\n",
      "        \"ExtraTreesEntr_BAG_L1_34\",\n",
      "        \"ExtraTreesEntr_BAG_L1_35\",\n",
      "        \"ExtraTreesEntr_BAG_L1_36\",\n",
      "        \"ExtraTreesEntr_BAG_L1_37\",\n",
      "        \"ExtraTreesEntr_BAG_L1_38\",\n",
      "        \"ExtraTreesEntr_BAG_L1_39\",\n",
      "        \"ExtraTreesEntr_BAG_L1_40\",\n",
      "        \"ExtraTreesEntr_BAG_L1_41\",\n",
      "        \"ExtraTreesEntr_BAG_L1_42\",\n",
      "        \"NeuralNetTorch_BAG_L1_0\",\n",
      "        \"NeuralNetTorch_BAG_L1_1\",\n",
      "        \"NeuralNetTorch_BAG_L1_2\",\n",
      "        \"NeuralNetTorch_BAG_L1_3\",\n",
      "        \"NeuralNetTorch_BAG_L1_4\",\n",
      "        \"NeuralNetTorch_BAG_L1_5\",\n",
      "        \"NeuralNetTorch_BAG_L1_6\",\n",
      "        \"NeuralNetTorch_BAG_L1_7\",\n",
      "        \"NeuralNetTorch_BAG_L1_8\",\n",
      "        \"NeuralNetTorch_BAG_L1_9\",\n",
      "        \"NeuralNetTorch_BAG_L1_10\",\n",
      "        \"NeuralNetTorch_BAG_L1_11\",\n",
      "        \"NeuralNetTorch_BAG_L1_12\",\n",
      "        \"NeuralNetTorch_BAG_L1_13\",\n",
      "        \"NeuralNetTorch_BAG_L1_14\",\n",
      "        \"NeuralNetTorch_BAG_L1_15\",\n",
      "        \"NeuralNetTorch_BAG_L1_16\",\n",
      "        \"NeuralNetTorch_BAG_L1_17\",\n",
      "        \"NeuralNetTorch_BAG_L1_18\",\n",
      "        \"NeuralNetTorch_BAG_L1_19\",\n",
      "        \"NeuralNetTorch_BAG_L1_20\",\n",
      "        \"NeuralNetTorch_BAG_L1_21\",\n",
      "        \"NeuralNetTorch_BAG_L1_22\",\n",
      "        \"NeuralNetTorch_BAG_L1_23\",\n",
      "        \"NeuralNetTorch_BAG_L1_24\",\n",
      "        \"NeuralNetTorch_BAG_L1_25\",\n",
      "        \"NeuralNetTorch_BAG_L1_26\",\n",
      "        \"NeuralNetTorch_BAG_L1_27\",\n",
      "        \"NeuralNetTorch_BAG_L1_28\",\n",
      "        \"NeuralNetTorch_BAG_L1_29\",\n",
      "        \"NeuralNetTorch_BAG_L1_30\",\n",
      "        \"NeuralNetTorch_BAG_L1_31\",\n",
      "        \"NeuralNetTorch_BAG_L1_32\",\n",
      "        \"NeuralNetTorch_BAG_L1_33\",\n",
      "        \"NeuralNetTorch_BAG_L1_34\",\n",
      "        \"NeuralNetTorch_BAG_L1_35\",\n",
      "        \"NeuralNetTorch_BAG_L1_36\",\n",
      "        \"NeuralNetTorch_BAG_L1_37\",\n",
      "        \"NeuralNetTorch_BAG_L1_38\",\n",
      "        \"NeuralNetTorch_BAG_L1_39\",\n",
      "        \"NeuralNetTorch_BAG_L1_40\",\n",
      "        \"NeuralNetTorch_BAG_L1_41\",\n",
      "        \"NeuralNetTorch_BAG_L1_42\",\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f91\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f212\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f349\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f441\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f468\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f609\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f639\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f647\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [\n",
      "        \"KNeighborsUnif_BAG_L1_18\",\n",
      "        \"KNeighborsUnif_BAG_L1_19\",\n",
      "        \"KNeighborsUnif_BAG_L1_20\",\n",
      "        \"KNeighborsUnif_BAG_L1_21\",\n",
      "        \"KNeighborsUnif_BAG_L1_22\",\n",
      "        \"KNeighborsUnif_BAG_L1_23\",\n",
      "        \"KNeighborsUnif_BAG_L1_24\",\n",
      "        \"KNeighborsUnif_BAG_L1_25\",\n",
      "        \"KNeighborsUnif_BAG_L1_26\",\n",
      "        \"KNeighborsUnif_BAG_L1_27\",\n",
      "        \"KNeighborsUnif_BAG_L1_28\",\n",
      "        \"KNeighborsUnif_BAG_L1_29\",\n",
      "        \"KNeighborsUnif_BAG_L1_30\",\n",
      "        \"KNeighborsUnif_BAG_L1_31\",\n",
      "        \"KNeighborsUnif_BAG_L1_32\",\n",
      "        \"KNeighborsUnif_BAG_L1_33\",\n",
      "        \"KNeighborsUnif_BAG_L1_35\",\n",
      "        \"KNeighborsUnif_BAG_L1_36\",\n",
      "        \"KNeighborsUnif_BAG_L1_37\",\n",
      "        \"KNeighborsUnif_BAG_L1_38\",\n",
      "        \"KNeighborsUnif_BAG_L1_39\",\n",
      "        \"KNeighborsUnif_BAG_L1_40\",\n",
      "        \"KNeighborsUnif_BAG_L1_41\",\n",
      "        \"KNeighborsUnif_BAG_L1_42\"\n",
      "    ],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f678\",\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92145 examples, 1061 features (1042 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(101, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(21, 8)\n",
      "    (16): Embedding(24, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1440, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.6045, Val mean_absolute_error: -0.6836, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5149, Val mean_absolute_error: -0.6836, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5099, Val mean_absolute_error: -0.6836, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.5072, Val mean_absolute_error: -0.6836, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.5047, Val mean_absolute_error: -0.6836, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.502, Val mean_absolute_error: -0.6836, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.5, Val mean_absolute_error: -0.6836, Best Epoch: 7\n",
      "Epoch 8 (Update 5752).\tTrain loss: 0.4989, Val mean_absolute_error: -0.6836, Best Epoch: 8\n",
      "Epoch 9 (Update 6471).\tTrain loss: 0.4968, Val mean_absolute_error: -0.6836, Best Epoch: 9\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 9)\n",
      "Best model found on Epoch 9 (Update 6471). Val mean_absolute_error: -0.683606806441811\n",
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"RandomForestEntr_BAG_L1_0\",\n",
      "        \"ExtraTreesGini_BAG_L1_0\",\n",
      "        \"ExtraTreesEntr_BAG_L1_0\",\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f91\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f211\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f609\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f638\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f647\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"KNeighborsUnif_BAG_L1_0\",\n",
      "        \"KNeighborsUnif_BAG_L1_1\",\n",
      "        \"KNeighborsUnif_BAG_L1_2\",\n",
      "        \"KNeighborsUnif_BAG_L1_3\",\n",
      "        \"KNeighborsUnif_BAG_L1_4\",\n",
      "        \"KNeighborsUnif_BAG_L1_5\",\n",
      "        \"KNeighborsUnif_BAG_L1_6\",\n",
      "        \"KNeighborsUnif_BAG_L1_7\",\n",
      "        \"KNeighborsUnif_BAG_L1_8\",\n",
      "        \"KNeighborsUnif_BAG_L1_9\",\n",
      "        \"KNeighborsUnif_BAG_L1_10\",\n",
      "        \"KNeighborsUnif_BAG_L1_11\",\n",
      "        \"KNeighborsUnif_BAG_L1_12\",\n",
      "        \"KNeighborsUnif_BAG_L1_13\",\n",
      "        \"KNeighborsUnif_BAG_L1_14\",\n",
      "        \"KNeighborsUnif_BAG_L1_15\",\n",
      "        \"KNeighborsUnif_BAG_L1_16\",\n",
      "        \"KNeighborsUnif_BAG_L1_17\",\n",
      "        \"KNeighborsUnif_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_0\",\n",
      "        \"KNeighborsDist_BAG_L1_1\",\n",
      "        \"KNeighborsDist_BAG_L1_2\",\n",
      "        \"KNeighborsDist_BAG_L1_3\",\n",
      "        \"KNeighborsDist_BAG_L1_4\",\n",
      "        \"KNeighborsDist_BAG_L1_5\",\n",
      "        \"KNeighborsDist_BAG_L1_6\",\n",
      "        \"KNeighborsDist_BAG_L1_7\",\n",
      "        \"KNeighborsDist_BAG_L1_8\",\n",
      "        \"KNeighborsDist_BAG_L1_9\",\n",
      "        \"KNeighborsDist_BAG_L1_10\",\n",
      "        \"KNeighborsDist_BAG_L1_11\",\n",
      "        \"KNeighborsDist_BAG_L1_12\",\n",
      "        \"KNeighborsDist_BAG_L1_13\",\n",
      "        \"KNeighborsDist_BAG_L1_14\",\n",
      "        \"KNeighborsDist_BAG_L1_15\",\n",
      "        \"KNeighborsDist_BAG_L1_16\",\n",
      "        \"KNeighborsDist_BAG_L1_17\",\n",
      "        \"KNeighborsDist_BAG_L1_18\",\n",
      "        \"KNeighborsDist_BAG_L1_19\",\n",
      "        \"KNeighborsDist_BAG_L1_20\",\n",
      "        \"KNeighborsDist_BAG_L1_21\",\n",
      "        \"KNeighborsDist_BAG_L1_22\",\n",
      "        \"KNeighborsDist_BAG_L1_23\",\n",
      "        \"KNeighborsDist_BAG_L1_24\",\n",
      "        \"KNeighborsDist_BAG_L1_25\",\n",
      "        \"KNeighborsDist_BAG_L1_26\",\n",
      "        \"KNeighborsDist_BAG_L1_27\",\n",
      "        \"KNeighborsDist_BAG_L1_28\",\n",
      "        \"KNeighborsDist_BAG_L1_29\",\n",
      "        \"KNeighborsDist_BAG_L1_30\",\n",
      "        \"KNeighborsDist_BAG_L1_31\",\n",
      "        \"KNeighborsDist_BAG_L1_32\",\n",
      "        \"KNeighborsDist_BAG_L1_33\",\n",
      "        \"KNeighborsDist_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_35\",\n",
      "        \"KNeighborsDist_BAG_L1_36\",\n",
      "        \"KNeighborsDist_BAG_L1_37\",\n",
      "        \"KNeighborsDist_BAG_L1_38\",\n",
      "        \"KNeighborsDist_BAG_L1_39\",\n",
      "        \"KNeighborsDist_BAG_L1_40\",\n",
      "        \"KNeighborsDist_BAG_L1_41\",\n",
      "        \"KNeighborsDist_BAG_L1_42\",\n",
      "        \"RandomForestGini_BAG_L1_0\",\n",
      "        \"RandomForestGini_BAG_L1_1\",\n",
      "        \"RandomForestGini_BAG_L1_2\",\n",
      "        \"RandomForestGini_BAG_L1_3\",\n",
      "        \"RandomForestGini_BAG_L1_4\",\n",
      "        \"RandomForestGini_BAG_L1_5\",\n",
      "        \"RandomForestGini_BAG_L1_6\",\n",
      "        \"RandomForestGini_BAG_L1_7\",\n",
      "        \"RandomForestGini_BAG_L1_8\",\n",
      "        \"RandomForestGini_BAG_L1_9\",\n",
      "        \"RandomForestGini_BAG_L1_10\",\n",
      "        \"RandomForestGini_BAG_L1_11\",\n",
      "        \"RandomForestGini_BAG_L1_12\",\n",
      "        \"RandomForestGini_BAG_L1_13\",\n",
      "        \"RandomForestGini_BAG_L1_14\",\n",
      "        \"RandomForestGini_BAG_L1_15\",\n",
      "        \"RandomForestGini_BAG_L1_16\",\n",
      "        \"RandomForestGini_BAG_L1_17\",\n",
      "        \"RandomForestGini_BAG_L1_18\",\n",
      "        \"RandomForestGini_BAG_L1_19\",\n",
      "        \"RandomForestGini_BAG_L1_20\",\n",
      "        \"RandomForestGini_BAG_L1_21\",\n",
      "        \"RandomForestGini_BAG_L1_22\",\n",
      "        \"RandomForestGini_BAG_L1_23\",\n",
      "        \"RandomForestGini_BAG_L1_24\",\n",
      "        \"RandomForestGini_BAG_L1_25\",\n",
      "        \"RandomForestGini_BAG_L1_26\",\n",
      "        \"RandomForestGini_BAG_L1_27\",\n",
      "        \"RandomForestGini_BAG_L1_28\",\n",
      "        \"RandomForestGini_BAG_L1_29\",\n",
      "        \"RandomForestGini_BAG_L1_30\",\n",
      "        \"RandomForestGini_BAG_L1_31\",\n",
      "        \"RandomForestGini_BAG_L1_32\",\n",
      "        \"RandomForestGini_BAG_L1_33\",\n",
      "        \"RandomForestGini_BAG_L1_34\",\n",
      "        \"RandomForestGini_BAG_L1_35\",\n",
      "        \"RandomForestGini_BAG_L1_36\",\n",
      "        \"RandomForestGini_BAG_L1_37\",\n",
      "        \"RandomForestGini_BAG_L1_38\",\n",
      "        \"RandomForestGini_BAG_L1_39\",\n",
      "        \"RandomForestGini_BAG_L1_40\",\n",
      "        \"RandomForestGini_BAG_L1_41\",\n",
      "        \"RandomForestGini_BAG_L1_42\",\n",
      "        \"RandomForestEntr_BAG_L1_1\",\n",
      "        \"RandomForestEntr_BAG_L1_2\",\n",
      "        \"RandomForestEntr_BAG_L1_3\",\n",
      "        \"RandomForestEntr_BAG_L1_4\",\n",
      "        \"RandomForestEntr_BAG_L1_5\",\n",
      "        \"RandomForestEntr_BAG_L1_6\",\n",
      "        \"RandomForestEntr_BAG_L1_7\",\n",
      "        \"RandomForestEntr_BAG_L1_8\",\n",
      "        \"RandomForestEntr_BAG_L1_9\",\n",
      "        \"RandomForestEntr_BAG_L1_10\",\n",
      "        \"RandomForestEntr_BAG_L1_11\",\n",
      "        \"RandomForestEntr_BAG_L1_12\",\n",
      "        \"RandomForestEntr_BAG_L1_13\",\n",
      "        \"RandomForestEntr_BAG_L1_14\",\n",
      "        \"RandomForestEntr_BAG_L1_15\",\n",
      "        \"RandomForestEntr_BAG_L1_16\",\n",
      "        \"RandomForestEntr_BAG_L1_17\",\n",
      "        \"RandomForestEntr_BAG_L1_18\",\n",
      "        \"RandomForestEntr_BAG_L1_19\",\n",
      "        \"RandomForestEntr_BAG_L1_20\",\n",
      "        \"RandomForestEntr_BAG_L1_21\",\n",
      "        \"RandomForestEntr_BAG_L1_22\",\n",
      "        \"RandomForestEntr_BAG_L1_23\",\n",
      "        \"RandomForestEntr_BAG_L1_24\",\n",
      "        \"RandomForestEntr_BAG_L1_25\",\n",
      "        \"RandomForestEntr_BAG_L1_26\",\n",
      "        \"RandomForestEntr_BAG_L1_27\",\n",
      "        \"RandomForestEntr_BAG_L1_28\",\n",
      "        \"RandomForestEntr_BAG_L1_29\",\n",
      "        \"RandomForestEntr_BAG_L1_30\",\n",
      "        \"RandomForestEntr_BAG_L1_31\",\n",
      "        \"RandomForestEntr_BAG_L1_32\",\n",
      "        \"RandomForestEntr_BAG_L1_33\",\n",
      "        \"RandomForestEntr_BAG_L1_34\",\n",
      "        \"RandomForestEntr_BAG_L1_35\",\n",
      "        \"RandomForestEntr_BAG_L1_36\",\n",
      "        \"RandomForestEntr_BAG_L1_37\",\n",
      "        \"RandomForestEntr_BAG_L1_38\",\n",
      "        \"RandomForestEntr_BAG_L1_39\",\n",
      "        \"RandomForestEntr_BAG_L1_40\",\n",
      "        \"RandomForestEntr_BAG_L1_41\",\n",
      "        \"RandomForestEntr_BAG_L1_42\",\n",
      "        \"ExtraTreesGini_BAG_L1_1\",\n",
      "        \"ExtraTreesGini_BAG_L1_2\",\n",
      "        \"ExtraTreesGini_BAG_L1_3\",\n",
      "        \"ExtraTreesGini_BAG_L1_4\",\n",
      "        \"ExtraTreesGini_BAG_L1_5\",\n",
      "        \"ExtraTreesGini_BAG_L1_6\",\n",
      "        \"ExtraTreesGini_BAG_L1_7\",\n",
      "        \"ExtraTreesGini_BAG_L1_8\",\n",
      "        \"ExtraTreesGini_BAG_L1_9\",\n",
      "        \"ExtraTreesGini_BAG_L1_10\",\n",
      "        \"ExtraTreesGini_BAG_L1_11\",\n",
      "        \"ExtraTreesGini_BAG_L1_12\",\n",
      "        \"ExtraTreesGini_BAG_L1_13\",\n",
      "        \"ExtraTreesGini_BAG_L1_14\",\n",
      "        \"ExtraTreesGini_BAG_L1_15\",\n",
      "        \"ExtraTreesGini_BAG_L1_16\",\n",
      "        \"ExtraTreesGini_BAG_L1_17\",\n",
      "        \"ExtraTreesGini_BAG_L1_18\",\n",
      "        \"ExtraTreesGini_BAG_L1_19\",\n",
      "        \"ExtraTreesGini_BAG_L1_20\",\n",
      "        \"ExtraTreesGini_BAG_L1_21\",\n",
      "        \"ExtraTreesGini_BAG_L1_22\",\n",
      "        \"ExtraTreesGini_BAG_L1_23\",\n",
      "        \"ExtraTreesGini_BAG_L1_24\",\n",
      "        \"ExtraTreesGini_BAG_L1_25\",\n",
      "        \"ExtraTreesGini_BAG_L1_26\",\n",
      "        \"ExtraTreesGini_BAG_L1_27\",\n",
      "        \"ExtraTreesGini_BAG_L1_28\",\n",
      "        \"ExtraTreesGini_BAG_L1_29\",\n",
      "        \"ExtraTreesGini_BAG_L1_30\",\n",
      "        \"ExtraTreesGini_BAG_L1_31\",\n",
      "        \"ExtraTreesGini_BAG_L1_32\",\n",
      "        \"ExtraTreesGini_BAG_L1_33\",\n",
      "        \"ExtraTreesGini_BAG_L1_34\",\n",
      "        \"ExtraTreesGini_BAG_L1_35\",\n",
      "        \"ExtraTreesGini_BAG_L1_36\",\n",
      "        \"ExtraTreesGini_BAG_L1_37\",\n",
      "        \"ExtraTreesGini_BAG_L1_38\",\n",
      "        \"ExtraTreesGini_BAG_L1_39\",\n",
      "        \"ExtraTreesGini_BAG_L1_40\",\n",
      "        \"ExtraTreesGini_BAG_L1_41\",\n",
      "        \"ExtraTreesGini_BAG_L1_42\",\n",
      "        \"ExtraTreesEntr_BAG_L1_1\",\n",
      "        \"ExtraTreesEntr_BAG_L1_2\",\n",
      "        \"ExtraTreesEntr_BAG_L1_3\",\n",
      "        \"ExtraTreesEntr_BAG_L1_4\",\n",
      "        \"ExtraTreesEntr_BAG_L1_5\",\n",
      "        \"ExtraTreesEntr_BAG_L1_6\",\n",
      "        \"ExtraTreesEntr_BAG_L1_7\",\n",
      "        \"ExtraTreesEntr_BAG_L1_8\",\n",
      "        \"ExtraTreesEntr_BAG_L1_9\",\n",
      "        \"ExtraTreesEntr_BAG_L1_10\",\n",
      "        \"ExtraTreesEntr_BAG_L1_11\",\n",
      "        \"ExtraTreesEntr_BAG_L1_12\",\n",
      "        \"ExtraTreesEntr_BAG_L1_13\",\n",
      "        \"ExtraTreesEntr_BAG_L1_14\",\n",
      "        \"ExtraTreesEntr_BAG_L1_15\",\n",
      "        \"ExtraTreesEntr_BAG_L1_16\",\n",
      "        \"ExtraTreesEntr_BAG_L1_17\",\n",
      "        \"ExtraTreesEntr_BAG_L1_18\",\n",
      "        \"ExtraTreesEntr_BAG_L1_19\",\n",
      "        \"ExtraTreesEntr_BAG_L1_20\",\n",
      "        \"ExtraTreesEntr_BAG_L1_21\",\n",
      "        \"ExtraTreesEntr_BAG_L1_22\",\n",
      "        \"ExtraTreesEntr_BAG_L1_23\",\n",
      "        \"ExtraTreesEntr_BAG_L1_24\",\n",
      "        \"ExtraTreesEntr_BAG_L1_25\",\n",
      "        \"ExtraTreesEntr_BAG_L1_26\",\n",
      "        \"ExtraTreesEntr_BAG_L1_27\",\n",
      "        \"ExtraTreesEntr_BAG_L1_28\",\n",
      "        \"ExtraTreesEntr_BAG_L1_29\",\n",
      "        \"ExtraTreesEntr_BAG_L1_30\",\n",
      "        \"ExtraTreesEntr_BAG_L1_31\",\n",
      "        \"ExtraTreesEntr_BAG_L1_32\",\n",
      "        \"ExtraTreesEntr_BAG_L1_33\",\n",
      "        \"ExtraTreesEntr_BAG_L1_34\",\n",
      "        \"ExtraTreesEntr_BAG_L1_35\",\n",
      "        \"ExtraTreesEntr_BAG_L1_36\",\n",
      "        \"ExtraTreesEntr_BAG_L1_37\",\n",
      "        \"ExtraTreesEntr_BAG_L1_38\",\n",
      "        \"ExtraTreesEntr_BAG_L1_39\",\n",
      "        \"ExtraTreesEntr_BAG_L1_40\",\n",
      "        \"ExtraTreesEntr_BAG_L1_41\",\n",
      "        \"ExtraTreesEntr_BAG_L1_42\",\n",
      "        \"NeuralNetTorch_BAG_L1_0\",\n",
      "        \"NeuralNetTorch_BAG_L1_1\",\n",
      "        \"NeuralNetTorch_BAG_L1_2\",\n",
      "        \"NeuralNetTorch_BAG_L1_3\",\n",
      "        \"NeuralNetTorch_BAG_L1_4\",\n",
      "        \"NeuralNetTorch_BAG_L1_5\",\n",
      "        \"NeuralNetTorch_BAG_L1_6\",\n",
      "        \"NeuralNetTorch_BAG_L1_7\",\n",
      "        \"NeuralNetTorch_BAG_L1_8\",\n",
      "        \"NeuralNetTorch_BAG_L1_9\",\n",
      "        \"NeuralNetTorch_BAG_L1_10\",\n",
      "        \"NeuralNetTorch_BAG_L1_11\",\n",
      "        \"NeuralNetTorch_BAG_L1_12\",\n",
      "        \"NeuralNetTorch_BAG_L1_13\",\n",
      "        \"NeuralNetTorch_BAG_L1_14\",\n",
      "        \"NeuralNetTorch_BAG_L1_15\",\n",
      "        \"NeuralNetTorch_BAG_L1_16\",\n",
      "        \"NeuralNetTorch_BAG_L1_17\",\n",
      "        \"NeuralNetTorch_BAG_L1_18\",\n",
      "        \"NeuralNetTorch_BAG_L1_19\",\n",
      "        \"NeuralNetTorch_BAG_L1_20\",\n",
      "        \"NeuralNetTorch_BAG_L1_21\",\n",
      "        \"NeuralNetTorch_BAG_L1_22\",\n",
      "        \"NeuralNetTorch_BAG_L1_23\",\n",
      "        \"NeuralNetTorch_BAG_L1_24\",\n",
      "        \"NeuralNetTorch_BAG_L1_25\",\n",
      "        \"NeuralNetTorch_BAG_L1_26\",\n",
      "        \"NeuralNetTorch_BAG_L1_27\",\n",
      "        \"NeuralNetTorch_BAG_L1_28\",\n",
      "        \"NeuralNetTorch_BAG_L1_29\",\n",
      "        \"NeuralNetTorch_BAG_L1_30\",\n",
      "        \"NeuralNetTorch_BAG_L1_31\",\n",
      "        \"NeuralNetTorch_BAG_L1_32\",\n",
      "        \"NeuralNetTorch_BAG_L1_33\",\n",
      "        \"NeuralNetTorch_BAG_L1_34\",\n",
      "        \"NeuralNetTorch_BAG_L1_35\",\n",
      "        \"NeuralNetTorch_BAG_L1_36\",\n",
      "        \"NeuralNetTorch_BAG_L1_37\",\n",
      "        \"NeuralNetTorch_BAG_L1_38\",\n",
      "        \"NeuralNetTorch_BAG_L1_39\",\n",
      "        \"NeuralNetTorch_BAG_L1_40\",\n",
      "        \"NeuralNetTorch_BAG_L1_41\",\n",
      "        \"NeuralNetTorch_BAG_L1_42\",\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f212\",\n",
      "        \"f213\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f349\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f441\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f468\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f639\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [\n",
      "        \"KNeighborsUnif_BAG_L1_18\",\n",
      "        \"KNeighborsUnif_BAG_L1_19\",\n",
      "        \"KNeighborsUnif_BAG_L1_20\",\n",
      "        \"KNeighborsUnif_BAG_L1_21\",\n",
      "        \"KNeighborsUnif_BAG_L1_22\",\n",
      "        \"KNeighborsUnif_BAG_L1_23\",\n",
      "        \"KNeighborsUnif_BAG_L1_24\",\n",
      "        \"KNeighborsUnif_BAG_L1_25\",\n",
      "        \"KNeighborsUnif_BAG_L1_26\",\n",
      "        \"KNeighborsUnif_BAG_L1_27\",\n",
      "        \"KNeighborsUnif_BAG_L1_28\",\n",
      "        \"KNeighborsUnif_BAG_L1_29\",\n",
      "        \"KNeighborsUnif_BAG_L1_30\",\n",
      "        \"KNeighborsUnif_BAG_L1_31\",\n",
      "        \"KNeighborsUnif_BAG_L1_32\",\n",
      "        \"KNeighborsUnif_BAG_L1_33\",\n",
      "        \"KNeighborsUnif_BAG_L1_35\",\n",
      "        \"KNeighborsUnif_BAG_L1_36\",\n",
      "        \"KNeighborsUnif_BAG_L1_37\",\n",
      "        \"KNeighborsUnif_BAG_L1_38\",\n",
      "        \"KNeighborsUnif_BAG_L1_39\",\n",
      "        \"KNeighborsUnif_BAG_L1_40\",\n",
      "        \"KNeighborsUnif_BAG_L1_41\",\n",
      "        \"KNeighborsUnif_BAG_L1_42\"\n",
      "    ],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f678\",\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92146 examples, 1061 features (1042 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(102, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(22, 9)\n",
      "    (16): Embedding(25, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1441, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.5954, Val mean_absolute_error: -0.6814, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5155, Val mean_absolute_error: -0.6814, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5116, Val mean_absolute_error: -0.6814, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.5082, Val mean_absolute_error: -0.6814, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.505, Val mean_absolute_error: -0.6814, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.5038, Val mean_absolute_error: -0.6814, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.5013, Val mean_absolute_error: -0.6814, Best Epoch: 7\n",
      "Epoch 8 (Update 5752).\tTrain loss: 0.4993, Val mean_absolute_error: -0.6814, Best Epoch: 8\n",
      "Epoch 9 (Update 6471).\tTrain loss: 0.4974, Val mean_absolute_error: -0.6814, Best Epoch: 9\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 9)\n",
      "Best model found on Epoch 9 (Update 6471). Val mean_absolute_error: -0.6813796247056142\n",
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"RandomForestEntr_BAG_L1_0\",\n",
      "        \"ExtraTreesGini_BAG_L1_0\",\n",
      "        \"ExtraTreesEntr_BAG_L1_0\",\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f91\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f647\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"KNeighborsUnif_BAG_L1_0\",\n",
      "        \"KNeighborsUnif_BAG_L1_1\",\n",
      "        \"KNeighborsUnif_BAG_L1_2\",\n",
      "        \"KNeighborsUnif_BAG_L1_3\",\n",
      "        \"KNeighborsUnif_BAG_L1_4\",\n",
      "        \"KNeighborsUnif_BAG_L1_5\",\n",
      "        \"KNeighborsUnif_BAG_L1_6\",\n",
      "        \"KNeighborsUnif_BAG_L1_7\",\n",
      "        \"KNeighborsUnif_BAG_L1_8\",\n",
      "        \"KNeighborsUnif_BAG_L1_9\",\n",
      "        \"KNeighborsUnif_BAG_L1_10\",\n",
      "        \"KNeighborsUnif_BAG_L1_11\",\n",
      "        \"KNeighborsUnif_BAG_L1_12\",\n",
      "        \"KNeighborsUnif_BAG_L1_13\",\n",
      "        \"KNeighborsUnif_BAG_L1_14\",\n",
      "        \"KNeighborsUnif_BAG_L1_15\",\n",
      "        \"KNeighborsUnif_BAG_L1_16\",\n",
      "        \"KNeighborsUnif_BAG_L1_17\",\n",
      "        \"KNeighborsUnif_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_0\",\n",
      "        \"KNeighborsDist_BAG_L1_1\",\n",
      "        \"KNeighborsDist_BAG_L1_2\",\n",
      "        \"KNeighborsDist_BAG_L1_3\",\n",
      "        \"KNeighborsDist_BAG_L1_4\",\n",
      "        \"KNeighborsDist_BAG_L1_5\",\n",
      "        \"KNeighborsDist_BAG_L1_6\",\n",
      "        \"KNeighborsDist_BAG_L1_7\",\n",
      "        \"KNeighborsDist_BAG_L1_8\",\n",
      "        \"KNeighborsDist_BAG_L1_9\",\n",
      "        \"KNeighborsDist_BAG_L1_10\",\n",
      "        \"KNeighborsDist_BAG_L1_11\",\n",
      "        \"KNeighborsDist_BAG_L1_12\",\n",
      "        \"KNeighborsDist_BAG_L1_13\",\n",
      "        \"KNeighborsDist_BAG_L1_14\",\n",
      "        \"KNeighborsDist_BAG_L1_15\",\n",
      "        \"KNeighborsDist_BAG_L1_16\",\n",
      "        \"KNeighborsDist_BAG_L1_17\",\n",
      "        \"KNeighborsDist_BAG_L1_18\",\n",
      "        \"KNeighborsDist_BAG_L1_19\",\n",
      "        \"KNeighborsDist_BAG_L1_20\",\n",
      "        \"KNeighborsDist_BAG_L1_21\",\n",
      "        \"KNeighborsDist_BAG_L1_22\",\n",
      "        \"KNeighborsDist_BAG_L1_23\",\n",
      "        \"KNeighborsDist_BAG_L1_24\",\n",
      "        \"KNeighborsDist_BAG_L1_25\",\n",
      "        \"KNeighborsDist_BAG_L1_26\",\n",
      "        \"KNeighborsDist_BAG_L1_27\",\n",
      "        \"KNeighborsDist_BAG_L1_28\",\n",
      "        \"KNeighborsDist_BAG_L1_29\",\n",
      "        \"KNeighborsDist_BAG_L1_30\",\n",
      "        \"KNeighborsDist_BAG_L1_31\",\n",
      "        \"KNeighborsDist_BAG_L1_32\",\n",
      "        \"KNeighborsDist_BAG_L1_33\",\n",
      "        \"KNeighborsDist_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_35\",\n",
      "        \"KNeighborsDist_BAG_L1_36\",\n",
      "        \"KNeighborsDist_BAG_L1_37\",\n",
      "        \"KNeighborsDist_BAG_L1_38\",\n",
      "        \"KNeighborsDist_BAG_L1_39\",\n",
      "        \"KNeighborsDist_BAG_L1_40\",\n",
      "        \"KNeighborsDist_BAG_L1_41\",\n",
      "        \"KNeighborsDist_BAG_L1_42\",\n",
      "        \"RandomForestGini_BAG_L1_0\",\n",
      "        \"RandomForestGini_BAG_L1_1\",\n",
      "        \"RandomForestGini_BAG_L1_2\",\n",
      "        \"RandomForestGini_BAG_L1_3\",\n",
      "        \"RandomForestGini_BAG_L1_4\",\n",
      "        \"RandomForestGini_BAG_L1_5\",\n",
      "        \"RandomForestGini_BAG_L1_6\",\n",
      "        \"RandomForestGini_BAG_L1_7\",\n",
      "        \"RandomForestGini_BAG_L1_8\",\n",
      "        \"RandomForestGini_BAG_L1_9\",\n",
      "        \"RandomForestGini_BAG_L1_10\",\n",
      "        \"RandomForestGini_BAG_L1_11\",\n",
      "        \"RandomForestGini_BAG_L1_12\",\n",
      "        \"RandomForestGini_BAG_L1_13\",\n",
      "        \"RandomForestGini_BAG_L1_14\",\n",
      "        \"RandomForestGini_BAG_L1_15\",\n",
      "        \"RandomForestGini_BAG_L1_16\",\n",
      "        \"RandomForestGini_BAG_L1_17\",\n",
      "        \"RandomForestGini_BAG_L1_18\",\n",
      "        \"RandomForestGini_BAG_L1_19\",\n",
      "        \"RandomForestGini_BAG_L1_20\",\n",
      "        \"RandomForestGini_BAG_L1_21\",\n",
      "        \"RandomForestGini_BAG_L1_22\",\n",
      "        \"RandomForestGini_BAG_L1_23\",\n",
      "        \"RandomForestGini_BAG_L1_24\",\n",
      "        \"RandomForestGini_BAG_L1_25\",\n",
      "        \"RandomForestGini_BAG_L1_26\",\n",
      "        \"RandomForestGini_BAG_L1_27\",\n",
      "        \"RandomForestGini_BAG_L1_28\",\n",
      "        \"RandomForestGini_BAG_L1_29\",\n",
      "        \"RandomForestGini_BAG_L1_30\",\n",
      "        \"RandomForestGini_BAG_L1_31\",\n",
      "        \"RandomForestGini_BAG_L1_32\",\n",
      "        \"RandomForestGini_BAG_L1_33\",\n",
      "        \"RandomForestGini_BAG_L1_34\",\n",
      "        \"RandomForestGini_BAG_L1_35\",\n",
      "        \"RandomForestGini_BAG_L1_36\",\n",
      "        \"RandomForestGini_BAG_L1_37\",\n",
      "        \"RandomForestGini_BAG_L1_38\",\n",
      "        \"RandomForestGini_BAG_L1_39\",\n",
      "        \"RandomForestGini_BAG_L1_40\",\n",
      "        \"RandomForestGini_BAG_L1_41\",\n",
      "        \"RandomForestGini_BAG_L1_42\",\n",
      "        \"RandomForestEntr_BAG_L1_1\",\n",
      "        \"RandomForestEntr_BAG_L1_2\",\n",
      "        \"RandomForestEntr_BAG_L1_3\",\n",
      "        \"RandomForestEntr_BAG_L1_4\",\n",
      "        \"RandomForestEntr_BAG_L1_5\",\n",
      "        \"RandomForestEntr_BAG_L1_6\",\n",
      "        \"RandomForestEntr_BAG_L1_7\",\n",
      "        \"RandomForestEntr_BAG_L1_8\",\n",
      "        \"RandomForestEntr_BAG_L1_9\",\n",
      "        \"RandomForestEntr_BAG_L1_10\",\n",
      "        \"RandomForestEntr_BAG_L1_11\",\n",
      "        \"RandomForestEntr_BAG_L1_12\",\n",
      "        \"RandomForestEntr_BAG_L1_13\",\n",
      "        \"RandomForestEntr_BAG_L1_14\",\n",
      "        \"RandomForestEntr_BAG_L1_15\",\n",
      "        \"RandomForestEntr_BAG_L1_16\",\n",
      "        \"RandomForestEntr_BAG_L1_17\",\n",
      "        \"RandomForestEntr_BAG_L1_18\",\n",
      "        \"RandomForestEntr_BAG_L1_19\",\n",
      "        \"RandomForestEntr_BAG_L1_20\",\n",
      "        \"RandomForestEntr_BAG_L1_21\",\n",
      "        \"RandomForestEntr_BAG_L1_22\",\n",
      "        \"RandomForestEntr_BAG_L1_23\",\n",
      "        \"RandomForestEntr_BAG_L1_24\",\n",
      "        \"RandomForestEntr_BAG_L1_25\",\n",
      "        \"RandomForestEntr_BAG_L1_26\",\n",
      "        \"RandomForestEntr_BAG_L1_27\",\n",
      "        \"RandomForestEntr_BAG_L1_28\",\n",
      "        \"RandomForestEntr_BAG_L1_29\",\n",
      "        \"RandomForestEntr_BAG_L1_30\",\n",
      "        \"RandomForestEntr_BAG_L1_31\",\n",
      "        \"RandomForestEntr_BAG_L1_32\",\n",
      "        \"RandomForestEntr_BAG_L1_33\",\n",
      "        \"RandomForestEntr_BAG_L1_34\",\n",
      "        \"RandomForestEntr_BAG_L1_35\",\n",
      "        \"RandomForestEntr_BAG_L1_36\",\n",
      "        \"RandomForestEntr_BAG_L1_37\",\n",
      "        \"RandomForestEntr_BAG_L1_38\",\n",
      "        \"RandomForestEntr_BAG_L1_39\",\n",
      "        \"RandomForestEntr_BAG_L1_40\",\n",
      "        \"RandomForestEntr_BAG_L1_41\",\n",
      "        \"RandomForestEntr_BAG_L1_42\",\n",
      "        \"ExtraTreesGini_BAG_L1_1\",\n",
      "        \"ExtraTreesGini_BAG_L1_2\",\n",
      "        \"ExtraTreesGini_BAG_L1_3\",\n",
      "        \"ExtraTreesGini_BAG_L1_4\",\n",
      "        \"ExtraTreesGini_BAG_L1_5\",\n",
      "        \"ExtraTreesGini_BAG_L1_6\",\n",
      "        \"ExtraTreesGini_BAG_L1_7\",\n",
      "        \"ExtraTreesGini_BAG_L1_8\",\n",
      "        \"ExtraTreesGini_BAG_L1_9\",\n",
      "        \"ExtraTreesGini_BAG_L1_10\",\n",
      "        \"ExtraTreesGini_BAG_L1_11\",\n",
      "        \"ExtraTreesGini_BAG_L1_12\",\n",
      "        \"ExtraTreesGini_BAG_L1_13\",\n",
      "        \"ExtraTreesGini_BAG_L1_14\",\n",
      "        \"ExtraTreesGini_BAG_L1_15\",\n",
      "        \"ExtraTreesGini_BAG_L1_16\",\n",
      "        \"ExtraTreesGini_BAG_L1_17\",\n",
      "        \"ExtraTreesGini_BAG_L1_18\",\n",
      "        \"ExtraTreesGini_BAG_L1_19\",\n",
      "        \"ExtraTreesGini_BAG_L1_20\",\n",
      "        \"ExtraTreesGini_BAG_L1_21\",\n",
      "        \"ExtraTreesGini_BAG_L1_22\",\n",
      "        \"ExtraTreesGini_BAG_L1_23\",\n",
      "        \"ExtraTreesGini_BAG_L1_24\",\n",
      "        \"ExtraTreesGini_BAG_L1_25\",\n",
      "        \"ExtraTreesGini_BAG_L1_26\",\n",
      "        \"ExtraTreesGini_BAG_L1_27\",\n",
      "        \"ExtraTreesGini_BAG_L1_28\",\n",
      "        \"ExtraTreesGini_BAG_L1_29\",\n",
      "        \"ExtraTreesGini_BAG_L1_30\",\n",
      "        \"ExtraTreesGini_BAG_L1_31\",\n",
      "        \"ExtraTreesGini_BAG_L1_32\",\n",
      "        \"ExtraTreesGini_BAG_L1_33\",\n",
      "        \"ExtraTreesGini_BAG_L1_34\",\n",
      "        \"ExtraTreesGini_BAG_L1_35\",\n",
      "        \"ExtraTreesGini_BAG_L1_36\",\n",
      "        \"ExtraTreesGini_BAG_L1_37\",\n",
      "        \"ExtraTreesGini_BAG_L1_38\",\n",
      "        \"ExtraTreesGini_BAG_L1_39\",\n",
      "        \"ExtraTreesGini_BAG_L1_40\",\n",
      "        \"ExtraTreesGini_BAG_L1_41\",\n",
      "        \"ExtraTreesGini_BAG_L1_42\",\n",
      "        \"ExtraTreesEntr_BAG_L1_1\",\n",
      "        \"ExtraTreesEntr_BAG_L1_2\",\n",
      "        \"ExtraTreesEntr_BAG_L1_3\",\n",
      "        \"ExtraTreesEntr_BAG_L1_4\",\n",
      "        \"ExtraTreesEntr_BAG_L1_5\",\n",
      "        \"ExtraTreesEntr_BAG_L1_6\",\n",
      "        \"ExtraTreesEntr_BAG_L1_7\",\n",
      "        \"ExtraTreesEntr_BAG_L1_8\",\n",
      "        \"ExtraTreesEntr_BAG_L1_9\",\n",
      "        \"ExtraTreesEntr_BAG_L1_10\",\n",
      "        \"ExtraTreesEntr_BAG_L1_11\",\n",
      "        \"ExtraTreesEntr_BAG_L1_12\",\n",
      "        \"ExtraTreesEntr_BAG_L1_13\",\n",
      "        \"ExtraTreesEntr_BAG_L1_14\",\n",
      "        \"ExtraTreesEntr_BAG_L1_15\",\n",
      "        \"ExtraTreesEntr_BAG_L1_16\",\n",
      "        \"ExtraTreesEntr_BAG_L1_17\",\n",
      "        \"ExtraTreesEntr_BAG_L1_18\",\n",
      "        \"ExtraTreesEntr_BAG_L1_19\",\n",
      "        \"ExtraTreesEntr_BAG_L1_20\",\n",
      "        \"ExtraTreesEntr_BAG_L1_21\",\n",
      "        \"ExtraTreesEntr_BAG_L1_22\",\n",
      "        \"ExtraTreesEntr_BAG_L1_23\",\n",
      "        \"ExtraTreesEntr_BAG_L1_24\",\n",
      "        \"ExtraTreesEntr_BAG_L1_25\",\n",
      "        \"ExtraTreesEntr_BAG_L1_26\",\n",
      "        \"ExtraTreesEntr_BAG_L1_27\",\n",
      "        \"ExtraTreesEntr_BAG_L1_28\",\n",
      "        \"ExtraTreesEntr_BAG_L1_29\",\n",
      "        \"ExtraTreesEntr_BAG_L1_30\",\n",
      "        \"ExtraTreesEntr_BAG_L1_31\",\n",
      "        \"ExtraTreesEntr_BAG_L1_32\",\n",
      "        \"ExtraTreesEntr_BAG_L1_33\",\n",
      "        \"ExtraTreesEntr_BAG_L1_34\",\n",
      "        \"ExtraTreesEntr_BAG_L1_35\",\n",
      "        \"ExtraTreesEntr_BAG_L1_36\",\n",
      "        \"ExtraTreesEntr_BAG_L1_37\",\n",
      "        \"ExtraTreesEntr_BAG_L1_38\",\n",
      "        \"ExtraTreesEntr_BAG_L1_39\",\n",
      "        \"ExtraTreesEntr_BAG_L1_40\",\n",
      "        \"ExtraTreesEntr_BAG_L1_41\",\n",
      "        \"ExtraTreesEntr_BAG_L1_42\",\n",
      "        \"NeuralNetTorch_BAG_L1_0\",\n",
      "        \"NeuralNetTorch_BAG_L1_1\",\n",
      "        \"NeuralNetTorch_BAG_L1_2\",\n",
      "        \"NeuralNetTorch_BAG_L1_3\",\n",
      "        \"NeuralNetTorch_BAG_L1_4\",\n",
      "        \"NeuralNetTorch_BAG_L1_5\",\n",
      "        \"NeuralNetTorch_BAG_L1_6\",\n",
      "        \"NeuralNetTorch_BAG_L1_7\",\n",
      "        \"NeuralNetTorch_BAG_L1_8\",\n",
      "        \"NeuralNetTorch_BAG_L1_9\",\n",
      "        \"NeuralNetTorch_BAG_L1_10\",\n",
      "        \"NeuralNetTorch_BAG_L1_11\",\n",
      "        \"NeuralNetTorch_BAG_L1_12\",\n",
      "        \"NeuralNetTorch_BAG_L1_13\",\n",
      "        \"NeuralNetTorch_BAG_L1_14\",\n",
      "        \"NeuralNetTorch_BAG_L1_15\",\n",
      "        \"NeuralNetTorch_BAG_L1_16\",\n",
      "        \"NeuralNetTorch_BAG_L1_17\",\n",
      "        \"NeuralNetTorch_BAG_L1_18\",\n",
      "        \"NeuralNetTorch_BAG_L1_19\",\n",
      "        \"NeuralNetTorch_BAG_L1_20\",\n",
      "        \"NeuralNetTorch_BAG_L1_21\",\n",
      "        \"NeuralNetTorch_BAG_L1_22\",\n",
      "        \"NeuralNetTorch_BAG_L1_23\",\n",
      "        \"NeuralNetTorch_BAG_L1_24\",\n",
      "        \"NeuralNetTorch_BAG_L1_25\",\n",
      "        \"NeuralNetTorch_BAG_L1_26\",\n",
      "        \"NeuralNetTorch_BAG_L1_27\",\n",
      "        \"NeuralNetTorch_BAG_L1_28\",\n",
      "        \"NeuralNetTorch_BAG_L1_29\",\n",
      "        \"NeuralNetTorch_BAG_L1_30\",\n",
      "        \"NeuralNetTorch_BAG_L1_31\",\n",
      "        \"NeuralNetTorch_BAG_L1_32\",\n",
      "        \"NeuralNetTorch_BAG_L1_33\",\n",
      "        \"NeuralNetTorch_BAG_L1_34\",\n",
      "        \"NeuralNetTorch_BAG_L1_35\",\n",
      "        \"NeuralNetTorch_BAG_L1_36\",\n",
      "        \"NeuralNetTorch_BAG_L1_37\",\n",
      "        \"NeuralNetTorch_BAG_L1_38\",\n",
      "        \"NeuralNetTorch_BAG_L1_39\",\n",
      "        \"NeuralNetTorch_BAG_L1_40\",\n",
      "        \"NeuralNetTorch_BAG_L1_41\",\n",
      "        \"NeuralNetTorch_BAG_L1_42\",\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f211\",\n",
      "        \"f212\",\n",
      "        \"f213\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f349\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f441\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f468\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f609\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f638\",\n",
      "        \"f639\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [\n",
      "        \"KNeighborsUnif_BAG_L1_18\",\n",
      "        \"KNeighborsUnif_BAG_L1_19\",\n",
      "        \"KNeighborsUnif_BAG_L1_20\",\n",
      "        \"KNeighborsUnif_BAG_L1_21\",\n",
      "        \"KNeighborsUnif_BAG_L1_22\",\n",
      "        \"KNeighborsUnif_BAG_L1_23\",\n",
      "        \"KNeighborsUnif_BAG_L1_24\",\n",
      "        \"KNeighborsUnif_BAG_L1_25\",\n",
      "        \"KNeighborsUnif_BAG_L1_26\",\n",
      "        \"KNeighborsUnif_BAG_L1_27\",\n",
      "        \"KNeighborsUnif_BAG_L1_28\",\n",
      "        \"KNeighborsUnif_BAG_L1_29\",\n",
      "        \"KNeighborsUnif_BAG_L1_30\",\n",
      "        \"KNeighborsUnif_BAG_L1_31\",\n",
      "        \"KNeighborsUnif_BAG_L1_32\",\n",
      "        \"KNeighborsUnif_BAG_L1_33\",\n",
      "        \"KNeighborsUnif_BAG_L1_35\",\n",
      "        \"KNeighborsUnif_BAG_L1_36\",\n",
      "        \"KNeighborsUnif_BAG_L1_37\",\n",
      "        \"KNeighborsUnif_BAG_L1_38\",\n",
      "        \"KNeighborsUnif_BAG_L1_39\",\n",
      "        \"KNeighborsUnif_BAG_L1_40\",\n",
      "        \"KNeighborsUnif_BAG_L1_41\",\n",
      "        \"KNeighborsUnif_BAG_L1_42\"\n",
      "    ],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f678\",\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92146 examples, 1061 features (1042 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(101, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(20, 8)\n",
      "    (16): Embedding(23, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1440, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.6019, Val mean_absolute_error: -0.6835, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5155, Val mean_absolute_error: -0.6835, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5107, Val mean_absolute_error: -0.6835, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.5079, Val mean_absolute_error: -0.6835, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.5061, Val mean_absolute_error: -0.6835, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.5016, Val mean_absolute_error: -0.6835, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.5025, Val mean_absolute_error: -0.6835, Best Epoch: 7\n",
      "Epoch 8 (Update 5752).\tTrain loss: 0.5004, Val mean_absolute_error: -0.6835, Best Epoch: 8\n",
      "Epoch 9 (Update 6471).\tTrain loss: 0.498, Val mean_absolute_error: -0.6835, Best Epoch: 9\n",
      "Epoch 10 (Update 7190).\tTrain loss: 0.4964, Val mean_absolute_error: -0.6835, Best Epoch: 10\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 10)\n",
      "Best model found on Epoch 10 (Update 7190). Val mean_absolute_error: -0.6835067993618476\n",
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"RandomForestEntr_BAG_L1_0\",\n",
      "        \"ExtraTreesGini_BAG_L1_0\",\n",
      "        \"ExtraTreesEntr_BAG_L1_0\",\n",
      "        \"id\",\n",
      "        \"f1\",\n",
      "        \"f3\",\n",
      "        \"f4\",\n",
      "        \"f5\",\n",
      "        \"f6\",\n",
      "        \"f7\",\n",
      "        \"f9\",\n",
      "        \"f10\",\n",
      "        \"f13\",\n",
      "        \"f19\",\n",
      "        \"f25\",\n",
      "        \"f26\",\n",
      "        \"f27\",\n",
      "        \"f43\",\n",
      "        \"f53\",\n",
      "        \"f55\",\n",
      "        \"f56\",\n",
      "        \"f57\",\n",
      "        \"f67\",\n",
      "        \"f68\",\n",
      "        \"f69\",\n",
      "        \"f70\",\n",
      "        \"f75\",\n",
      "        \"f78\",\n",
      "        \"f79\",\n",
      "        \"f90\",\n",
      "        \"f91\",\n",
      "        \"f100\",\n",
      "        \"f105\",\n",
      "        \"f106\",\n",
      "        \"f107\",\n",
      "        \"f108\",\n",
      "        \"f114\",\n",
      "        \"f120\",\n",
      "        \"f122\",\n",
      "        \"f123\",\n",
      "        \"f124\",\n",
      "        \"f140\",\n",
      "        \"f141\",\n",
      "        \"f144\",\n",
      "        \"f158\",\n",
      "        \"f168\",\n",
      "        \"f170\",\n",
      "        \"f180\",\n",
      "        \"f191\",\n",
      "        \"f192\",\n",
      "        \"f193\",\n",
      "        \"f209\",\n",
      "        \"f219\",\n",
      "        \"f228\",\n",
      "        \"f229\",\n",
      "        \"f249\",\n",
      "        \"f253\",\n",
      "        \"f259\",\n",
      "        \"f261\",\n",
      "        \"f262\",\n",
      "        \"f263\",\n",
      "        \"f279\",\n",
      "        \"f280\",\n",
      "        \"f297\",\n",
      "        \"f305\",\n",
      "        \"f313\",\n",
      "        \"f322\",\n",
      "        \"f323\",\n",
      "        \"f324\",\n",
      "        \"f341\",\n",
      "        \"f342\",\n",
      "        \"f343\",\n",
      "        \"f356\",\n",
      "        \"f359\",\n",
      "        \"f365\",\n",
      "        \"f373\",\n",
      "        \"f376\",\n",
      "        \"f377\",\n",
      "        \"f378\",\n",
      "        \"f383\",\n",
      "        \"f403\",\n",
      "        \"f404\",\n",
      "        \"f405\",\n",
      "        \"f406\",\n",
      "        \"f412\",\n",
      "        \"f422\",\n",
      "        \"f432\",\n",
      "        \"f437\",\n",
      "        \"f438\",\n",
      "        \"f439\",\n",
      "        \"f440\",\n",
      "        \"f442\",\n",
      "        \"f445\",\n",
      "        \"f446\",\n",
      "        \"f447\",\n",
      "        \"f449\",\n",
      "        \"f450\",\n",
      "        \"f457\",\n",
      "        \"f458\",\n",
      "        \"f467\",\n",
      "        \"f478\",\n",
      "        \"f479\",\n",
      "        \"f488\",\n",
      "        \"f489\",\n",
      "        \"f498\",\n",
      "        \"f499\",\n",
      "        \"f503\",\n",
      "        \"f504\",\n",
      "        \"f505\",\n",
      "        \"f508\",\n",
      "        \"f509\",\n",
      "        \"f511\",\n",
      "        \"f514\",\n",
      "        \"f516\",\n",
      "        \"f524\",\n",
      "        \"f533\",\n",
      "        \"f596\",\n",
      "        \"f597\",\n",
      "        \"f598\",\n",
      "        \"f599\",\n",
      "        \"f606\",\n",
      "        \"f607\",\n",
      "        \"f608\",\n",
      "        \"f610\",\n",
      "        \"f611\",\n",
      "        \"f640\",\n",
      "        \"f645\",\n",
      "        \"f646\",\n",
      "        \"f647\",\n",
      "        \"f653\",\n",
      "        \"f662\",\n",
      "        \"f663\",\n",
      "        \"f664\",\n",
      "        \"f669\",\n",
      "        \"f675\",\n",
      "        \"f676\",\n",
      "        \"f677\",\n",
      "        \"f696\",\n",
      "        \"f716\",\n",
      "        \"f717\",\n",
      "        \"f725\",\n",
      "        \"f727\",\n",
      "        \"f733\",\n",
      "        \"f734\",\n",
      "        \"f735\",\n",
      "        \"f737\",\n",
      "        \"f738\",\n",
      "        \"f739\",\n",
      "        \"f745\",\n",
      "        \"f746\",\n",
      "        \"f756\",\n",
      "        \"f765\",\n",
      "        \"f766\",\n",
      "        \"f767\",\n",
      "        \"f768\",\n",
      "        \"f774\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"KNeighborsUnif_BAG_L1_0\",\n",
      "        \"KNeighborsUnif_BAG_L1_1\",\n",
      "        \"KNeighborsUnif_BAG_L1_2\",\n",
      "        \"KNeighborsUnif_BAG_L1_3\",\n",
      "        \"KNeighborsUnif_BAG_L1_4\",\n",
      "        \"KNeighborsUnif_BAG_L1_5\",\n",
      "        \"KNeighborsUnif_BAG_L1_6\",\n",
      "        \"KNeighborsUnif_BAG_L1_7\",\n",
      "        \"KNeighborsUnif_BAG_L1_8\",\n",
      "        \"KNeighborsUnif_BAG_L1_9\",\n",
      "        \"KNeighborsUnif_BAG_L1_10\",\n",
      "        \"KNeighborsUnif_BAG_L1_11\",\n",
      "        \"KNeighborsUnif_BAG_L1_12\",\n",
      "        \"KNeighborsUnif_BAG_L1_13\",\n",
      "        \"KNeighborsUnif_BAG_L1_14\",\n",
      "        \"KNeighborsUnif_BAG_L1_15\",\n",
      "        \"KNeighborsUnif_BAG_L1_16\",\n",
      "        \"KNeighborsUnif_BAG_L1_17\",\n",
      "        \"KNeighborsUnif_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_0\",\n",
      "        \"KNeighborsDist_BAG_L1_1\",\n",
      "        \"KNeighborsDist_BAG_L1_2\",\n",
      "        \"KNeighborsDist_BAG_L1_3\",\n",
      "        \"KNeighborsDist_BAG_L1_4\",\n",
      "        \"KNeighborsDist_BAG_L1_5\",\n",
      "        \"KNeighborsDist_BAG_L1_6\",\n",
      "        \"KNeighborsDist_BAG_L1_7\",\n",
      "        \"KNeighborsDist_BAG_L1_8\",\n",
      "        \"KNeighborsDist_BAG_L1_9\",\n",
      "        \"KNeighborsDist_BAG_L1_10\",\n",
      "        \"KNeighborsDist_BAG_L1_11\",\n",
      "        \"KNeighborsDist_BAG_L1_12\",\n",
      "        \"KNeighborsDist_BAG_L1_13\",\n",
      "        \"KNeighborsDist_BAG_L1_14\",\n",
      "        \"KNeighborsDist_BAG_L1_15\",\n",
      "        \"KNeighborsDist_BAG_L1_16\",\n",
      "        \"KNeighborsDist_BAG_L1_17\",\n",
      "        \"KNeighborsDist_BAG_L1_18\",\n",
      "        \"KNeighborsDist_BAG_L1_19\",\n",
      "        \"KNeighborsDist_BAG_L1_20\",\n",
      "        \"KNeighborsDist_BAG_L1_21\",\n",
      "        \"KNeighborsDist_BAG_L1_22\",\n",
      "        \"KNeighborsDist_BAG_L1_23\",\n",
      "        \"KNeighborsDist_BAG_L1_24\",\n",
      "        \"KNeighborsDist_BAG_L1_25\",\n",
      "        \"KNeighborsDist_BAG_L1_26\",\n",
      "        \"KNeighborsDist_BAG_L1_27\",\n",
      "        \"KNeighborsDist_BAG_L1_28\",\n",
      "        \"KNeighborsDist_BAG_L1_29\",\n",
      "        \"KNeighborsDist_BAG_L1_30\",\n",
      "        \"KNeighborsDist_BAG_L1_31\",\n",
      "        \"KNeighborsDist_BAG_L1_32\",\n",
      "        \"KNeighborsDist_BAG_L1_33\",\n",
      "        \"KNeighborsDist_BAG_L1_34\",\n",
      "        \"KNeighborsDist_BAG_L1_35\",\n",
      "        \"KNeighborsDist_BAG_L1_36\",\n",
      "        \"KNeighborsDist_BAG_L1_37\",\n",
      "        \"KNeighborsDist_BAG_L1_38\",\n",
      "        \"KNeighborsDist_BAG_L1_39\",\n",
      "        \"KNeighborsDist_BAG_L1_40\",\n",
      "        \"KNeighborsDist_BAG_L1_41\",\n",
      "        \"KNeighborsDist_BAG_L1_42\",\n",
      "        \"RandomForestGini_BAG_L1_0\",\n",
      "        \"RandomForestGini_BAG_L1_1\",\n",
      "        \"RandomForestGini_BAG_L1_2\",\n",
      "        \"RandomForestGini_BAG_L1_3\",\n",
      "        \"RandomForestGini_BAG_L1_4\",\n",
      "        \"RandomForestGini_BAG_L1_5\",\n",
      "        \"RandomForestGini_BAG_L1_6\",\n",
      "        \"RandomForestGini_BAG_L1_7\",\n",
      "        \"RandomForestGini_BAG_L1_8\",\n",
      "        \"RandomForestGini_BAG_L1_9\",\n",
      "        \"RandomForestGini_BAG_L1_10\",\n",
      "        \"RandomForestGini_BAG_L1_11\",\n",
      "        \"RandomForestGini_BAG_L1_12\",\n",
      "        \"RandomForestGini_BAG_L1_13\",\n",
      "        \"RandomForestGini_BAG_L1_14\",\n",
      "        \"RandomForestGini_BAG_L1_15\",\n",
      "        \"RandomForestGini_BAG_L1_16\",\n",
      "        \"RandomForestGini_BAG_L1_17\",\n",
      "        \"RandomForestGini_BAG_L1_18\",\n",
      "        \"RandomForestGini_BAG_L1_19\",\n",
      "        \"RandomForestGini_BAG_L1_20\",\n",
      "        \"RandomForestGini_BAG_L1_21\",\n",
      "        \"RandomForestGini_BAG_L1_22\",\n",
      "        \"RandomForestGini_BAG_L1_23\",\n",
      "        \"RandomForestGini_BAG_L1_24\",\n",
      "        \"RandomForestGini_BAG_L1_25\",\n",
      "        \"RandomForestGini_BAG_L1_26\",\n",
      "        \"RandomForestGini_BAG_L1_27\",\n",
      "        \"RandomForestGini_BAG_L1_28\",\n",
      "        \"RandomForestGini_BAG_L1_29\",\n",
      "        \"RandomForestGini_BAG_L1_30\",\n",
      "        \"RandomForestGini_BAG_L1_31\",\n",
      "        \"RandomForestGini_BAG_L1_32\",\n",
      "        \"RandomForestGini_BAG_L1_33\",\n",
      "        \"RandomForestGini_BAG_L1_34\",\n",
      "        \"RandomForestGini_BAG_L1_35\",\n",
      "        \"RandomForestGini_BAG_L1_36\",\n",
      "        \"RandomForestGini_BAG_L1_37\",\n",
      "        \"RandomForestGini_BAG_L1_38\",\n",
      "        \"RandomForestGini_BAG_L1_39\",\n",
      "        \"RandomForestGini_BAG_L1_40\",\n",
      "        \"RandomForestGini_BAG_L1_41\",\n",
      "        \"RandomForestGini_BAG_L1_42\",\n",
      "        \"RandomForestEntr_BAG_L1_1\",\n",
      "        \"RandomForestEntr_BAG_L1_2\",\n",
      "        \"RandomForestEntr_BAG_L1_3\",\n",
      "        \"RandomForestEntr_BAG_L1_4\",\n",
      "        \"RandomForestEntr_BAG_L1_5\",\n",
      "        \"RandomForestEntr_BAG_L1_6\",\n",
      "        \"RandomForestEntr_BAG_L1_7\",\n",
      "        \"RandomForestEntr_BAG_L1_8\",\n",
      "        \"RandomForestEntr_BAG_L1_9\",\n",
      "        \"RandomForestEntr_BAG_L1_10\",\n",
      "        \"RandomForestEntr_BAG_L1_11\",\n",
      "        \"RandomForestEntr_BAG_L1_12\",\n",
      "        \"RandomForestEntr_BAG_L1_13\",\n",
      "        \"RandomForestEntr_BAG_L1_14\",\n",
      "        \"RandomForestEntr_BAG_L1_15\",\n",
      "        \"RandomForestEntr_BAG_L1_16\",\n",
      "        \"RandomForestEntr_BAG_L1_17\",\n",
      "        \"RandomForestEntr_BAG_L1_18\",\n",
      "        \"RandomForestEntr_BAG_L1_19\",\n",
      "        \"RandomForestEntr_BAG_L1_20\",\n",
      "        \"RandomForestEntr_BAG_L1_21\",\n",
      "        \"RandomForestEntr_BAG_L1_22\",\n",
      "        \"RandomForestEntr_BAG_L1_23\",\n",
      "        \"RandomForestEntr_BAG_L1_24\",\n",
      "        \"RandomForestEntr_BAG_L1_25\",\n",
      "        \"RandomForestEntr_BAG_L1_26\",\n",
      "        \"RandomForestEntr_BAG_L1_27\",\n",
      "        \"RandomForestEntr_BAG_L1_28\",\n",
      "        \"RandomForestEntr_BAG_L1_29\",\n",
      "        \"RandomForestEntr_BAG_L1_30\",\n",
      "        \"RandomForestEntr_BAG_L1_31\",\n",
      "        \"RandomForestEntr_BAG_L1_32\",\n",
      "        \"RandomForestEntr_BAG_L1_33\",\n",
      "        \"RandomForestEntr_BAG_L1_34\",\n",
      "        \"RandomForestEntr_BAG_L1_35\",\n",
      "        \"RandomForestEntr_BAG_L1_36\",\n",
      "        \"RandomForestEntr_BAG_L1_37\",\n",
      "        \"RandomForestEntr_BAG_L1_38\",\n",
      "        \"RandomForestEntr_BAG_L1_39\",\n",
      "        \"RandomForestEntr_BAG_L1_40\",\n",
      "        \"RandomForestEntr_BAG_L1_41\",\n",
      "        \"RandomForestEntr_BAG_L1_42\",\n",
      "        \"ExtraTreesGini_BAG_L1_1\",\n",
      "        \"ExtraTreesGini_BAG_L1_2\",\n",
      "        \"ExtraTreesGini_BAG_L1_3\",\n",
      "        \"ExtraTreesGini_BAG_L1_4\",\n",
      "        \"ExtraTreesGini_BAG_L1_5\",\n",
      "        \"ExtraTreesGini_BAG_L1_6\",\n",
      "        \"ExtraTreesGini_BAG_L1_7\",\n",
      "        \"ExtraTreesGini_BAG_L1_8\",\n",
      "        \"ExtraTreesGini_BAG_L1_9\",\n",
      "        \"ExtraTreesGini_BAG_L1_10\",\n",
      "        \"ExtraTreesGini_BAG_L1_11\",\n",
      "        \"ExtraTreesGini_BAG_L1_12\",\n",
      "        \"ExtraTreesGini_BAG_L1_13\",\n",
      "        \"ExtraTreesGini_BAG_L1_14\",\n",
      "        \"ExtraTreesGini_BAG_L1_15\",\n",
      "        \"ExtraTreesGini_BAG_L1_16\",\n",
      "        \"ExtraTreesGini_BAG_L1_17\",\n",
      "        \"ExtraTreesGini_BAG_L1_18\",\n",
      "        \"ExtraTreesGini_BAG_L1_19\",\n",
      "        \"ExtraTreesGini_BAG_L1_20\",\n",
      "        \"ExtraTreesGini_BAG_L1_21\",\n",
      "        \"ExtraTreesGini_BAG_L1_22\",\n",
      "        \"ExtraTreesGini_BAG_L1_23\",\n",
      "        \"ExtraTreesGini_BAG_L1_24\",\n",
      "        \"ExtraTreesGini_BAG_L1_25\",\n",
      "        \"ExtraTreesGini_BAG_L1_26\",\n",
      "        \"ExtraTreesGini_BAG_L1_27\",\n",
      "        \"ExtraTreesGini_BAG_L1_28\",\n",
      "        \"ExtraTreesGini_BAG_L1_29\",\n",
      "        \"ExtraTreesGini_BAG_L1_30\",\n",
      "        \"ExtraTreesGini_BAG_L1_31\",\n",
      "        \"ExtraTreesGini_BAG_L1_32\",\n",
      "        \"ExtraTreesGini_BAG_L1_33\",\n",
      "        \"ExtraTreesGini_BAG_L1_34\",\n",
      "        \"ExtraTreesGini_BAG_L1_35\",\n",
      "        \"ExtraTreesGini_BAG_L1_36\",\n",
      "        \"ExtraTreesGini_BAG_L1_37\",\n",
      "        \"ExtraTreesGini_BAG_L1_38\",\n",
      "        \"ExtraTreesGini_BAG_L1_39\",\n",
      "        \"ExtraTreesGini_BAG_L1_40\",\n",
      "        \"ExtraTreesGini_BAG_L1_41\",\n",
      "        \"ExtraTreesGini_BAG_L1_42\",\n",
      "        \"ExtraTreesEntr_BAG_L1_1\",\n",
      "        \"ExtraTreesEntr_BAG_L1_2\",\n",
      "        \"ExtraTreesEntr_BAG_L1_3\",\n",
      "        \"ExtraTreesEntr_BAG_L1_4\",\n",
      "        \"ExtraTreesEntr_BAG_L1_5\",\n",
      "        \"ExtraTreesEntr_BAG_L1_6\",\n",
      "        \"ExtraTreesEntr_BAG_L1_7\",\n",
      "        \"ExtraTreesEntr_BAG_L1_8\",\n",
      "        \"ExtraTreesEntr_BAG_L1_9\",\n",
      "        \"ExtraTreesEntr_BAG_L1_10\",\n",
      "        \"ExtraTreesEntr_BAG_L1_11\",\n",
      "        \"ExtraTreesEntr_BAG_L1_12\",\n",
      "        \"ExtraTreesEntr_BAG_L1_13\",\n",
      "        \"ExtraTreesEntr_BAG_L1_14\",\n",
      "        \"ExtraTreesEntr_BAG_L1_15\",\n",
      "        \"ExtraTreesEntr_BAG_L1_16\",\n",
      "        \"ExtraTreesEntr_BAG_L1_17\",\n",
      "        \"ExtraTreesEntr_BAG_L1_18\",\n",
      "        \"ExtraTreesEntr_BAG_L1_19\",\n",
      "        \"ExtraTreesEntr_BAG_L1_20\",\n",
      "        \"ExtraTreesEntr_BAG_L1_21\",\n",
      "        \"ExtraTreesEntr_BAG_L1_22\",\n",
      "        \"ExtraTreesEntr_BAG_L1_23\",\n",
      "        \"ExtraTreesEntr_BAG_L1_24\",\n",
      "        \"ExtraTreesEntr_BAG_L1_25\",\n",
      "        \"ExtraTreesEntr_BAG_L1_26\",\n",
      "        \"ExtraTreesEntr_BAG_L1_27\",\n",
      "        \"ExtraTreesEntr_BAG_L1_28\",\n",
      "        \"ExtraTreesEntr_BAG_L1_29\",\n",
      "        \"ExtraTreesEntr_BAG_L1_30\",\n",
      "        \"ExtraTreesEntr_BAG_L1_31\",\n",
      "        \"ExtraTreesEntr_BAG_L1_32\",\n",
      "        \"ExtraTreesEntr_BAG_L1_33\",\n",
      "        \"ExtraTreesEntr_BAG_L1_34\",\n",
      "        \"ExtraTreesEntr_BAG_L1_35\",\n",
      "        \"ExtraTreesEntr_BAG_L1_36\",\n",
      "        \"ExtraTreesEntr_BAG_L1_37\",\n",
      "        \"ExtraTreesEntr_BAG_L1_38\",\n",
      "        \"ExtraTreesEntr_BAG_L1_39\",\n",
      "        \"ExtraTreesEntr_BAG_L1_40\",\n",
      "        \"ExtraTreesEntr_BAG_L1_41\",\n",
      "        \"ExtraTreesEntr_BAG_L1_42\",\n",
      "        \"NeuralNetTorch_BAG_L1_0\",\n",
      "        \"NeuralNetTorch_BAG_L1_1\",\n",
      "        \"NeuralNetTorch_BAG_L1_2\",\n",
      "        \"NeuralNetTorch_BAG_L1_3\",\n",
      "        \"NeuralNetTorch_BAG_L1_4\",\n",
      "        \"NeuralNetTorch_BAG_L1_5\",\n",
      "        \"NeuralNetTorch_BAG_L1_6\",\n",
      "        \"NeuralNetTorch_BAG_L1_7\",\n",
      "        \"NeuralNetTorch_BAG_L1_8\",\n",
      "        \"NeuralNetTorch_BAG_L1_9\",\n",
      "        \"NeuralNetTorch_BAG_L1_10\",\n",
      "        \"NeuralNetTorch_BAG_L1_11\",\n",
      "        \"NeuralNetTorch_BAG_L1_12\",\n",
      "        \"NeuralNetTorch_BAG_L1_13\",\n",
      "        \"NeuralNetTorch_BAG_L1_14\",\n",
      "        \"NeuralNetTorch_BAG_L1_15\",\n",
      "        \"NeuralNetTorch_BAG_L1_16\",\n",
      "        \"NeuralNetTorch_BAG_L1_17\",\n",
      "        \"NeuralNetTorch_BAG_L1_18\",\n",
      "        \"NeuralNetTorch_BAG_L1_19\",\n",
      "        \"NeuralNetTorch_BAG_L1_20\",\n",
      "        \"NeuralNetTorch_BAG_L1_21\",\n",
      "        \"NeuralNetTorch_BAG_L1_22\",\n",
      "        \"NeuralNetTorch_BAG_L1_23\",\n",
      "        \"NeuralNetTorch_BAG_L1_24\",\n",
      "        \"NeuralNetTorch_BAG_L1_25\",\n",
      "        \"NeuralNetTorch_BAG_L1_26\",\n",
      "        \"NeuralNetTorch_BAG_L1_27\",\n",
      "        \"NeuralNetTorch_BAG_L1_28\",\n",
      "        \"NeuralNetTorch_BAG_L1_29\",\n",
      "        \"NeuralNetTorch_BAG_L1_30\",\n",
      "        \"NeuralNetTorch_BAG_L1_31\",\n",
      "        \"NeuralNetTorch_BAG_L1_32\",\n",
      "        \"NeuralNetTorch_BAG_L1_33\",\n",
      "        \"NeuralNetTorch_BAG_L1_34\",\n",
      "        \"NeuralNetTorch_BAG_L1_35\",\n",
      "        \"NeuralNetTorch_BAG_L1_36\",\n",
      "        \"NeuralNetTorch_BAG_L1_37\",\n",
      "        \"NeuralNetTorch_BAG_L1_38\",\n",
      "        \"NeuralNetTorch_BAG_L1_39\",\n",
      "        \"NeuralNetTorch_BAG_L1_40\",\n",
      "        \"NeuralNetTorch_BAG_L1_41\",\n",
      "        \"NeuralNetTorch_BAG_L1_42\",\n",
      "        \"f2\",\n",
      "        \"f8\",\n",
      "        \"f14\",\n",
      "        \"f15\",\n",
      "        \"f16\",\n",
      "        \"f17\",\n",
      "        \"f18\",\n",
      "        \"f20\",\n",
      "        \"f21\",\n",
      "        \"f22\",\n",
      "        \"f23\",\n",
      "        \"f24\",\n",
      "        \"f28\",\n",
      "        \"f29\",\n",
      "        \"f30\",\n",
      "        \"f31\",\n",
      "        \"f32\",\n",
      "        \"f36\",\n",
      "        \"f39\",\n",
      "        \"f40\",\n",
      "        \"f41\",\n",
      "        \"f42\",\n",
      "        \"f44\",\n",
      "        \"f45\",\n",
      "        \"f46\",\n",
      "        \"f47\",\n",
      "        \"f48\",\n",
      "        \"f49\",\n",
      "        \"f50\",\n",
      "        \"f51\",\n",
      "        \"f52\",\n",
      "        \"f54\",\n",
      "        \"f58\",\n",
      "        \"f59\",\n",
      "        \"f60\",\n",
      "        \"f61\",\n",
      "        \"f62\",\n",
      "        \"f63\",\n",
      "        \"f64\",\n",
      "        \"f65\",\n",
      "        \"f66\",\n",
      "        \"f71\",\n",
      "        \"f72\",\n",
      "        \"f73\",\n",
      "        \"f74\",\n",
      "        \"f76\",\n",
      "        \"f77\",\n",
      "        \"f80\",\n",
      "        \"f81\",\n",
      "        \"f82\",\n",
      "        \"f83\",\n",
      "        \"f84\",\n",
      "        \"f85\",\n",
      "        \"f86\",\n",
      "        \"f87\",\n",
      "        \"f88\",\n",
      "        \"f89\",\n",
      "        \"f92\",\n",
      "        \"f93\",\n",
      "        \"f94\",\n",
      "        \"f95\",\n",
      "        \"f96\",\n",
      "        \"f97\",\n",
      "        \"f98\",\n",
      "        \"f99\",\n",
      "        \"f101\",\n",
      "        \"f102\",\n",
      "        \"f103\",\n",
      "        \"f104\",\n",
      "        \"f109\",\n",
      "        \"f110\",\n",
      "        \"f111\",\n",
      "        \"f112\",\n",
      "        \"f113\",\n",
      "        \"f115\",\n",
      "        \"f116\",\n",
      "        \"f117\",\n",
      "        \"f118\",\n",
      "        \"f119\",\n",
      "        \"f121\",\n",
      "        \"f125\",\n",
      "        \"f126\",\n",
      "        \"f127\",\n",
      "        \"f128\",\n",
      "        \"f129\",\n",
      "        \"f130\",\n",
      "        \"f131\",\n",
      "        \"f132\",\n",
      "        \"f133\",\n",
      "        \"f134\",\n",
      "        \"f135\",\n",
      "        \"f136\",\n",
      "        \"f139\",\n",
      "        \"f142\",\n",
      "        \"f143\",\n",
      "        \"f145\",\n",
      "        \"f146\",\n",
      "        \"f147\",\n",
      "        \"f148\",\n",
      "        \"f149\",\n",
      "        \"f150\",\n",
      "        \"f151\",\n",
      "        \"f152\",\n",
      "        \"f153\",\n",
      "        \"f154\",\n",
      "        \"f155\",\n",
      "        \"f156\",\n",
      "        \"f157\",\n",
      "        \"f159\",\n",
      "        \"f160\",\n",
      "        \"f161\",\n",
      "        \"f162\",\n",
      "        \"f163\",\n",
      "        \"f164\",\n",
      "        \"f165\",\n",
      "        \"f166\",\n",
      "        \"f167\",\n",
      "        \"f169\",\n",
      "        \"f171\",\n",
      "        \"f172\",\n",
      "        \"f173\",\n",
      "        \"f174\",\n",
      "        \"f175\",\n",
      "        \"f176\",\n",
      "        \"f177\",\n",
      "        \"f178\",\n",
      "        \"f179\",\n",
      "        \"f181\",\n",
      "        \"f182\",\n",
      "        \"f183\",\n",
      "        \"f184\",\n",
      "        \"f185\",\n",
      "        \"f186\",\n",
      "        \"f187\",\n",
      "        \"f188\",\n",
      "        \"f189\",\n",
      "        \"f190\",\n",
      "        \"f194\",\n",
      "        \"f195\",\n",
      "        \"f196\",\n",
      "        \"f197\",\n",
      "        \"f198\",\n",
      "        \"f199\",\n",
      "        \"f200\",\n",
      "        \"f201\",\n",
      "        \"f202\",\n",
      "        \"f203\",\n",
      "        \"f204\",\n",
      "        \"f205\",\n",
      "        \"f208\",\n",
      "        \"f210\",\n",
      "        \"f211\",\n",
      "        \"f212\",\n",
      "        \"f213\",\n",
      "        \"f214\",\n",
      "        \"f215\",\n",
      "        \"f216\",\n",
      "        \"f217\",\n",
      "        \"f218\",\n",
      "        \"f220\",\n",
      "        \"f221\",\n",
      "        \"f222\",\n",
      "        \"f223\",\n",
      "        \"f224\",\n",
      "        \"f225\",\n",
      "        \"f226\",\n",
      "        \"f227\",\n",
      "        \"f230\",\n",
      "        \"f231\",\n",
      "        \"f232\",\n",
      "        \"f233\",\n",
      "        \"f234\",\n",
      "        \"f235\",\n",
      "        \"f236\",\n",
      "        \"f237\",\n",
      "        \"f238\",\n",
      "        \"f239\",\n",
      "        \"f240\",\n",
      "        \"f241\",\n",
      "        \"f242\",\n",
      "        \"f243\",\n",
      "        \"f244\",\n",
      "        \"f245\",\n",
      "        \"f246\",\n",
      "        \"f247\",\n",
      "        \"f248\",\n",
      "        \"f250\",\n",
      "        \"f251\",\n",
      "        \"f252\",\n",
      "        \"f254\",\n",
      "        \"f255\",\n",
      "        \"f256\",\n",
      "        \"f257\",\n",
      "        \"f258\",\n",
      "        \"f260\",\n",
      "        \"f264\",\n",
      "        \"f265\",\n",
      "        \"f266\",\n",
      "        \"f267\",\n",
      "        \"f268\",\n",
      "        \"f269\",\n",
      "        \"f270\",\n",
      "        \"f271\",\n",
      "        \"f272\",\n",
      "        \"f273\",\n",
      "        \"f274\",\n",
      "        \"f275\",\n",
      "        \"f278\",\n",
      "        \"f281\",\n",
      "        \"f282\",\n",
      "        \"f283\",\n",
      "        \"f284\",\n",
      "        \"f285\",\n",
      "        \"f286\",\n",
      "        \"f287\",\n",
      "        \"f288\",\n",
      "        \"f289\",\n",
      "        \"f290\",\n",
      "        \"f291\",\n",
      "        \"f292\",\n",
      "        \"f293\",\n",
      "        \"f294\",\n",
      "        \"f295\",\n",
      "        \"f296\",\n",
      "        \"f298\",\n",
      "        \"f299\",\n",
      "        \"f300\",\n",
      "        \"f301\",\n",
      "        \"f302\",\n",
      "        \"f303\",\n",
      "        \"f304\",\n",
      "        \"f306\",\n",
      "        \"f307\",\n",
      "        \"f308\",\n",
      "        \"f309\",\n",
      "        \"f310\",\n",
      "        \"f311\",\n",
      "        \"f312\",\n",
      "        \"f314\",\n",
      "        \"f315\",\n",
      "        \"f316\",\n",
      "        \"f317\",\n",
      "        \"f318\",\n",
      "        \"f319\",\n",
      "        \"f320\",\n",
      "        \"f321\",\n",
      "        \"f325\",\n",
      "        \"f326\",\n",
      "        \"f327\",\n",
      "        \"f328\",\n",
      "        \"f329\",\n",
      "        \"f330\",\n",
      "        \"f331\",\n",
      "        \"f332\",\n",
      "        \"f333\",\n",
      "        \"f334\",\n",
      "        \"f335\",\n",
      "        \"f336\",\n",
      "        \"f337\",\n",
      "        \"f339\",\n",
      "        \"f340\",\n",
      "        \"f344\",\n",
      "        \"f345\",\n",
      "        \"f346\",\n",
      "        \"f347\",\n",
      "        \"f348\",\n",
      "        \"f349\",\n",
      "        \"f350\",\n",
      "        \"f351\",\n",
      "        \"f352\",\n",
      "        \"f353\",\n",
      "        \"f354\",\n",
      "        \"f355\",\n",
      "        \"f357\",\n",
      "        \"f358\",\n",
      "        \"f360\",\n",
      "        \"f361\",\n",
      "        \"f362\",\n",
      "        \"f363\",\n",
      "        \"f364\",\n",
      "        \"f366\",\n",
      "        \"f367\",\n",
      "        \"f368\",\n",
      "        \"f369\",\n",
      "        \"f370\",\n",
      "        \"f371\",\n",
      "        \"f372\",\n",
      "        \"f374\",\n",
      "        \"f375\",\n",
      "        \"f379\",\n",
      "        \"f380\",\n",
      "        \"f381\",\n",
      "        \"f382\",\n",
      "        \"f384\",\n",
      "        \"f385\",\n",
      "        \"f386\",\n",
      "        \"f387\",\n",
      "        \"f388\",\n",
      "        \"f389\",\n",
      "        \"f392\",\n",
      "        \"f393\",\n",
      "        \"f394\",\n",
      "        \"f395\",\n",
      "        \"f396\",\n",
      "        \"f397\",\n",
      "        \"f398\",\n",
      "        \"f399\",\n",
      "        \"f400\",\n",
      "        \"f401\",\n",
      "        \"f402\",\n",
      "        \"f407\",\n",
      "        \"f408\",\n",
      "        \"f409\",\n",
      "        \"f410\",\n",
      "        \"f411\",\n",
      "        \"f413\",\n",
      "        \"f414\",\n",
      "        \"f415\",\n",
      "        \"f416\",\n",
      "        \"f417\",\n",
      "        \"f418\",\n",
      "        \"f421\",\n",
      "        \"f423\",\n",
      "        \"f424\",\n",
      "        \"f425\",\n",
      "        \"f426\",\n",
      "        \"f427\",\n",
      "        \"f428\",\n",
      "        \"f429\",\n",
      "        \"f430\",\n",
      "        \"f431\",\n",
      "        \"f433\",\n",
      "        \"f434\",\n",
      "        \"f435\",\n",
      "        \"f436\",\n",
      "        \"f441\",\n",
      "        \"f443\",\n",
      "        \"f444\",\n",
      "        \"f448\",\n",
      "        \"f451\",\n",
      "        \"f452\",\n",
      "        \"f453\",\n",
      "        \"f454\",\n",
      "        \"f455\",\n",
      "        \"f456\",\n",
      "        \"f459\",\n",
      "        \"f460\",\n",
      "        \"f461\",\n",
      "        \"f464\",\n",
      "        \"f465\",\n",
      "        \"f466\",\n",
      "        \"f468\",\n",
      "        \"f470\",\n",
      "        \"f471\",\n",
      "        \"f475\",\n",
      "        \"f476\",\n",
      "        \"f477\",\n",
      "        \"f480\",\n",
      "        \"f481\",\n",
      "        \"f482\",\n",
      "        \"f483\",\n",
      "        \"f484\",\n",
      "        \"f485\",\n",
      "        \"f486\",\n",
      "        \"f487\",\n",
      "        \"f490\",\n",
      "        \"f491\",\n",
      "        \"f492\",\n",
      "        \"f493\",\n",
      "        \"f494\",\n",
      "        \"f495\",\n",
      "        \"f496\",\n",
      "        \"f497\",\n",
      "        \"f500\",\n",
      "        \"f501\",\n",
      "        \"f502\",\n",
      "        \"f506\",\n",
      "        \"f507\",\n",
      "        \"f510\",\n",
      "        \"f512\",\n",
      "        \"f513\",\n",
      "        \"f515\",\n",
      "        \"f517\",\n",
      "        \"f518\",\n",
      "        \"f519\",\n",
      "        \"f520\",\n",
      "        \"f521\",\n",
      "        \"f522\",\n",
      "        \"f523\",\n",
      "        \"f525\",\n",
      "        \"f526\",\n",
      "        \"f527\",\n",
      "        \"f528\",\n",
      "        \"f529\",\n",
      "        \"f530\",\n",
      "        \"f531\",\n",
      "        \"f532\",\n",
      "        \"f535\",\n",
      "        \"f536\",\n",
      "        \"f538\",\n",
      "        \"f539\",\n",
      "        \"f540\",\n",
      "        \"f541\",\n",
      "        \"f542\",\n",
      "        \"f543\",\n",
      "        \"f544\",\n",
      "        \"f545\",\n",
      "        \"f546\",\n",
      "        \"f547\",\n",
      "        \"f548\",\n",
      "        \"f549\",\n",
      "        \"f550\",\n",
      "        \"f551\",\n",
      "        \"f552\",\n",
      "        \"f553\",\n",
      "        \"f554\",\n",
      "        \"f555\",\n",
      "        \"f556\",\n",
      "        \"f557\",\n",
      "        \"f558\",\n",
      "        \"f559\",\n",
      "        \"f560\",\n",
      "        \"f561\",\n",
      "        \"f562\",\n",
      "        \"f563\",\n",
      "        \"f564\",\n",
      "        \"f565\",\n",
      "        \"f566\",\n",
      "        \"f567\",\n",
      "        \"f568\",\n",
      "        \"f569\",\n",
      "        \"f570\",\n",
      "        \"f571\",\n",
      "        \"f572\",\n",
      "        \"f573\",\n",
      "        \"f574\",\n",
      "        \"f575\",\n",
      "        \"f576\",\n",
      "        \"f577\",\n",
      "        \"f578\",\n",
      "        \"f579\",\n",
      "        \"f580\",\n",
      "        \"f581\",\n",
      "        \"f582\",\n",
      "        \"f583\",\n",
      "        \"f584\",\n",
      "        \"f585\",\n",
      "        \"f586\",\n",
      "        \"f587\",\n",
      "        \"f588\",\n",
      "        \"f589\",\n",
      "        \"f590\",\n",
      "        \"f591\",\n",
      "        \"f592\",\n",
      "        \"f593\",\n",
      "        \"f594\",\n",
      "        \"f595\",\n",
      "        \"f600\",\n",
      "        \"f601\",\n",
      "        \"f604\",\n",
      "        \"f609\",\n",
      "        \"f612\",\n",
      "        \"f613\",\n",
      "        \"f614\",\n",
      "        \"f615\",\n",
      "        \"f616\",\n",
      "        \"f617\",\n",
      "        \"f618\",\n",
      "        \"f619\",\n",
      "        \"f620\",\n",
      "        \"f621\",\n",
      "        \"f622\",\n",
      "        \"f623\",\n",
      "        \"f624\",\n",
      "        \"f625\",\n",
      "        \"f628\",\n",
      "        \"f629\",\n",
      "        \"f630\",\n",
      "        \"f631\",\n",
      "        \"f632\",\n",
      "        \"f633\",\n",
      "        \"f634\",\n",
      "        \"f635\",\n",
      "        \"f636\",\n",
      "        \"f637\",\n",
      "        \"f638\",\n",
      "        \"f639\",\n",
      "        \"f641\",\n",
      "        \"f642\",\n",
      "        \"f643\",\n",
      "        \"f644\",\n",
      "        \"f648\",\n",
      "        \"f649\",\n",
      "        \"f650\",\n",
      "        \"f651\",\n",
      "        \"f652\",\n",
      "        \"f654\",\n",
      "        \"f655\",\n",
      "        \"f656\",\n",
      "        \"f657\",\n",
      "        \"f658\",\n",
      "        \"f659\",\n",
      "        \"f660\",\n",
      "        \"f661\",\n",
      "        \"f665\",\n",
      "        \"f666\",\n",
      "        \"f667\",\n",
      "        \"f668\",\n",
      "        \"f670\",\n",
      "        \"f671\",\n",
      "        \"f672\",\n",
      "        \"f673\",\n",
      "        \"f674\",\n",
      "        \"f679\",\n",
      "        \"f680\",\n",
      "        \"f681\",\n",
      "        \"f682\",\n",
      "        \"f683\",\n",
      "        \"f684\",\n",
      "        \"f685\",\n",
      "        \"f686\",\n",
      "        \"f687\",\n",
      "        \"f688\",\n",
      "        \"f689\",\n",
      "        \"f690\",\n",
      "        \"f691\",\n",
      "        \"f692\",\n",
      "        \"f693\",\n",
      "        \"f694\",\n",
      "        \"f697\",\n",
      "        \"f699\",\n",
      "        \"f703\",\n",
      "        \"f704\",\n",
      "        \"f705\",\n",
      "        \"f706\",\n",
      "        \"f707\",\n",
      "        \"f708\",\n",
      "        \"f709\",\n",
      "        \"f710\",\n",
      "        \"f711\",\n",
      "        \"f712\",\n",
      "        \"f713\",\n",
      "        \"f714\",\n",
      "        \"f715\",\n",
      "        \"f718\",\n",
      "        \"f719\",\n",
      "        \"f720\",\n",
      "        \"f721\",\n",
      "        \"f722\",\n",
      "        \"f723\",\n",
      "        \"f724\",\n",
      "        \"f726\",\n",
      "        \"f728\",\n",
      "        \"f729\",\n",
      "        \"f730\",\n",
      "        \"f731\",\n",
      "        \"f732\",\n",
      "        \"f740\",\n",
      "        \"f741\",\n",
      "        \"f742\",\n",
      "        \"f743\",\n",
      "        \"f744\",\n",
      "        \"f747\",\n",
      "        \"f748\",\n",
      "        \"f749\",\n",
      "        \"f750\",\n",
      "        \"f751\",\n",
      "        \"f752\",\n",
      "        \"f753\",\n",
      "        \"f754\",\n",
      "        \"f755\",\n",
      "        \"f757\",\n",
      "        \"f758\",\n",
      "        \"f759\",\n",
      "        \"f760\",\n",
      "        \"f761\",\n",
      "        \"f762\",\n",
      "        \"f763\",\n",
      "        \"f769\",\n",
      "        \"f770\",\n",
      "        \"f771\",\n",
      "        \"f772\",\n",
      "        \"f773\",\n",
      "        \"f775\",\n",
      "        \"f778\"\n",
      "    ],\n",
      "    \"onehot\": [\n",
      "        \"KNeighborsUnif_BAG_L1_18\",\n",
      "        \"KNeighborsUnif_BAG_L1_19\",\n",
      "        \"KNeighborsUnif_BAG_L1_20\",\n",
      "        \"KNeighborsUnif_BAG_L1_21\",\n",
      "        \"KNeighborsUnif_BAG_L1_22\",\n",
      "        \"KNeighborsUnif_BAG_L1_23\",\n",
      "        \"KNeighborsUnif_BAG_L1_24\",\n",
      "        \"KNeighborsUnif_BAG_L1_25\",\n",
      "        \"KNeighborsUnif_BAG_L1_26\",\n",
      "        \"KNeighborsUnif_BAG_L1_27\",\n",
      "        \"KNeighborsUnif_BAG_L1_28\",\n",
      "        \"KNeighborsUnif_BAG_L1_29\",\n",
      "        \"KNeighborsUnif_BAG_L1_30\",\n",
      "        \"KNeighborsUnif_BAG_L1_31\",\n",
      "        \"KNeighborsUnif_BAG_L1_32\",\n",
      "        \"KNeighborsUnif_BAG_L1_33\",\n",
      "        \"KNeighborsUnif_BAG_L1_35\",\n",
      "        \"KNeighborsUnif_BAG_L1_36\",\n",
      "        \"KNeighborsUnif_BAG_L1_37\",\n",
      "        \"KNeighborsUnif_BAG_L1_38\",\n",
      "        \"KNeighborsUnif_BAG_L1_39\",\n",
      "        \"KNeighborsUnif_BAG_L1_40\",\n",
      "        \"KNeighborsUnif_BAG_L1_41\",\n",
      "        \"KNeighborsUnif_BAG_L1_42\"\n",
      "    ],\n",
      "    \"embed\": [\n",
      "        \"f137\",\n",
      "        \"f138\",\n",
      "        \"f206\",\n",
      "        \"f207\",\n",
      "        \"f276\",\n",
      "        \"f277\",\n",
      "        \"f338\",\n",
      "        \"f390\",\n",
      "        \"f391\",\n",
      "        \"f419\",\n",
      "        \"f420\",\n",
      "        \"f469\",\n",
      "        \"f472\",\n",
      "        \"f534\",\n",
      "        \"f537\",\n",
      "        \"f626\",\n",
      "        \"f627\",\n",
      "        \"f695\",\n",
      "        \"f698\"\n",
      "    ],\n",
      "    \"language\": [],\n",
      "    \"bool\": [\n",
      "        \"f678\",\n",
      "        \"f776\",\n",
      "        \"f777\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 92146 examples, 1061 features (1042 vector, 19 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(102, 21)\n",
      "    (2): Embedding(102, 21)\n",
      "    (3): Embedding(102, 21)\n",
      "    (4): Embedding(102, 21)\n",
      "    (5): Embedding(102, 21)\n",
      "    (6): Embedding(102, 21)\n",
      "    (7): Embedding(102, 21)\n",
      "    (8): Embedding(102, 21)\n",
      "    (9): Embedding(102, 21)\n",
      "    (10): Embedding(102, 21)\n",
      "    (11): Embedding(102, 21)\n",
      "    (12): Embedding(102, 21)\n",
      "    (13): Embedding(102, 21)\n",
      "    (14): Embedding(102, 21)\n",
      "    (15): Embedding(21, 8)\n",
      "    (16): Embedding(24, 9)\n",
      "    (17): Embedding(102, 21)\n",
      "    (18): Embedding(102, 21)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=1440, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=43, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 719).\tTrain loss: 0.6094, Val mean_absolute_error: -0.6828, Best Epoch: 1\n",
      "Epoch 2 (Update 1438).\tTrain loss: 0.5168, Val mean_absolute_error: -0.6828, Best Epoch: 2\n",
      "Epoch 3 (Update 2157).\tTrain loss: 0.5111, Val mean_absolute_error: -0.6828, Best Epoch: 3\n",
      "Epoch 4 (Update 2876).\tTrain loss: 0.508, Val mean_absolute_error: -0.6828, Best Epoch: 4\n",
      "Epoch 5 (Update 3595).\tTrain loss: 0.5056, Val mean_absolute_error: -0.6828, Best Epoch: 5\n",
      "Epoch 6 (Update 4314).\tTrain loss: 0.5032, Val mean_absolute_error: -0.6828, Best Epoch: 6\n",
      "Epoch 7 (Update 5033).\tTrain loss: 0.5006, Val mean_absolute_error: -0.6828, Best Epoch: 7\n",
      "Epoch 8 (Update 5752).\tTrain loss: 0.5003, Val mean_absolute_error: -0.6828, Best Epoch: 8\n",
      "Epoch 9 (Update 6471).\tTrain loss: 0.4973, Val mean_absolute_error: -0.6828, Best Epoch: 9\n",
      "Epoch 10 (Update 7190).\tTrain loss: 0.4963, Val mean_absolute_error: -0.6828, Best Epoch: 10\n",
      "Epoch 11 (Update 7909).\tTrain loss: 0.4931, Val mean_absolute_error: -0.6828, Best Epoch: 11\n",
      "Epoch 12 (Update 8628).\tTrain loss: 0.4917, Val mean_absolute_error: -0.6828, Best Epoch: 12\n",
      "Epoch 13 (Update 9347).\tTrain loss: 0.4893, Val mean_absolute_error: -0.6828, Best Epoch: 13\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 13)\n",
      "Best model found on Epoch 13 (Update 9347). Val mean_absolute_error: -0.6828230646509155\n",
      "Saving AutoGluonLoan/\\models\\NeuralNetTorch_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonLoan/\\models\\NeuralNetTorch_BAG_L2\\model.pkl\n",
      "\t-0.6842\t = Validation score   (-mean_absolute_error)\n",
      "\t1011.81s\t = Training   runtime\n",
      "\t24.52s\t = Validation runtime\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 42.96s of the 42.4s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\LightGBMLarge_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 11.699 GB, but only 8.949 GB is available...\n",
      "\tNot enough memory to train LightGBMLarge_BAG_L2... Skipping this model.\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Not enough time left to finish repeated k-fold bagging, stopping early ...\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestGini_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestEntr_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\ExtraTreesGini_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\ExtraTreesEntr_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonLoan/\\models\\NeuralNetTorch_BAG_L2\\utils\\oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L3: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 34.06s of remaining time.\n",
      "Saving AutoGluonLoan/\\models\\WeightedEnsemble_L3\\utils\\model_template.pkl\n",
      "Loading: AutoGluonLoan/\\models\\WeightedEnsemble_L3\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "Ensemble size: 1\n",
      "Ensemble weights: \n",
      "[0. 0. 0. 0. 1.]\n",
      "\t0.41s\t= Estimated out-of-fold prediction time...\n",
      "Saving AutoGluonLoan/\\models\\WeightedEnsemble_L3\\utils\\oof.pkl\n",
      "Saving AutoGluonLoan/\\models\\WeightedEnsemble_L3\\model.pkl\n",
      "\t-0.6842\t = Validation score   (-mean_absolute_error)\n",
      "\t48.52s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "AutoGluon training complete, total runtime = 5420.28s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Loading: AutoGluonLoan/\\models\\trainer.pkl\n",
      "Saving AutoGluonLoan/\\models\\trainer.pkl\n",
      "Saving AutoGluonLoan/\\learner.pkl\n",
      "Saving AutoGluonLoan/\\predictor.pkl\n",
      "Saving AutoGluonLoan/\\__version__ with contents \"0.5.2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutoGluonLoan/\\\")\n",
      "Loading: AutoGluonLoan/\\models\\KNeighborsUnif_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLoan/\\models\\KNeighborsDist_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestGini_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestEntr_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLoan/\\models\\ExtraTreesGini_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLoan/\\models\\ExtraTreesEntr_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLoan/\\models\\NeuralNetTorch_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonLoan/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestGini_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestEntr_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonLoan/\\models\\ExtraTreesGini_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonLoan/\\models\\ExtraTreesEntr_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonLoan/\\models\\NeuralNetTorch_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonLoan/\\models\\WeightedEnsemble_L3\\model.pkl\n",
      "D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:138: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                      model  score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     NeuralNetTorch_BAG_L1  -0.684234      18.258384  1672.107822               18.258384        1672.107822            1       True          7\n",
      "1     ExtraTreesEntr_BAG_L1  -0.684234      19.782512    19.361375               19.782512          19.361375            1       True          6\n",
      "2   RandomForestGini_BAG_L1  -0.684234      26.854063   222.709442               26.854063         222.709442            1       True          3\n",
      "3       WeightedEnsemble_L2  -0.684234      26.874060   278.284177                0.019998          55.574735            2       True          8\n",
      "4   RandomForestEntr_BAG_L1  -0.684234      28.005741   376.197912               28.005741         376.197912            1       True          4\n",
      "5     NeuralNetTorch_BAG_L2  -0.684234     847.982699  3334.622390               24.522249        1011.813510            2       True         13\n",
      "6       WeightedEnsemble_L3  -0.684234     848.002665  3383.143159                0.019966          48.520769            3       True         14\n",
      "7     ExtraTreesEntr_BAG_L2  -0.684234     850.415367  2346.016908               26.954917          23.208027            2       True         12\n",
      "8     ExtraTreesGini_BAG_L2  -0.684234     851.142449  2358.989406               27.681999          36.180526            2       True         11\n",
      "9   RandomForestEntr_BAG_L2  -0.684234     860.347227  2664.226585               36.886777         341.417705            2       True         10\n",
      "10  RandomForestGini_BAG_L2  -0.684234     861.775139  2581.181499               38.314689         258.372619            2       True          9\n",
      "11    ExtraTreesGini_BAG_L1  -0.684357      19.955108    25.673334               19.955108          25.673334            1       True          5\n",
      "12    KNeighborsUnif_BAG_L1  -0.685298     353.323323     3.347000              353.323323           3.347000            1       True          1\n",
      "13    KNeighborsDist_BAG_L1  -0.872974     357.281321     3.411995              357.281321           3.411995            1       True          2\n",
      "Number of models trained: 14\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_KNN', 'StackerEnsembleModel_RF', 'WeightedEnsembleModel', 'StackerEnsembleModel_TabularNeuralNetTorch', 'StackerEnsembleModel_XT'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])  :  19 | ['f137', 'f138', 'f206', 'f207', 'f276', ...]\n",
      "('float', [])     : 651 | ['f3', 'f7', 'f8', 'f9', 'f10', ...]\n",
      "('int', [])       :  87 | ['id', 'f1', 'f2', 'f4', 'f5', ...]\n",
      "('int', ['bool']) :   3 | ['f678', 'f776', 'f777']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Int features without null values at train time contain null values at inference time! Imputing nulls to 0. To avoid this, pass the features as floats during fit!\n",
      "WARNING: Int features with nulls: ['f5']\n",
      "Loading: AutoGluonLoan/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "Loading: AutoGluonLoan/\\models\\RandomForestGini_BAG_L1\\model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "label = 'loss'  # name of target variable to predict in this competition\n",
    "eval_metric = 'mae'  # Optional: specify that competition evaluation metric is AUC\n",
    "save_path = 'AutoGluonLoan/'  # where to store trained models\n",
    "\n",
    "train_data = pd.read_csv('train_loan.csv', low_memory=False)\n",
    "predictor = TabularPredictor(label=label, eval_metric=eval_metric, path=save_path, verbosity=3).fit(\n",
    "    train_data, presets='best_quality', time_limit=5400\n",
    ")\n",
    "\n",
    "results = predictor.fit_summary()\n",
    "test_data = pd.read_csv('test_loan.csv', low_memory=False)\n",
    "y_predproba = predictor.predict(test_data)\n",
    "\n",
    "test_data['loss'] = y_predproba\n",
    "test_data[['id', 'loss']].to_csv('loan_autogluon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d965a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['high_quality']\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'_save_bag_folds': False,\n",
      " 'auto_stack': True,\n",
      " 'refit_full': True,\n",
      " 'set_best_to_refit_full': True}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': False,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': None,\n",
      " 'auto_stack': True,\n",
      " 'calibrate': 'auto',\n",
      " 'excluded_model_types': None,\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'keep_only_best': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': None,\n",
      " 'num_bag_sets': None,\n",
      " 'num_stack_levels': None,\n",
      " 'pseudo_data': None,\n",
      " 'quantile_levels': None,\n",
      " 'refit_full': True,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': True,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 3}\n",
      "========================================\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Saving AutoGluonRGA3/\\learner.pkl\n",
      "Saving AutoGluonRGA3/\\predictor.pkl\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"AutoGluonRGA3/\\\"\n",
      "AutoGluon Version:  0.5.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Train Data Rows:    45227\n",
      "Train Data Columns: 55\n",
      "Label Column: target\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1655808000.0, 0.0, 11455927.91143, 35551035.27251)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    10246.62 MB\n",
      "\tTrain Data (Original)  Memory Usage: 81.17 MB (0.8% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float64', 'float') : 10 | ['Cuánto cobrás por guardia', '¿Gente a cargo?', 'Años en la empresa actual', '¿La recomendás como un buen lugar para trabajar?', '¿Cómo calificás las políticas de diversidad e inclusión?', ...]\n",
      "\t\t\t\t('int64', 'int')     :  1 | ['¿Qué tan conforme estás con tu sueldo?']\n",
      "\t\t\t\t('object', 'object') : 37 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])        : 10 | ['Cuánto cobrás por guardia', '¿Gente a cargo?', 'Años en la empresa actual', '¿La recomendás como un buen lugar para trabajar?', '¿Cómo calificás las políticas de diversidad e inclusión?', ...]\n",
      "\t\t\t\t('int', [])          :  1 | ['¿Qué tan conforme estás con tu sueldo?']\n",
      "\t\t\t\t('object', [])       : 26 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "\t\t\t\t('object', ['text']) : 11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])        : 10 | ['Cuánto cobrás por guardia', '¿Gente a cargo?', 'Años en la empresa actual', '¿La recomendás como un buen lugar para trabajar?', '¿Cómo calificás las políticas de diversidad e inclusión?', ...]\n",
      "\t\t\t\t('int', [])          :  1 | ['¿Qué tan conforme estás con tu sueldo?']\n",
      "\t\t\t\t('int', ['bool'])    :  1 | ['Trabajo para una empresa que no tiene oficina en mi ciudad']\n",
      "\t\t\t\t('object', [])       : 25 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "\t\t\t\t('object', ['text']) : 11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t48 features in original data used to generate 48 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])        : 10 | ['Cuánto cobrás por guardia', '¿Gente a cargo?', 'Años en la empresa actual', '¿La recomendás como un buen lugar para trabajar?', '¿Cómo calificás las políticas de diversidad e inclusión?', ...]\n",
      "\t\t\t\t('int', [])          :  1 | ['¿Qué tan conforme estás con tu sueldo?']\n",
      "\t\t\t\t('int', ['bool'])    :  1 | ['Trabajo para una empresa que no tiene oficina en mi ciudad']\n",
      "\t\t\t\t('object', [])       : 25 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "\t\t\t\t('object', ['text']) : 11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])        : 10 | ['Cuánto cobrás por guardia', '¿Gente a cargo?', 'Años en la empresa actual', '¿La recomendás como un buen lugar para trabajar?', '¿Cómo calificás las políticas de diversidad e inclusión?', ...]\n",
      "\t\t\t\t('int', [])          :  1 | ['¿Qué tan conforme estás con tu sueldo?']\n",
      "\t\t\t\t('int', ['bool'])    :  1 | ['Trabajo para una empresa que no tiene oficina en mi ciudad']\n",
      "\t\t\t\t('object', [])       : 25 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "\t\t\t\t('object', ['text']) : 11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "\t\t\t0.1s = Fit runtime\n",
      "\t\t\t48 features in original data used to generate 48 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 10 | ['Cuánto cobrás por guardia', '¿Gente a cargo?', 'Años en la empresa actual', '¿La recomendás como un buen lugar para trabajar?', '¿Cómo calificás las políticas de diversidad e inclusión?', ...]\n",
      "\t\t\t\t('int', [])       :  1 | ['¿Qué tan conforme estás con tu sueldo?']\n",
      "\t\t\t\t('int', ['bool']) :  1 | ['Trabajo para una empresa que no tiene oficina en mi ciudad']\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', [])     : 10 | ['Cuánto cobrás por guardia', '¿Gente a cargo?', 'Años en la empresa actual', '¿La recomendás como un buen lugar para trabajar?', '¿Cómo calificás las políticas de diversidad e inclusión?', ...]\n",
      "\t\t\t\t('int', [])       :  1 | ['¿Qué tan conforme estás con tu sueldo?']\n",
      "\t\t\t\t('int', ['bool']) :  1 | ['Trabajo para una empresa que no tiene oficina en mi ciudad']\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t12 features in original data used to generate 12 features in processed data.\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', [])                   : 25 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "\t\t\t\t\t('category', ['text_as_category']) : 11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('category', [])                   : 25 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "\t\t\t\t\t('category', ['text_as_category']) : 11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "\t\t\t\t0.0s = Fit runtime\n",
      "\t\t\t\t36 features in original data used to generate 36 features in processed data.\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('object', [])       : 25 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t\t\t('object', ['text']) : 11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])                   : 25 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "\t\t\t\t('category', ['text_as_category']) : 11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "\t\t\t0.2s = Fit runtime\n",
      "\t\t\t36 features in original data used to generate 36 features in processed data.\n",
      "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('float', ['text_special']) : 72 | ['Plataformas.capital_ratio', 'Plataformas.lower_ratio', 'Plataformas.digit_ratio', 'Plataformas.special_ratio', 'Plataformas.symbol_ratio.*', ...]\n",
      "\t\t\t\t\t('int', ['text_special'])   : 50 | ['Plataformas.char_count', 'Plataformas.word_count', 'Plataformas.symbol_count.*', 'Plataformas.symbol_count. ', 'Plataformas.symbol_count./', ...]\n",
      "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('int', ['binned', 'text_special']) : 122 | ['Plataformas.char_count', 'Plataformas.word_count', 'Plataformas.capital_ratio', 'Plataformas.lower_ratio', 'Plataformas.digit_ratio', ...]\n",
      "\t\t\t\t0.8s = Fit runtime\n",
      "\t\t\t\t122 features in original data used to generate 122 features in processed data.\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('int', ['binned', 'text_special']) : 122 | ['Plataformas.char_count', 'Plataformas.word_count', 'Plataformas.capital_ratio', 'Plataformas.lower_ratio', 'Plataformas.digit_ratio', ...]\n",
      "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t\t('int', ['binned', 'text_special']) : 122 | ['Plataformas.char_count', 'Plataformas.word_count', 'Plataformas.capital_ratio', 'Plataformas.lower_ratio', 'Plataformas.digit_ratio', ...]\n",
      "\t\t\t\t0.5s = Fit runtime\n",
      "\t\t\t\t122 features in original data used to generate 122 features in processed data.\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('object', ['text']) : 11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', ['binned', 'text_special']) : 122 | ['Plataformas.char_count', 'Plataformas.word_count', 'Plataformas.capital_ratio', 'Plataformas.lower_ratio', 'Plataformas.digit_ratio', ...]\n",
      "\t\t\t6.2s = Fit runtime\n",
      "\t\t\t11 features in original data used to generate 122 features in processed data.\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', 'Frameworks, herramientas y librerías', 'Carrera', 'Universidad', 'Lenguajes de programación', 'IDEs', '¿Cómo se vio afectada tu empresa/organización?']\n",
      "\t\t\t\tCountVectorizer(dtype=<class 'numpy.uint8'>, max_features=10000, min_df=30,\n",
      "                ngram_range=(1, 3))\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 4835\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('object', ['text']) : 11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', ['text_ngram']) : 4836 | ['__nlp__.21', '__nlp__.21 bash', '__nlp__.21 bash shell', '__nlp__.21 ninguno', '__nlp__.21 ninguno de', ...]\n",
      "\t\t\t4.3s = Fit runtime\n",
      "\t\t\t11 features in original data used to generate 4836 features in processed data.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])                    :   25 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "\t\t\t\t('category', ['text_as_category'])  :   11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "\t\t\t\t('float', [])                       :   10 | ['Cuánto cobrás por guardia', '¿Gente a cargo?', 'Años en la empresa actual', '¿La recomendás como un buen lugar para trabajar?', '¿Cómo calificás las políticas de diversidad e inclusión?', ...]\n",
      "\t\t\t\t('int', [])                         :    1 | ['¿Qué tan conforme estás con tu sueldo?']\n",
      "\t\t\t\t('int', ['binned', 'text_special']) :  122 | ['Plataformas.char_count', 'Plataformas.word_count', 'Plataformas.capital_ratio', 'Plataformas.lower_ratio', 'Plataformas.digit_ratio', ...]\n",
      "\t\t\t\t('int', ['bool'])                   :    1 | ['Trabajo para una empresa que no tiene oficina en mi ciudad']\n",
      "\t\t\t\t('int', ['text_ngram'])             : 4836 | ['__nlp__.21', '__nlp__.21 bash', '__nlp__.21 bash shell', '__nlp__.21 ninguno', '__nlp__.21 ninguno de', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('category', [])                    :   25 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "\t\t\t\t('category', ['text_as_category'])  :   11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "\t\t\t\t('float', [])                       :   10 | ['Cuánto cobrás por guardia', '¿Gente a cargo?', 'Años en la empresa actual', '¿La recomendás como un buen lugar para trabajar?', '¿Cómo calificás las políticas de diversidad e inclusión?', ...]\n",
      "\t\t\t\t('int', [])                         :    1 | ['¿Qué tan conforme estás con tu sueldo?']\n",
      "\t\t\t\t('int', ['binned', 'text_special']) :  122 | ['Plataformas.char_count', 'Plataformas.word_count', 'Plataformas.capital_ratio', 'Plataformas.lower_ratio', 'Plataformas.digit_ratio', ...]\n",
      "\t\t\t\t('int', ['bool'])                   :    1 | ['Trabajo para una empresa que no tiene oficina en mi ciudad']\n",
      "\t\t\t\t('int', ['text_ngram'])             : 4836 | ['__nlp__.21', '__nlp__.21 bash', '__nlp__.21 bash shell', '__nlp__.21 ninguno', '__nlp__.21 ninguno de', ...]\n",
      "\t\t\t6.1s = Fit runtime\n",
      "\t\t\t5006 features in original data used to generate 5006 features in processed data.\n",
      "\tUseless Original Features (Count: 7): ['Sufriste o presenciaste situaciones de violencia y/o acoso por motivo de', '¿Considerás que en tu empresa/organización hay una marcada tendencia a escuchar más a los hombres?', '¿Sentís que podés ser vos en tu trabajo?', '¿Considerás que tenés oportunidades de crecimiento siendo quien sos dentro de tu organización?', '¿Sentís que alguna vez los prejuicios culturales/sociales sobre tu orientación, género, etnia o discapacidad pudieron obstaculizar el que consigas un trabajo?', 'En el último año, en tu trabajo ¿recibiste o escuchaste comentarios que considerás inapropiados, subidos de tono y/o discriminatorios?', '¿Cuántas veces a la semana vas a trabajar a la oficina?']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('float64', 'float') : 10 | ['Cuánto cobrás por guardia', '¿Gente a cargo?', 'Años en la empresa actual', '¿La recomendás como un buen lugar para trabajar?', '¿Cómo calificás las políticas de diversidad e inclusión?', ...]\n",
      "\t\t('int64', 'int')     :  1 | ['¿Qué tan conforme estás con tu sueldo?']\n",
      "\t\t('object', 'object') : 37 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])        : 10 | ['Cuánto cobrás por guardia', '¿Gente a cargo?', 'Años en la empresa actual', '¿La recomendás como un buen lugar para trabajar?', '¿Cómo calificás las políticas de diversidad e inclusión?', ...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t('int', [])          :  1 | ['¿Qué tan conforme estás con tu sueldo?']\n",
      "\t\t('object', [])       : 26 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "\t\t('object', ['text']) : 11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('category', 'category') :   36 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "\t\t('float64', 'float')     :   10 | ['Cuánto cobrás por guardia', '¿Gente a cargo?', 'Años en la empresa actual', '¿La recomendás como un buen lugar para trabajar?', '¿Cómo calificás las políticas de diversidad e inclusión?', ...]\n",
      "\t\t('int64', 'int')         :    1 | ['¿Qué tan conforme estás con tu sueldo?']\n",
      "\t\t('int8', 'int')          :    1 | ['Trabajo para una empresa que no tiene oficina en mi ciudad']\n",
      "\t\t('uint16', 'int')        : 4836 | ['__nlp__.21', '__nlp__.21 bash', '__nlp__.21 bash shell', '__nlp__.21 ninguno', '__nlp__.21 ninguno de', ...]\n",
      "\t\t('uint8', 'int')         :  122 | ['Plataformas.char_count', 'Plataformas.word_count', 'Plataformas.capital_ratio', 'Plataformas.lower_ratio', 'Plataformas.digit_ratio', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :   25 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "\t\t('category', ['text_as_category'])  :   11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "\t\t('float', [])                       :   10 | ['Cuánto cobrás por guardia', '¿Gente a cargo?', 'Años en la empresa actual', '¿La recomendás como un buen lugar para trabajar?', '¿Cómo calificás las políticas de diversidad e inclusión?', ...]\n",
      "\t\t('int', [])                         :    1 | ['¿Qué tan conforme estás con tu sueldo?']\n",
      "\t\t('int', ['binned', 'text_special']) :  122 | ['Plataformas.char_count', 'Plataformas.word_count', 'Plataformas.capital_ratio', 'Plataformas.lower_ratio', 'Plataformas.digit_ratio', ...]\n",
      "\t\t('int', ['bool'])                   :    1 | ['Trabajo para una empresa que no tiene oficina en mi ciudad']\n",
      "\t\t('int', ['text_ngram'])             : 4836 | ['__nlp__.21', '__nlp__.21 bash', '__nlp__.21 bash shell', '__nlp__.21 ninguno', '__nlp__.21 ninguno de', ...]\n",
      "\t18.9s = Fit runtime\n",
      "\t48 features in original data used to generate 5006 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 449.12 MB (4.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 21.23s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving AutoGluonRGA3/\\learner.pkl\n",
      "Saving AutoGluonRGA3/\\utils\\data\\X.pkl\n",
      "Saving AutoGluonRGA3/\\utils\\data\\y.pkl\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Model configs that will be trained (in order):\n",
      "\tKNeighborsUnif_BAG_L1: \t{'weights': 'uniform', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Unif', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True, 'save_bag_folds': False}}\n",
      "\tKNeighborsDist_BAG_L1: \t{'weights': 'distance', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Dist', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True, 'save_bag_folds': False}}\n",
      "\tLightGBMXT_BAG_L1: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "\tLightGBM_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "\tRandomForestMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True, 'save_bag_folds': False}}\n",
      "\tCatBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "\tExtraTreesMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True, 'save_bag_folds': False}}\n",
      "\tNeuralNetFastAI_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "\tXGBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "\tNeuralNetTorch_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "\tLightGBMLarge_BAG_L1: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 2385.25s of the 3578.59s of remaining time.\n",
      "Saving AutoGluonRGA3/\\models\\KNeighborsUnif_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\KNeighborsUnif_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Model is expected to require 33.01% of available memory...\n",
      "\tNot enough memory to train KNeighborsUnif_BAG_L1... Skipping this model.\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 2378.25s of the 3571.61s of remaining time.\n",
      "Saving AutoGluonRGA3/\\models\\KNeighborsDist_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\KNeighborsDist_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\tWarning: Model is expected to require 32.99% of available memory...\n",
      "\tNot enough memory to train KNeighborsDist_BAG_L1... Skipping this model.\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2371.27s of the 3564.63s of remaining time.\n",
      "Saving AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 1.70316e+07\n",
      "[100]\tvalid_set's rmse: 1.69298e+07\n",
      "[150]\tvalid_set's rmse: 1.72784e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 1.51135e+07\n",
      "[100]\tvalid_set's rmse: 1.54465e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.24945e+07\n",
      "[100]\tvalid_set's rmse: 3.24396e+07\n",
      "[150]\tvalid_set's rmse: 3.2416e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.66848e+07\n",
      "[100]\tvalid_set's rmse: 3.64372e+07\n",
      "[150]\tvalid_set's rmse: 3.67529e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.53519e+07\n",
      "[100]\tvalid_set's rmse: 3.43874e+07\n",
      "[150]\tvalid_set's rmse: 3.40531e+07\n",
      "[200]\tvalid_set's rmse: 3.39794e+07\n",
      "[250]\tvalid_set's rmse: 3.39791e+07\n",
      "[300]\tvalid_set's rmse: 3.4016e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 2.87751e+07\n",
      "[100]\tvalid_set's rmse: 2.86132e+07\n",
      "[150]\tvalid_set's rmse: 2.83401e+07\n",
      "[200]\tvalid_set's rmse: 2.81943e+07\n",
      "[250]\tvalid_set's rmse: 2.81554e+07\n",
      "[300]\tvalid_set's rmse: 2.82128e+07\n",
      "[350]\tvalid_set's rmse: 2.80734e+07\n",
      "[400]\tvalid_set's rmse: 2.81092e+07\n",
      "[450]\tvalid_set's rmse: 2.80844e+07\n",
      "[500]\tvalid_set's rmse: 2.8086e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 2.45004e+07\n",
      "[100]\tvalid_set's rmse: 2.46454e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.48499e+07\n",
      "[100]\tvalid_set's rmse: 3.4714e+07\n",
      "[150]\tvalid_set's rmse: 3.47163e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "\t-28727416.2215\t = Validation score   (-root_mean_squared_error)\n",
      "\t103.72s\t = Training   runtime\n",
      "\t5.16s\t = Validation runtime\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2254.75s of the 3448.09s of remaining time.\n",
      "Saving AutoGluonRGA3/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 1.73712e+07\n",
      "[100]\tvalid_set's rmse: 1.72285e+07\n",
      "[150]\tvalid_set's rmse: 1.75887e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 1.68568e+07\n",
      "[100]\tvalid_set's rmse: 1.75939e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.18621e+07\n",
      "[100]\tvalid_set's rmse: 3.17655e+07\n",
      "[150]\tvalid_set's rmse: 3.18617e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.63519e+07\n",
      "[100]\tvalid_set's rmse: 3.69717e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.44581e+07\n",
      "[100]\tvalid_set's rmse: 3.49998e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 2.82702e+07\n",
      "[100]\tvalid_set's rmse: 2.84631e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 2.50239e+07\n",
      "[100]\tvalid_set's rmse: 2.50587e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.41567e+07\n",
      "[100]\tvalid_set's rmse: 3.44128e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonRGA3/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonRGA3/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "\t-28820953.7498\t = Validation score   (-root_mean_squared_error)\n",
      "\t100.66s\t = Training   runtime\n",
      "\t5.2s\t = Validation runtime\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 2141.0s of the 3334.33s of remaining time.\n",
      "Saving AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t153.61s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1\\utils\\oof.pkl\n",
      "Saving AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1\\model.pkl\n",
      "\t-29426772.3499\t = Validation score   (-root_mean_squared_error)\n",
      "\t2512.31s\t = Training   runtime\n",
      "\t326.5s\t = Validation runtime\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Skipping CatBoost_BAG_L1 due to lack of time remaining.\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Skipping ExtraTreesMSE_BAG_L1 due to lack of time remaining.\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Skipping NeuralNetFastAI_BAG_L1 due to lack of time remaining.\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Skipping XGBoost_BAG_L1 due to lack of time remaining.\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Skipping NeuralNetTorch_BAG_L1 due to lack of time remaining.\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Skipping LightGBMLarge_BAG_L1 due to lack of time remaining.\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Not enough time left to finish repeated k-fold bagging, stopping early ...\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1\\utils\\oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 487.59s of remaining time.\n",
      "Saving AutoGluonRGA3/\\models\\WeightedEnsemble_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\WeightedEnsemble_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "Ensemble size: 99\n",
      "Ensemble weights: \n",
      "[0.41414141 0.31313131 0.27272727]\n",
      "\t0.0s\t= Estimated out-of-fold prediction time...\n",
      "Saving AutoGluonRGA3/\\models\\WeightedEnsemble_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonRGA3/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "\t-28441811.7826\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tLightGBMXT_BAG_L2: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "\tLightGBM_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "\tRandomForestMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True, 'save_bag_folds': False}}\n",
      "\tCatBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "\tExtraTreesMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True, 'save_bag_folds': False}}\n",
      "\tNeuralNetFastAI_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "\tXGBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "\tNeuralNetTorch_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "\tLightGBMLarge_BAG_L2: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "Fitting 9 L2 models ...\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1\\utils\\oof.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 487.1s of the 486.36s of remaining time.\n",
      "Saving AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 2.21859e+07\n",
      "[100]\tvalid_set's rmse: 2.28319e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 2.51044e+07\n",
      "[100]\tvalid_set's rmse: 2.52688e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.2644e+07\n",
      "[100]\tvalid_set's rmse: 3.2564e+07\n",
      "[150]\tvalid_set's rmse: 3.26543e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 2.90484e+07\n",
      "[100]\tvalid_set's rmse: 2.90908e+07\n",
      "[150]\tvalid_set's rmse: 2.92107e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.11798e+07\n",
      "[100]\tvalid_set's rmse: 3.12297e+07\n",
      "[150]\tvalid_set's rmse: 3.12396e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.18711e+07\n",
      "[100]\tvalid_set's rmse: 3.13714e+07\n",
      "[150]\tvalid_set's rmse: 3.13692e+07\n",
      "[200]\tvalid_set's rmse: 3.12711e+07\n",
      "[250]\tvalid_set's rmse: 3.13475e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 2.63677e+07\n",
      "[100]\tvalid_set's rmse: 2.66286e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.18579e+07\n",
      "[100]\tvalid_set's rmse: 3.22388e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "\t-28839082.1323\t = Validation score   (-root_mean_squared_error)\n",
      "\t101.83s\t = Training   runtime\n",
      "\t5.36s\t = Validation runtime\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 371.57s of the 370.75s of remaining time.\n",
      "Saving AutoGluonRGA3/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting  with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 2.33563e+07\n",
      "[100]\tvalid_set's rmse: 2.38487e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F2 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 2.59286e+07\n",
      "[100]\tvalid_set's rmse: 2.6754e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F3 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.21647e+07\n",
      "[100]\tvalid_set's rmse: 3.28918e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F4 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 2.88665e+07\n",
      "[100]\tvalid_set's rmse: 2.97431e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F5 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.24454e+07\n",
      "[100]\tvalid_set's rmse: 3.28193e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F6 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.11317e+07\n",
      "[100]\tvalid_set's rmse: 3.14362e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F7 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 2.74319e+07\n",
      "[100]\tvalid_set's rmse: 2.77048e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFitting S1F8 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_set's rmse: 3.22398e+07\n",
      "[100]\tvalid_set's rmse: 3.25734e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving AutoGluonRGA3/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonRGA3/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "\t-29211509.2453\t = Validation score   (-root_mean_squared_error)\n",
      "\t100.58s\t = Training   runtime\n",
      "\t5.38s\t = Validation runtime\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 257.06s of the 256.26s of remaining time.\n",
      "Saving AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L2\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "\t6.3s\t= Estimated out-of-fold prediction time...\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L2\\utils\\oof.pkl\n",
      "Saving AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L2\\model.pkl\n",
      "\t-30226095.1114\t = Validation score   (-root_mean_squared_error)\n",
      "\t3255.94s\t = Training   runtime\n",
      "\t325.55s\t = Validation runtime\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Skipping CatBoost_BAG_L2 due to lack of time remaining.\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Skipping ExtraTreesMSE_BAG_L2 due to lack of time remaining.\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Skipping NeuralNetFastAI_BAG_L2 due to lack of time remaining.\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Skipping XGBoost_BAG_L2 due to lack of time remaining.\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Skipping NeuralNetTorch_BAG_L2 due to lack of time remaining.\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Skipping LightGBMLarge_BAG_L2 due to lack of time remaining.\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Not enough time left to finish repeated k-fold bagging, stopping early ...\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L2\\utils\\oof.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L2\\utils\\oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L3: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': False}}\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -3336.27s of remaining time.\n",
      "Saving AutoGluonRGA3/\\models\\WeightedEnsemble_L3\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\WeightedEnsemble_L3\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 12\n",
      "Ensemble size: 40\n",
      "Ensemble weights: \n",
      "[0.7   0.225 0.075]\n",
      "\t0.0s\t= Estimated out-of-fold prediction time...\n",
      "Saving AutoGluonRGA3/\\models\\WeightedEnsemble_L3\\utils\\oof.pkl\n",
      "Saving AutoGluonRGA3/\\models\\WeightedEnsemble_L3\\model.pkl\n",
      "\t-28777248.9728\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "AutoGluon training complete, total runtime = 6940.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Loading: AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Loading: AutoGluonRGA3/\\utils\\data\\X.pkl\n",
      "Loading: AutoGluonRGA3/\\utils\\data\\y.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1\\utils\\model_template.pkl\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "Saving AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1_FULL\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1_FULL\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 133 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n",
      "Saving AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1_FULL\\utils\\oof.pkl\n",
      "Saving AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1_FULL\\model.pkl\n",
      "\t11.92s\t = Training   runtime\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L1\\utils\\model_template.pkl\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "Saving AutoGluonRGA3/\\models\\LightGBM_BAG_L1_FULL\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L1_FULL\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 60 rounds... Hyperparameters: {'learning_rate': 0.05}\n",
      "Saving AutoGluonRGA3/\\models\\LightGBM_BAG_L1_FULL\\utils\\oof.pkl\n",
      "Saving AutoGluonRGA3/\\models\\LightGBM_BAG_L1_FULL\\model.pkl\n",
      "\t11.61s\t = Training   runtime\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1\\model.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t2512.31s\t = Training   runtime\n",
      "\t326.5s\t = Validation runtime\n",
      "Saving AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t0.4s\t = Training   runtime\n",
      "Saving AutoGluonRGA3/\\models\\WeightedEnsemble_L2_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2\\utils\\model_template.pkl\n",
      "Fitting 1 L2 models ...\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1\\utils\\oof.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
      "Saving AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2_FULL\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2_FULL\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 81 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n",
      "Saving AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2_FULL\\utils\\oof.pkl\n",
      "Saving AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2_FULL\\model.pkl\n",
      "\t11.54s\t = Training   runtime\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L2\\utils\\model_template.pkl\n",
      "Fitting 1 L2 models ...\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L1\\utils\\oof.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1\\utils\\oof.pkl\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "Saving AutoGluonRGA3/\\models\\LightGBM_BAG_L2_FULL\\utils\\model_template.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L2_FULL\\utils\\model_template.pkl\n",
      "\tFitting S1F1 with 'num_gpus': 0, 'num_cpus': 6\n",
      "\tFitting 41 rounds... Hyperparameters: {'learning_rate': 0.05}\n",
      "Saving AutoGluonRGA3/\\models\\LightGBM_BAG_L2_FULL\\utils\\oof.pkl\n",
      "Saving AutoGluonRGA3/\\models\\LightGBM_BAG_L2_FULL\\model.pkl\n",
      "\t11.31s\t = Training   runtime\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L2\\model.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t3255.94s\t = Training   runtime\n",
      "\t325.55s\t = Validation runtime\n",
      "Saving AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L2_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\WeightedEnsemble_L3\\model.pkl\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\t0.4s\t = Training   runtime\n",
      "Saving AutoGluonRGA3/\\models\\WeightedEnsemble_L3_FULL\\model.pkl\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1_FULL\\model.pkl\n",
      "Saving AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L1_FULL\\model.pkl\n",
      "Saving AutoGluonRGA3/\\models\\LightGBM_BAG_L1_FULL\\model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1_FULL\\model.pkl\n",
      "Saving AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\WeightedEnsemble_L2_FULL\\model.pkl\n",
      "Saving AutoGluonRGA3/\\models\\WeightedEnsemble_L2_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2_FULL\\model.pkl\n",
      "Saving AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L2_FULL\\model.pkl\n",
      "Saving AutoGluonRGA3/\\models\\LightGBM_BAG_L2_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L2_FULL\\model.pkl\n",
      "Saving AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L2_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\WeightedEnsemble_L3_FULL\\model.pkl\n",
      "Saving AutoGluonRGA3/\\models\\WeightedEnsemble_L3_FULL\\model.pkl\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Saving AutoGluonRGA3/\\models\\trainer.pkl\n",
      "Saving AutoGluonRGA3/\\learner.pkl\n",
      "Saving AutoGluonRGA3/\\predictor.pkl\n",
      "Saving AutoGluonRGA3/\\__version__ with contents \"0.5.2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutoGluonRGA3/\\\")\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\WeightedEnsemble_L2\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L2\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\WeightedEnsemble_L3\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L1_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\WeightedEnsemble_L2_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L2_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L2_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L2_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\WeightedEnsemble_L3_FULL\\model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                          model     score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0           WeightedEnsemble_L2 -2.844181e+07     336.870555  2717.090273                0.002054           0.396896            2      False          4\n",
      "1             LightGBMXT_BAG_L1 -2.872742e+07       5.163044   103.719431                5.163044         103.719431            1      False          1\n",
      "2           WeightedEnsemble_L3 -2.877725e+07     673.158936  6175.441727                0.000000           0.400934            3      False          8\n",
      "3               LightGBM_BAG_L1 -2.882095e+07       5.202985   100.661297                5.202985         100.661297            1      False          2\n",
      "4             LightGBMXT_BAG_L2 -2.883908e+07     342.224289  2818.521837                5.355789         101.828461            2      False          5\n",
      "5               LightGBM_BAG_L2 -2.921151e+07     342.248312  2817.275504                5.379811         100.582127            2      False          6\n",
      "6        RandomForestMSE_BAG_L1 -2.942677e+07     326.502471  2512.312649              326.502471        2512.312649            1       True          3\n",
      "7        RandomForestMSE_BAG_L2 -3.022610e+07     662.423336  5972.630205              325.554835        3255.936829            2      False          7\n",
      "8   RandomForestMSE_BAG_L1_FULL           NaN     326.502471  2512.312649              326.502471        2512.312649            1       True         11\n",
      "9      WeightedEnsemble_L3_FULL           NaN            NaN  5815.036456                     NaN           0.400934            3       True         16\n",
      "10     WeightedEnsemble_L2_FULL           NaN            NaN  2536.248204                     NaN           0.396896            2       True         12\n",
      "11  RandomForestMSE_BAG_L2_FULL           NaN            NaN  5791.788136              325.554835        3255.936829            2       True         15\n",
      "12         LightGBM_BAG_L2_FULL           NaN            NaN  2547.163270                     NaN          11.311962            2       True         14\n",
      "13         LightGBM_BAG_L1_FULL           NaN            NaN    11.614853                     NaN          11.614853            1       True         10\n",
      "14       LightGBMXT_BAG_L2_FULL           NaN            NaN  2547.386732                     NaN          11.535424            2       True         13\n",
      "15       LightGBMXT_BAG_L1_FULL           NaN            NaN    11.923806                     NaN          11.923806            1       True          9\n",
      "Number of models trained: 16\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_LGB', 'StackerEnsembleModel_RF', 'WeightedEnsembleModel'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])                    :   25 | ['Me identifico', 'Tengo', 'Dónde estás trabajando', 'Años de experiencia', 'Años en el puesto actual', ...]\n",
      "('category', ['text_as_category'])  :   11 | ['Plataformas', 'Lenguajes de programación o tecnologías.', 'QA / Testing', 'Bases de datos', 'A qué está atado el bono', ...]\n",
      "('float', [])                       :   10 | ['Cuánto cobrás por guardia', '¿Gente a cargo?', 'Años en la empresa actual', '¿La recomendás como un buen lugar para trabajar?', '¿Cómo calificás las políticas de diversidad e inclusión?', ...]\n",
      "('int', [])                         :    1 | ['¿Qué tan conforme estás con tu sueldo?']\n",
      "('int', ['binned', 'text_special']) :  122 | ['Plataformas.char_count', 'Plataformas.word_count', 'Plataformas.capital_ratio', 'Plataformas.lower_ratio', 'Plataformas.digit_ratio', ...]\n",
      "('int', ['bool'])                   :    1 | ['Trabajo para una empresa que no tiene oficina en mi ciudad']\n",
      "('int', ['text_ngram'])             : 4836 | ['__nlp__.21', '__nlp__.21 bash', '__nlp__.21 bash shell', '__nlp__.21 ninguno', '__nlp__.21 ninguno de', ...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:138: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutoGluonRGA3/\\models\\WeightedEnsemble_L2_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBMXT_BAG_L1_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\LightGBM_BAG_L1_FULL\\model.pkl\n",
      "Loading: AutoGluonRGA3/\\models\\RandomForestMSE_BAG_L1_FULL\\model.pkl\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['id'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m y_predproba \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mpredict(test_data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValor dólar informal semestral\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124midentificador\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAño\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSemestre\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     24\u001b[0m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_predproba\n\u001b[1;32m---> 25\u001b[0m test_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrga_autogluon.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\pandas\\core\\frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5796\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5793\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5794\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5798\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5800\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5859\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5858\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 5859\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['id'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "label = 'target'  # name of target variable to predict in this competition\n",
    "eval_metric = 'rmse'  # Optional: specify that competition evaluation metric is AUC\n",
    "save_path = 'AutoGluonRGA3/'  # where to store trained models\n",
    "\n",
    "col_to_drop = ['Valor dólar informal semestral', 'identificador', 'Año', 'Semestre', 'target']\n",
    "\n",
    "train_data = pd.read_csv('train_data.csv', low_memory=False)\n",
    "train_data['target'] = train_data['Valor dólar informal semestral']*train_data['Salario mensual (en tu moneda local)']\n",
    "test_data = pd.read_csv('features.csv', low_memory=False)\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric=eval_metric, path=save_path, verbosity=3).fit(\n",
    "    train_data.drop(columns=['Salario mensual (en tu moneda local)', 'Valor dólar informal semestral', 'identificador', 'Año', 'Semestre']), \n",
    "    presets='high_quality', \n",
    "    time_limit=3600\n",
    ")\n",
    "\n",
    "results = predictor.fit_summary()\n",
    "y_predproba = predictor.predict(test_data.drop(columns=['Valor dólar informal semestral', 'identificador', 'Año', 'Semestre']))\n",
    "\n",
    "test_data['target'] = y_predproba\n",
    "test_data[['id', 'target']].to_csv('rga_autogluon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d6ca7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['identificador', 'target']].to_csv('rga_autogluon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47e90c50",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array 1360.829285247493 cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalario mensual (en tu moneda local)\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39mtrain_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValor dólar informal semestral\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m truth \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m mean_absolute_error(truth[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m], train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\sklearn\\metrics\\_regression.py:191\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_absolute_error\u001b[39m(\n\u001b[0;32m    136\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;124;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    0.85...\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    195\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(np\u001b[38;5;241m.\u001b[39mabs(y_pred \u001b[38;5;241m-\u001b[39m y_true), weights\u001b[38;5;241m=\u001b[39msample_weight, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\sklearn\\metrics\\_regression.py:94\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m        the dtype argument passed to check_array.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m     96\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\sklearn\\utils\\validation.py:329\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_consistent_length\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    330\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\sklearn\\utils\\validation.py:329\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_consistent_length\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[43m_num_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    330\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\autogluon\\lib\\site-packages\\sklearn\\utils\\validation.py:269\u001b[0m, in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 269\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    270\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingleton array \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m cannot be considered a valid collection.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m x\n\u001b[0;32m    271\u001b[0m         )\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# Check that shape is returning an integer or default to len\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# Dask dataframes may not return numeric shape[0] value\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array 1360.829285247493 cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_data.csv', low_memory=False)\n",
    "train_data['target'] = train_data['Salario mensual (en tu moneda local)']/train_data['Valor dólar informal semestral']\n",
    "truth = pd.read_csv('target.csv')\n",
    "mean_absolute_error(truth['target'], train_data['target'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3573baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth['pred']= train_data['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcad0ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275876.39575201034"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(truth['target'], truth['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6781e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoGluon ML",
   "language": "python",
   "name": "autogluon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
